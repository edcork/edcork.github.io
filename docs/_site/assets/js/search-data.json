{"0": {
    "doc": "Page Not Found",
    "title": "Page Not Found",
    "content": "This is probably my fault. Sorry. Please try browsing to the page you were looking for, otherwise shoot me an email to complain. ",
    "url": "/404.html",
    
    "relUrl": "/404.html"
  },"1": {
    "doc": "Create an agreement model",
    "title": "Create an agreement model",
    "content": "On this page . | Method 1: Create an agreement model from an existing record | Method 2: Create an agreement model from scratch | Activate the new agreement model | . There are two methods to create an agreement model. ",
    "url": "/pages/write/create_agreement_model.html",
    
    "relUrl": "/pages/write/create_agreement_model.html"
  },"2": {
    "doc": "Create an agreement model",
    "title": "Method 1: Create an agreement model from an existing record",
    "content": "When you create an agreement model created this way, all field values from the existing record (including user options, but excluding service offerings and workflow status) are copied to the draft model. You can then edit the values as required. | From the main menu, select Plan &gt; Service Level, and then select Service Agreements, Support Agreements, or Human Resources Agreements from the drop-down menu. Service Management displays a list of the selected type of agreement record. | In the ID column, click the number of the agreement record that you want to use as a model. Service Management displays the agreement record details. | Click More &gt; Create model from record. Service Management displays the new model form, populated with the values copied from the original record. | Edit the model as required. The title and description of the original record are copied to the default values in the model. You must enter a new title and description for the model itself. | On the toolbar, click Save. | . ",
    "url": "/pages/write/create_agreement_model.html#method-1-create-an-agreement-model-from-an-existing-record",
    
    "relUrl": "/pages/write/create_agreement_model.html#method-1-create-an-agreement-model-from-an-existing-record"
  },"3": {
    "doc": "Create an agreement model",
    "title": "Method 2: Create an agreement model from scratch",
    "content": ". | From the main menu, select Plan &gt; Service Level &gt; Agreement Models &gt; New. Service Management displays the New Model form. | Complete the General model details section. | Field | Description | . | Title | A meaningful, descriptive, and relatively short name for the model. The model name is often the only identifier displayed in selection lists. | . | Description | A description that summarizes the model details. | . | Agreement flavor | Whether this model applies to service agreements, support agreements, or human resources agreements. | . | Agreement type | The type of agreement record sets the workflow type. | . | (Optional) Click Attachments &gt; Add attachment to upload a file to the agreement model. Supported attachment file formats and the maximum file size are defined in the tenant’s Application settings tab in Suite Administration. If the Attachments field of agreement models is encrypted and you are a member of an encryption domain, you can click Add encrypted attachments to attach an encrypted file to the record. Attachments to models aren’t visible in the Service Portal. | (Optional) Click the Approvals tab to add approvals to the model. For more information about approvals, see Task plans. | Click the Default values tab and enter the values that you want to include in the model. Details . | Field | Description | . | Title | A unique word or phrase that's an identifier for the agreement records created with this model. This should be a value that makes it easy for end users to understand the purpose of the agreement record. Example: Initial review for priority 1 incidents . | . | Default agreement | This option applies this model by default when end users create agreement records. | . | Owner | The Service Level Agreement owner for records created with this model. If you leave this field empty, the Service Level Agreement owner defaults to the current user when end users create a record with this model. Example: Service Level Manager. | . | Technical group | The technical group to which records created with this model are assigned. | . | Financial group | The financial group to which records created with this model are assigned. | . | Description | A description of the records created with this model. | . Requirement . | Field | Description | . | Cost | The cost of agreement records created with this model. | . | Effort | The effort associated with agreement records created with this model. | . | Validity start date | The start date of the period of validity of agreement records created with this model. | . | Validity end date | The end date of the period of validity of agreement records created with this model. | . Review and improvement . | Field | Description | . | Next review date | The date when agreement records created with this model must next be reviewed. | . | Service quality report | The service quality report associated with agreement records created with this model. | . | Improvement measures | The improvement measures associated with agreement records created with this model. | . | Service improvement plan | The service improvement plan associated with agreement records created with this model. | . | On the toolbar, click Save. | . ",
    "url": "/pages/write/create_agreement_model.html#method-2-create-an-agreement-model-from-scratch",
    
    "relUrl": "/pages/write/create_agreement_model.html#method-2-create-an-agreement-model-from-scratch"
  },"4": {
    "doc": "Create an agreement model",
    "title": "Activate the new agreement model",
    "content": "When you create a model, Service Management automatically creates its workflow and displays the model in Draft status. In order for the model to be available for selection when creating agreement records, its status must be Active. To set a model’s status to Active, click the model’s workflow tab to see the workflow and status. On the toolbar, click Activate, and then click Save. To change the status to Active, you must have the appropriate rights. You may edit the agreement type only when the model has Draft status. ",
    "url": "/pages/write/create_agreement_model.html#activate-the-new-agreement-model",
    
    "relUrl": "/pages/write/create_agreement_model.html#activate-the-new-agreement-model"
  },"5": {
    "doc": "Create an agreement model",
    "title": "Related topics",
    "content": ". | Edit an agreement model | Retire an agreement model | . ",
    "url": "/pages/write/create_agreement_model.html#related-topics",
    
    "relUrl": "/pages/write/create_agreement_model.html#related-topics"
  },"6": {
    "doc": "Enable a regular user to install OMT",
    "title": "Enable a regular user to install OMT",
    "content": "On this page . | Delegate the authority to install OMT to a regular user | Install OMT as a regular user . | Embedded Kubernetes minimal installation (regular user) | . | . sudo is a command line utility for Unix and Unix based operating systems. The utility provides an efficient way for system administrators to temporarily delegate certain regular users or user groups privileged access to system resources. Therefore, the users can run commands that they can’t run under their regular accounts. OMT requires root user permissions to install OMT. If the root users have security concerns to give a regular user the root password, you can delegate authority to the user before installing OMT. You can revoke the permissions after OMT installation. The node_prereq script can grant a specified regular user authority to run all the commands required to install OMT. You can also grant authority to administer OMT at the same or simply use the default value. But you will have to run the script again when they want to upgrade, as the regular user needs permission to run the upgrade command from the upgrade package. For a full command list that a regular user can run after granting permissions, refer to Use sudo to enable a regular user to install, upgrade, or administer OMT. ",
    "url": "/pages/write/create_sudouser.html",
    
    "relUrl": "/pages/write/create_sudouser.html"
  },"7": {
    "doc": "Enable a regular user to install OMT",
    "title": "Delegate the authority to install OMT to a regular user",
    "content": "To configure sudo permission, you must run the node_prereq script on the cluster node with a root user and the regular user must already exist on the node. Perform the following steps to delegate authority to install OMT to a regular user: . | Download the OMT installation package to the first master node and unzip it. | Change directory into the OMT installation package directory and run the following command to delegate authority to install OMT to a regular user: ./node_prereq --sudo-user &lt;username&gt; --sudo-for install . | After finishing the permission configuration on the first node, copy the node_prereq script to any directory (For example, /tmp) on other nodes and ensure that it has executable permission. Then run the following command: ./node_prereq --sudo-user &lt;username&gt; --sudo-for install . | . | If you’ll specify the OMT installation directory using the --cdf-home option when running the ./install script, specify the same cdf home with --cdf-home option when running node_prereq: ./node_prereq --sudo-user &lt;username&gt; --sudo-for install --cdf-home &lt;cdf_home&gt; . | If you’ll specify the temporary directory using the --tmp-folder option when running the ./install script, specify the same temporary directory with --tmp-folder option when running node_prereq: ./node_prereq --sudo-user &lt;username&gt; --sudo-for install --tmp-folder &lt;tmp_folder&gt; . | If you’ll perform administrative tasks with this regular user after installation, add administer to --sudo-for parameter as below or use the default value to delegate the install and administer authorities to the user: ./node_prereq --sudo-user &lt;username&gt; --sudo-for 'install,administer' . | . ",
    "url": "/pages/write/create_sudouser.html#delegate-the-authority-to-install-omt-to-a-regular-user",
    
    "relUrl": "/pages/write/create_sudouser.html#delegate-the-authority-to-install-omt-to-a-regular-user"
  },"8": {
    "doc": "Enable a regular user to install OMT",
    "title": "Install OMT as a regular user",
    "content": "After the regular user got the required authorities, run the command to get a list of commands you can run as a regular user: . sudo -n -ll -U &lt;username&gt; . To run the commands as a regular user, prefix the command with sudo. For example, if you’ll run the install command: ./install -c /tmp/config.json --nfsprov-server 192.0.2.0 --nfsprov-folder var/vols/itom/data as a regular user, run sudo ./install -c /tmp/config.json --nfsprov-server 192.0.2.0 --nfsprov-folder var/vols/itom/data instead. Embedded Kubernetes minimal installation (regular user) . Use this command to install OMT as a regular user. sudo ./install \\ -c &lt;config.json file&gt; \\ --nfsprov-server &lt;nfs-server&gt; \\ --nfsprov-folder &lt;nfs-directory&gt; . For example: . sudo ./install \\ -c /tmp/config.json \\ --nfsprov-server 192.0.2.0 \\ --nfsprov-folder /var/vols/itom/data . ",
    "url": "/pages/write/create_sudouser.html#install-omt-as-a-regular-user",
    
    "relUrl": "/pages/write/create_sudouser.html#install-omt-as-a-regular-user"
  },"9": {
    "doc": "Enable a regular user to install OMT",
    "title": "Related topics",
    "content": ". | When you have finished, return to Set up prerequisites (embedded K8s) to continue. | For how to revoke authority after OMT installation, refer to Revoke authority to install OMT. | If you don’t want to delegate authority to a regular user with the node_prereq script, refer to Delegate authority to a regular user without using the script. | . ",
    "url": "/pages/write/create_sudouser.html#related-topics",
    
    "relUrl": "/pages/write/create_sudouser.html#related-topics"
  },"10": {
    "doc": "Design",
    "title": "Design",
    "content": "Examples of doc design. CV has stats on help restrtucture . ",
    "url": "/pages/design.html",
    
    "relUrl": "/pages/design.html"
  },"11": {
    "doc": "Dev2Prod REST API",
    "title": "Dev2Prod REST API",
    "content": "On this page . | /export/types/{package_id} | /export/{id} | /import/{id} | /import | . The Service Management Dev2Prod API enables you to synchronize configuration information or data from a development environment to a production environment (or between two development environments). The Dev2Prod API includes the following endpoints: . | /export/types/{package_id} | /export/{id} | /import/{id} | /import/ | . ",
    "url": "/pages/write/dev2prod_api.html",
    
    "relUrl": "/pages/write/dev2prod_api.html"
  },"12": {
    "doc": "Dev2Prod REST API",
    "title": "/export/types/{package_id}",
    "content": "Exports a customized package (created in Package Manager) on the source tenant to the File Repository Service (FRS). URL structure . https://{serverAddress}/rest/{tenant_id}/dev2prod/export/types/{package_id} . Method . POST . Parameters . | {tenant_id}: The tenant ID of the Service Management tenant. | {package_id}: The ID of the export package. | . Example call . https://company.net/rest/123456789/dev2prod/export/types/54321 . Example request body . {} . Example response . { \"Id\": \"66fabdf1e4b06908b04d4051\" } . Error codes . | 400: Invalid Request | 401: Unauthorized | 403: Forbidden | 404: Page/Resource was not found | 500: Internal Server Error | . ",
    "url": "/pages/write/dev2prod_api.html#exporttypespackage_id",
    
    "relUrl": "/pages/write/dev2prod_api.html#exporttypespackage_id"
  },"13": {
    "doc": "Dev2Prod REST API",
    "title": "/export/{id}",
    "content": "Gets the status of an export process. URL structure . https://{serverAddress}/rest/{tenant_id}/dev2prod/export/{id} . Method . GET . Parameters . | {tenant_id}: The tenant ID of the Service Management tenant. | {id}: The ID of the export process. | . Example call . https://company.net/rest/123456789/dev2prod/export/66fabdf1e4b06908b04d4051 . Example response . { \"id\": \"6797556be4b0c05084915394\", \"PackageBundleName\": \"Package-10204-555500000-2025-01-27 09-44\", \"Status\": \"COMPLETED\", \"FileId\": \"c7e87b89-f214-4766-8032-bba6967e1c49\", \"StartTime\": 1737971051843, \"LastUpdateTime\": 1737971053134, \"ItemCount\": 352, \"Statistics\": [ { \"PackageName\": \"metadata\", \"ItemCount\": 0 }, { \"PackageName\": \"workflow\", \"ItemCount\": 24 }, { \"PackageName\": \"formLayouts\", \"ItemCount\": 117 } ], \"OperationType\": \"Export\" } . Error codes . | 400: Invalid Request | 401: Unauthorized | 403: Forbidden | 404: Page/Resource was not found | 500: Internal Server Error | . ",
    "url": "/pages/write/dev2prod_api.html#exportid",
    
    "relUrl": "/pages/write/dev2prod_api.html#exportid"
  },"14": {
    "doc": "Dev2Prod REST API",
    "title": "/import/{id}",
    "content": "Gets the status of an import process. URL structure . https://{serverAddress}/rest/{tenant_id}/dev2prod/import/{id} . Method . GET . Parameters . | {tenant_id}: The tenant ID of the Service Management tenant. | {id}: The ID of the import process. | . Example call . https://company.net/rest/123456789/dev2prod/import/66fabdf1e4b06908b04d4051 . Example response - success . { \"TimeToLive\": 9946986, \"StudioLocked\": false, \"SystemLocked\": false, \"id\": \"67975a80e4b0c050849153ae\", \"FileId\": \"9bf82520-f087-4763-af10-cf1ed38d893d\", \"Status\": \"COMPLETED\", \"DryRun\": true, \"PackageManagerId\": \"67975a8be4b0c050849153b2\", \"StartTime\": 1737972352849, \"LastUpdateTime\": 1737972406603, \"ProcessingServerName\": \"company-server\", \"Results\": [ { \"PackageName\": \"metadata\", \"Statistics\": { \"Modified\": 300, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] }, { \"PackageName\": \"SLTTenantSetting\", \"Statistics\": { \"Modified\": 0, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] }, { \"PackageName\": \"indexConfiguration\", \"Statistics\": { \"Modified\": 26, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] } ], \"Totals\": { \"Added\": 0, \"Modified\": 326, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"OperationType\": \"Import\" } . Error codes . | 400: Invalid Request | 401: Unauthorized | 403: Forbidden | 404: Page/Resource was not found | 500: Internal Server Error | . ",
    "url": "/pages/write/dev2prod_api.html#importid",
    
    "relUrl": "/pages/write/dev2prod_api.html#importid"
  },"15": {
    "doc": "Dev2Prod REST API",
    "title": "/import",
    "content": "Imports a Dev2Prod package to the current tenant. URL structure . https://{serverAddress}/rest/{tenant_id}/dev2prod/import . Method . POST . Parameters . | {tenant_id}: The tenant ID of the Service Management tenant. | dryRun: Simulates the import process. Helps to find any conflicts or errors without committing any changes. | . Example call . https://company.net/rest/123456789/dev2prod/import?dryRun=true . Example request body . { \"FileId\": \"9bf82520-f087-4763-af10-cf1ed38d893d\" } . Example response . { \"Id\": \"67975cd8e4b0c0508491541f\" } . Error codes . | 400: Invalid Request | 401: Unauthorized | 403: Forbidden | 404: Page/Resource was not found | 500: Internal Server Error | . ",
    "url": "/pages/write/dev2prod_api.html#import",
    
    "relUrl": "/pages/write/dev2prod_api.html#import"
  },"16": {
    "doc": "Use API calls to synchronize two environments",
    "title": "Related topics",
    "content": ". | For an example of how to use this API, see Use API calls to synchronize two environments | . parent: Dev2Prod REST API nav_order: 1 . ",
    "url": "/pages/write/dev2prod_use_case.html#related-topics",
    
    "relUrl": "/pages/write/dev2prod_use_case.html#related-topics"
  },"17": {
    "doc": "Use API calls to synchronize two environments",
    "title": "Use case: Use API calls to synchronize two environments",
    "content": "On this page . | Prerequisites | Workflow and examples | . To synchronize two environments using API calls, you will use the Case Exchange REST API together with Dev2Prod REST API. ",
    "url": "/pages/write/dev2prod_use_case.html#use-case-use-api-calls-to-synchronize-two-environments",
    
    "relUrl": "/pages/write/dev2prod_use_case.html#use-case-use-api-calls-to-synchronize-two-environments"
  },"18": {
    "doc": "Use API calls to synchronize two environments",
    "title": "Prerequisites",
    "content": "You have already generated a full or granular export package on the source tenant. For detailed instructions how to do this, see Dev2Prod - Synchronize your development and production tenants. ",
    "url": "/pages/write/dev2prod_use_case.html#prerequisites",
    
    "relUrl": "/pages/write/dev2prod_use_case.html#prerequisites"
  },"19": {
    "doc": "Use API calls to synchronize two environments",
    "title": "Workflow and examples",
    "content": ". | Run the https://{serverAddress}/rest/{tenant_id}/dev2prod/export/types/{id} POST method API to export the package to the File Repository Service (FRS). The expected response is the export ID. Example This example exports package 54321 on tenant 123456789. | API: https://company.net/rest/123456789/dev2prod/export/types/54321 | Response body: { \"Id\": \"66fabdf1e4b06908b04d4051\" } . | . | Use the export ID returned in the previous step to run the https://{serverAddress}/rest/{tenant_id}/dev2prod/export/{id} GET method API and view the export details. Example This example queries export 66fabdf1e4b06908b04d4051 on tenant 123456789. | API: https://company.net/rest/123456789/dev2prod/export/66fabdf1e4b06908b04d4051 | Response body: { \"id\": \"6797556be4b0c05084915394\", \"PackageBundleName\": \"Package-54321-123456789-2025-01-27 09-44\", \"Status\": \"COMPLETED\", \"FileId\": \"c7e87b89-f214-4766-8032-bba6967e1c49\", \"StartTime\": 1737971051843, \"LastUpdateTime\": 1737971053134, \"ItemCount\": 352, \"Statistics\": [ { \"PackageName\": \"metadata\", \"ItemCount\": 0 }, { \"PackageName\": \"workflow\", \"ItemCount\": 24 }, { \"PackageName\": \"formLayouts\", \"ItemCount\": 117 }, { \"PackageName\": \"customAction\", \"ItemCount\": 0 }, { \"PackageName\": \"indexConfiguration\", \"ItemCount\": 0 }, { \"PackageName\": \"featureInstances\", \"ItemCount\": 26 }, { \"PackageName\": \"uiComponentConfig\", \"ItemCount\": 0 }, { \"PackageName\": \"notificationTemplateDefinitions\", \"ItemCount\": 88 }, { \"PackageName\": \"notificationTemplateBodies\", \"ItemCount\": 97 }, { \"PackageName\": \"SLTTenantSetting\", \"ItemCount\": 0 }, { \"PackageName\": \"data\", \"ItemCount\": 0 }, { \"PackageName\": \"userOptionMetadata\", \"ItemCount\": 0 }, { \"PackageName\": \"rulesInEntity\", \"ItemCount\": 0 }, { \"PackageName\": \"many2manyRelations\", \"ItemCount\": 0 }, { \"PackageName\": \"attachments\", \"ItemCount\": 0 }, { \"PackageName\": \"customAppSltSettings\", \"ItemCount\": 0 }, { \"PackageName\": \"resourceBundles\", \"ItemCount\": 0 } ], \"OperationType\": \"Export\" } . | . | Run the https://{serverAddress}/rest/{tenant_id}/ces/attachment/{attachment_id} GET method API to download the exported package to the source tenant. Use the FileID value from the previous response as the attachment ID. Example This example downloads package c7e87b89-f214-4766-8032-bba6967e1c49 on tenant 123456789. | API: https://company.net/rest/123456789/dev2prod/ces/attachment/c7e87b89-f214-4766-8032-bba6967e1c49 | . | Run the https://{serverAddress}/rest/{tenant_id}/ces/attachment POST method API to upload the package to FRS. Example This example uploads package c7e87b89-f214-4766-8032-bba6967e1c49 on tenant 123456789. | API: https://company.net/rest/123456789/ces/attachment | Headers: Set User-Agent to Apache-HttpClient/4.4.1. | Request body: Set to Files[], and then click Choose Files to select the file to attach. | Response body: { \"Id\": \"66fabdf1e4b06908b04d4051\" } | . | Run the https://{serverAddress}/rest/{tenant_id}/dev2prod/import?dryRun=true POST method API to simulate importing the package to the target tenant. Example This example simulates importing package c7e87b89-f214-4766-8032-bba6967e1c49 on tenant 123456789. | API: https://company.net/rest/123456789/dev2prod/import?dryRun=true | Request body: { \"FileId\": \"c7e87b89-f214-4766-8032-bba6967e1c49\" } . | Response body: { \"Id\": \"66fabdf1e4b06908b04d4051\" } . | . | Run the https://{serverAddress}/rest/{tenant_id}/dev2prod/import/{id} GET method API to check the import status. If there are no errors, the dry run was successful, and you can import the package. Example This example checks the status of import 66fabdf1e4b06908b04d4051 on tenant 123456789. | API: https://company.net/rest/123456789/dev2prod/import/66fabdf1e4b06908b04d4051 | Response body: { \"TimeToLive\": 9946986, \"StudioLocked\": false, \"SystemLocked\": false, \"id\": \"66fabdf1e4b06908b04d4051\", \"FileId\": \"c7e87b89-f214-4766-8032-bba6967e1c49\", \"Status\": \"COMPLETED\", \"DryRun\": true, \"PackageManagerId\": \"67975a8be4b0c050849153b2\", \"StartTime\": 1737972352849, \"LastUpdateTime\": 1737972406603, \"ProcessingServerName\": \"company-server\", \"Results\": [ { \"PackageName\": \"metadata\", \"Statistics\": { \"Modified\": 300, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] }, { \"PackageName\": \"SLTTenantSetting\", \"Statistics\": { \"Modified\": 0, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] }, { \"PackageName\": \"indexConfiguration\", \"Statistics\": { \"Modified\": 26, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] } ], \"Totals\": { \"Added\": 0, \"Modified\": 326, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"OperationType\": \"Import\" } . | . | Run the https://{serverAddress}/rest/{tenant_id}/dev2prod/import POST method API to import the package to the source tenant. Example This example imports package c7e87b89-f214-4766-8032-bba6967e1c49 on tenant 123456789. | API:https://company.net/rest/123456789/dev2prod/import | Request body: { \"FileId\": \"c7e87b89-f214-4766-8032-bba6967e1c49\" } . | Response body: { \"Id\": \"66fabdf1e4b06908b04d4051\" } . | . | . ",
    "url": "/pages/write/dev2prod_use_case.html#workflow-and-examples",
    
    "relUrl": "/pages/write/dev2prod_use_case.html#workflow-and-examples"
  },"20": {
    "doc": "Use API calls to synchronize two environments",
    "title": "Use API calls to synchronize two environments",
    "content": " ",
    "url": "/pages/write/dev2prod_use_case.html",
    
    "relUrl": "/pages/write/dev2prod_use_case.html"
  },"21": {
    "doc": "Editing",
    "title": "Editing",
    "content": "This section contains some examples of my editing. ",
    "url": "/pages/edit.html",
    
    "relUrl": "/pages/edit.html"
  },"22": {
    "doc": "Editing",
    "title": "Sample 1: Troubleshooting topic 1",
    "content": "This troubleshooting topic is an example of fixing generally poor language. ",
    "url": "/pages/edit.html#sample-1-troubleshooting-topic-1",
    
    "relUrl": "/pages/edit.html#sample-1-troubleshooting-topic-1"
  },"23": {
    "doc": "Editing",
    "title": "Sample 1: Troubleshooting topic 2",
    "content": "Another troubleshooting topic, this time containing numerous technical inaccuracies. ",
    "url": "/pages/edit.html#sample-1-troubleshooting-topic-2",
    
    "relUrl": "/pages/edit.html#sample-1-troubleshooting-topic-2"
  },"24": {
    "doc": "Editing",
    "title": "Sample 3: Integration guide",
    "content": "In this parent topic that introduces some integration guides, poor structure makes the basic message unclear. ",
    "url": "/pages/edit.html#sample-3-integration-guide",
    
    "relUrl": "/pages/edit.html#sample-3-integration-guide"
  },"25": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Editing sample 1: Troubleshooting topic 1",
    "content": "On this page . | ORIGINAL TEXT: Kubelet gets stuck with massive “runtime service failed: rpc error: code = Unknown” error messages | EDITED TEXT: “runtime service failed: rpc error: code = Unknown” error messages and the kubelet enters a restart loop | . Source: UCMDB 2021.08 documentation. Summary of issues . | Non-native language mistakes | Including: “Massive” error messages, “a known issue of Kubernetes”, “see details from”, and “then run below command.” | . | Formatting issues | The link to the Kubernetes documentation uses the URL as title text. The “Terminating” state is formatted inconsistently. There is no need to format “Kubelet” with code tags. | . | Technical accuracy | “Kubelet” should be “the kubelet”, as defined by the Kubernetes glossary. | . | Structure | The first sentence of the Cause section is actually part of the symptoms. | . | Redundancies | “On a node that has CDF installed” is redundant: this is documentation about the product CDF (therefore, all nodes have CDF installed on them). | . ",
    "url": "/pages/edit/edit1.html#editing-sample-1-troubleshooting-topic-1",
    
    "relUrl": "/pages/edit/edit1.html#editing-sample-1-troubleshooting-topic-1"
  },"26": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "ORIGINAL TEXT: Kubelet gets stuck with massive “runtime service failed: rpc error: code = Unknown” error messages",
    "content": "You receive massive error messages on a node with CDF installed that resemble the following: . b83ace3b5e0870284e554502e8922563a4d9587b800b6b699dc2c2acfcc9b7cc\" from runtime service failed: rpc error: code = Unknown desc = unable to inspect docker image \"sha256:a950dd441cee8f60ce4ee325799c62e5fe444fa8e851b5c96b9172da0ced8d28\" while inspecting docker container \"b83ace3b5e0870284e554502e8922563a4d9587b800b6b699dc2c2acfcc9b7cc\": no such image: \"sha256:a950dd441cee8f60ce4ee325799c62e5fe444fa8e851b5c96b9172da0ced8d28\" . When you check the pod status, some pods are stuck in Terminating state. ",
    "url": "/pages/edit/edit1.html#original-text-kubelet-gets-stuck-with-massive-runtime-service-failed-rpc-error-code--unknown-error-messages",
    
    "relUrl": "/pages/edit/edit1.html#original-text-kubelet-gets-stuck-with-massive-runtime-service-failed-rpc-error-code--unknown-error-messages"
  },"27": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Cause",
    "content": "This issue occurs because Kubelet loops when trying to inspect a Docker container for a pod whose image has been deleted or cleaned up. This is a known issue of Kubernetes. See details from https://github.com/kubernetes/kubernetes/issues/84214. ",
    "url": "/pages/edit/edit1.html#cause",
    
    "relUrl": "/pages/edit/edit1.html#cause"
  },"28": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Solution",
    "content": ". | Log on to the node where you receive these error messages. | Run the following command to check the pod status: kubectl get pods -n core -o wide | Identify the pods that are stuck in the “Terminating” state on this node. Then run below command to delete the pods. You need to replace the &lt;pod name&gt; placeholder with the name of the pod that is in the “Terminating” state. Run the following command for all the “terminating” pods: kubectl delete pod &lt;pod name&gt; -n core --force --grace-period=0 | Run the following command to restart CDF: K8S_HOME/bin/kube-restart.sh | . ",
    "url": "/pages/edit/edit1.html#solution",
    
    "relUrl": "/pages/edit/edit1.html#solution"
  },"29": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "EDITED TEXT: “runtime service failed: rpc error: code = Unknown” error messages and the kubelet enters a restart loop",
    "content": "When the kubelet tries to inspect the Docker container of a pod whose image was deleted or cleaned up, the kubelet enters a cyclical restart loop. When this issue occurs, the pod becomes stuck in the “Terminating” state, and you receive error messages that resemble the following: . b83ace3b5e0870284e554502e8922563a4d9587b800b6b699dc2c2acfcc9b7cc\" from runtime service failed: rpc error: code = Unknown desc = unable to inspect docker image \"sha256:a950dd441cee8f60ce4ee325799c62e5fe444fa8e851b5c96b9172da0ced8d28\" while inspecting docker container \"b83ace3b5e0870284e554502e8922563a4d9587b800b6b699dc2c2acfcc9b7cc\": no such image: \"sha256:a950dd441cee8f60ce4ee325799c62e5fe444fa8e851b5c96b9172da0ced8d28\"` . ",
    "url": "/pages/edit/edit1.html#edited-text-runtime-service-failed-rpc-error-code--unknown-error-messages-and-the-kubelet-enters-a-restart-loop",
    
    "relUrl": "/pages/edit/edit1.html#edited-text-runtime-service-failed-rpc-error-code--unknown-error-messages-and-the-kubelet-enters-a-restart-loop"
  },"30": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Cause",
    "content": "This is a known issue in Kubernetes. For more information, see Kubelet gets stuck trying to inspect a container whose image has been cleaned up. ",
    "url": "/pages/edit/edit1.html#cause-1",
    
    "relUrl": "/pages/edit/edit1.html#cause-1"
  },"31": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Solution",
    "content": ". | Log on to the node where you receive the error messages. | Run the following command to check the pod status, and then identify the pods that are stuck in the “Terminating” state: kubectl get pods -n core -o wide | Run the following command to delete the pods. Replace the &lt;pod name&gt; placeholder with the name of the pod that is in the “Terminating” state. Do this for all pods stuck in the “Terminating” state. kubectl delete pod &lt;pod name&gt; -n core --force --grace-period=0 | Run the following command to restart CDF: $K8S_HOME/bin/kube-restart.sh | . ",
    "url": "/pages/edit/edit1.html#solution-1",
    
    "relUrl": "/pages/edit/edit1.html#solution-1"
  },"32": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Sample 1: Troubleshooting topic 1",
    "content": " ",
    "url": "/pages/edit/edit1.html",
    
    "relUrl": "/pages/edit/edit1.html"
  },"33": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Editing sample 2: Troubleshooting topic",
    "content": "On this page . | ORIGINAL TEXT: RabbitMQ isn’t ready | EDITED TEXT: RabbitMQ pod isn’t ready | . Source: SMAX 2021.11 documentation. Summary of issues . | Technical accuracy | You do not power off an environment. The note says “If you… fail to remove these folders,” but it’s not the user that fails to do this. Further, there are no instructions at the linked Azure files topic, as the described issue was fixed 2.5 years earlier. | . | Clarity | The connection between the first sentence and the first step of the Solution is lost due to poor structure. | . | Product naming and consistency | The terms “environment” and “system” are used interchangeably. The term “suite” is used instead of the correct product name. Inconsistent formatting of directory paths. | . ",
    "url": "/pages/edit/edit2.html#editing-sample-2-troubleshooting-topic",
    
    "relUrl": "/pages/edit/edit2.html#editing-sample-2-troubleshooting-topic"
  },"34": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "ORIGINAL TEXT: RabbitMQ isn’t ready",
    "content": "The infra-rabbitmq-&lt;n&gt; (&lt;n&gt;=0, 1, or 2) pod isn’t ready. The pod’s readiness state is stuck in 1/2. The following is an example: . NAME READY STATUS RESTARTS AGE infra-rabbitmq-0 1/2 Running 0 16h . ",
    "url": "/pages/edit/edit2.html#original-text-rabbitmq-isnt-ready",
    
    "relUrl": "/pages/edit/edit2.html#original-text-rabbitmq-isnt-ready"
  },"35": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Cause",
    "content": "There are many causes for this issue. Here are some examples. | The suite environment wasn’t shut down gracefully. For example, you powered off the environment directly without shutting down the suite and OMT pods first. | Your system has insufficient hardware resources. | There are issues with the network connectivity between the NFS server and worker nodes. | . ",
    "url": "/pages/edit/edit2.html#cause",
    
    "relUrl": "/pages/edit/edit2.html#cause"
  },"36": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Solution",
    "content": "When RabbitMQ fails to start twice, the system will automatically perform a fresh start of RabbitMQ. When this issue occurs: . | Wait up to 15 minutes, the issue is probably gone. | If the problem still persists, check your system resources and network connectivity. | If it’s not a resource or network issue, manually restart RabbitMQ. | . How to manually restart RabbitMQ: . | Run the following command on a master node (embedded Kubernetes) or the bastion node (managed Kubernetes) to stop RabbitMQ: . kubectl scale statefulset infra-rabbitmq -n &amp;lt;suite namespace&amp;gt; --replicas=0 . | Wait until all RabbitMQ pods are terminated. | Remove the &lt;rabbitmq-infra-rabbitmq-n&gt;/data/xservices/rabbitmq/x.x.x.xx/mnesia folders on the NFS server or the bastion node (if managed NFS is used, including EFS, Azure Files, Azure NetApp Files, and Filestore). For example, remove the following folders: . /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-0/data/xservices/rabbitmq/x.x.x.xx/mnesia /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-1/data/xservices/rabbitmq/x.x.x.xx/mnesia /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-2/data/xservices/rabbitmq/x.x.x.xx/mnesia . Note If you use Azure Files as the storage service and fail to remove these folders, follow the instructions at Fail to delete files except that you don’t need to install Azure Powershell Module. Instead, you can run all the commands with Azure Cloud Shell from the Azure portal. | Run the following command on a master node (embedded Kubernetes) or the bastion node (managed Kubernetes) to restart RabbitMQ: . kubectl scale statefulset infra-rabbitmq -n &lt;suite namespace&gt; --replicas=3 . | . ",
    "url": "/pages/edit/edit2.html#solution",
    
    "relUrl": "/pages/edit/edit2.html#solution"
  },"37": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "EDITED TEXT: RabbitMQ pod isn’t ready",
    "content": "The RabbitMQ pod (for example, infra-rabbitmq-&lt;n&gt;, where &lt;n&gt; is 0, 1, or 2) isn’t ready. The pod’s readiness state is stuck at 1/2. For example: . NAME READY STATUS RESTARTS AGE infra-rabbitmq-0 1/2 Running 0 16h . ",
    "url": "/pages/edit/edit2.html#edited-text-rabbitmq-pod-isnt-ready",
    
    "relUrl": "/pages/edit/edit2.html#edited-text-rabbitmq-pod-isnt-ready"
  },"38": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Cause",
    "content": "This issue may occur because: . | The system was shut down incorrectly. For example, you powered off the system without first shutting down Service Management and OMT. | Your system doesn’t have enough hardware resources. | There are network connectivity issues between the NFS server and the worker nodes. | . ",
    "url": "/pages/edit/edit2.html#cause-1",
    
    "relUrl": "/pages/edit/edit2.html#cause-1"
  },"39": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Solution",
    "content": "OMT automatically restarts RabbitMQ if it fails to start twice. Therefore, first wait 15 minutes, and then check if the issue was resolved automatically. If the issue still exists, check the system resources and network connectivity. If there are no resource or network issues, manually restart RabbitMQ. To do this, follow these steps: . | Run the following command on a master node (embedded Kubernetes) or the bastion node (managed Kubernetes) to stop RabbitMQ: . kubectl scale statefulset infra-rabbitmq -n &lt;Service Management namespace&gt; --replicas=0 . | Wait until all RabbitMQ pods have stopped. | Remove the rabbitmq-infra-rabbitmq-&lt;n&gt;/data/xservices/rabbitmq/x.x.x.xx/mnesia folders from the NFS server (or the bastion node if you use a managed NFS, including: EFS, Azure NetApp Files, and Filestore). For example, remove the following folders: . | /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-0/data/xservices/rabbitmq/x.x.x.xx/mnesia | /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-1/data/xservices/rabbitmq/x.x.x.xx/mnesia | /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-2/data/xservices/rabbitmq/x.x.x.xx/mnesia | . | Restart RabbitMQ. To do this, run the following command on a master node (embedded Kubernetes) or on the bastion node (managed Kubernetes): . kubectl scale statefulset infra-rabbitmq -n &lt; Service Management namespace&gt; --replicas=3 . | . ",
    "url": "/pages/edit/edit2.html#solution-1",
    
    "relUrl": "/pages/edit/edit2.html#solution-1"
  },"40": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Sample 2: Troubleshooting topic 2",
    "content": " ",
    "url": "/pages/edit/edit2.html",
    
    "relUrl": "/pages/edit/edit2.html"
  },"41": {
    "doc": "Sample 3: Integration guide",
    "title": "Editing sample 3: Integration guide",
    "content": "On this page . | ORIGINAL TEXT: Integrate with cloud platforms for CMP FinOps | EDITED TEXT: Integrate with cloud platforms to enable CMP FinOps | . Source: Service Management 25.1 documentation. Summary of issues . | Clarity and structure | The topic is a series of lists, which obscure what should be a simple message: the product relies on 2 types of data, and each type of data is retrieved a different way. | . | Non-native language mistakes | “Firstly you need to…” | . ",
    "url": "/pages/edit/edit3.html#editing-sample-3-integration-guide",
    
    "relUrl": "/pages/edit/edit3.html#editing-sample-3-integration-guide"
  },"42": {
    "doc": "Sample 3: Integration guide",
    "title": "ORIGINAL TEXT: Integrate with cloud platforms for CMP FinOps",
    "content": "To use the Cloud Management Platform (CMP) FinOps solution, firstly you need to retrieve cloud data from the cloud platforms that we support: Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP). The CMP FinOps solution uses two types of cloud data: . | Cloud billing data - The CMP FinOps’ cloud cost visualization and optimization functionalities, including cost governance reports, cloud insights, and cloud cost limits, are all based on the retrieved billing data. | Cloud account data - The CMP FinOps account management functionality enables you to centrally manage cloud accounts of supported cloud platforms. | . You need to set up two types of integrations to retrieve the cloud data: . | Cloud Cost Data Provider-based integrations that retrieve cloud billing data | Integration Studio-based integrations that import cloud account data | . See these topics for how to set up the integrations to retrieve cloud data from each cloud platform: . | Integrate with AWS for CMP FinOps | Integrate with Azure for CMP FinOps | Integrate with GCP for CMP FinOp | . ",
    "url": "/pages/edit/edit3.html#original-text-integrate-with-cloud-platforms-for-cmp-finops",
    
    "relUrl": "/pages/edit/edit3.html#original-text-integrate-with-cloud-platforms-for-cmp-finops"
  },"43": {
    "doc": "Sample 3: Integration guide",
    "title": "Related topics",
    "content": ". | Integration Studio | Cloud cost data providers | Get started with CMP FinOps | . ",
    "url": "/pages/edit/edit3.html#related-topics",
    
    "relUrl": "/pages/edit/edit3.html#related-topics"
  },"44": {
    "doc": "Sample 3: Integration guide",
    "title": "EDITED TEXT: Integrate with cloud platforms to enable CMP FinOps",
    "content": "Cloud Management Platform (CMP) FinOps uses two types of data: cloud account data and cloud cost data. Before you can use CMP FinOps, you must set up two integrations with your cloud provider to retrieve this data. ",
    "url": "/pages/edit/edit3.html#edited-text-integrate-with-cloud-platforms-to-enable-cmp-finops",
    
    "relUrl": "/pages/edit/edit3.html#edited-text-integrate-with-cloud-platforms-to-enable-cmp-finops"
  },"45": {
    "doc": "Sample 3: Integration guide",
    "title": "Cloud account data",
    "content": "CMP FinOps uses cloud account data to help you manage and configure your cloud accounts. You can set up the integration to retrieve cloud account data using Integration Studio, Service Management’s built-in integration platform. ",
    "url": "/pages/edit/edit3.html#cloud-account-data",
    
    "relUrl": "/pages/edit/edit3.html#cloud-account-data"
  },"46": {
    "doc": "Sample 3: Integration guide",
    "title": "Cloud cost data",
    "content": "CMP FinOps uses cloud cost data to provide cost visualizations and suggest improvements. This includes cloud cost governance reports, cloud insights, and cloud cost limits. You can’t set up an integration to retrieve cloud cost data using Integration Studio, as CMP FinOps retrieves this data differently for each cloud provider. You must instead set up a Cloud Cost Data Provider in the Admin &amp; Providers section of Service Management. ",
    "url": "/pages/edit/edit3.html#cloud-cost-data",
    
    "relUrl": "/pages/edit/edit3.html#cloud-cost-data"
  },"47": {
    "doc": "Sample 3: Integration guide",
    "title": "Related topics",
    "content": ". | To learn how to set up these integrations with AWS, see Integrate with AWS for CMP FinOps. | To learn how to set up these integrations with Azure, see Integrate with Azure for CMP FinOps. | To learn how to set up these integrations with GCP, see Integrate with GCP for CMP FinOp. | For general information about Integration Studio, see Integration Studio. | For general information about Cloud Cost Data Providers, see Cloud cost data providers. | For an introduction to FinOps, see Get started with CMP FinOps. | . ",
    "url": "/pages/edit/edit3.html#related-topics-1",
    
    "relUrl": "/pages/edit/edit3.html#related-topics-1"
  },"48": {
    "doc": "Sample 3: Integration guide",
    "title": "Sample 3: Integration guide",
    "content": " ",
    "url": "/pages/edit/edit3.html",
    
    "relUrl": "/pages/edit/edit3.html"
  },"49": {
    "doc": "Edit an agreement model",
    "title": "Edit an agreement model",
    "content": "When you edit an agreement model, the changes have no effect on existing service level records. However, they do affect all new service level records created after the edit. You can edit multiple records simultaneously by selecting them in the grid and updating them in the Preview pane on the right. For more information, see Mass update. | From the main menu, select Plan &gt; Service Level &gt; Agreement Models. | In the ID column, click the number of the agreement model that you want to edit. | Edit the General model details section as required. | Field | Description | . | Title | A meaningful, descriptive, and relatively short name for the model. The model name is often the only identifier displayed in selection lists. | . | Description | A description that summarizes the model details. | . | Agreement flavor | You can't edit this field once the model is created. | . | Agreement type | You can't edit this field once the model is created. | . | Click Attachments &gt; Add attachment to upload a file to the agreement model. Supported attachment file formats and the maximum file size are defined in the tenant’s Application settings tab in Suite Administration. If the Attachments field of agreement models is encrypted and you are a member of an encryption domain, you can click Add encrypted attachments to attach an encrypted file to the record. Attachments to models aren’t visible in the Service Portal. | Click the Workflow tab to view and update the model’s workflow. Note that to update the workflow status, you must have the appropriate rights. | Click the Approvals tab to add approvals to the model. For more information about approvals, see Task plans. | Click the Default values tab to update the values that you want to include in the model. Details . | Field | Description | . | Title | A unique word or phrase that's an identifier for the agreement records created with this model. This should be a value that makes it easy for end users to understand the purpose of the agreement record. Example: Initial review for priority 1 incidents . | . | Default agreement | This option applies this model by default when end users create agreement records. | . | Owner | The Service Level Agreement owner for records created with this model. If you leave this field empty, the Service Level Agreement owner defaults to the current user when end users create a record with this model. Example: Service Level Manager. | . | Technical group | The technical group to which records created with this model are assigned. | . | Financial group | The financial group to which records created with this model are assigned. | . | Description | A description of the records created with this model. | . Requirement . | Field | Description | . | Cost | The cost of agreement records created with this model. | . | Effort | The effort associated with agreement records created with this model. | . | Validity start date | The start date of the period of validity of agreement records created with this model. | . | Validity end date | The end date of the period of validity of agreement records created with this model. | . Review and improvement . | Field | Description | . | Next review date | The date when agreement records created with this model must next be reviewed. | . | Service quality report | The service quality report associated with agreement records created with this model. | . | Improvement measures | The improvement measures associated with agreement records created with this model. | . | Service improvement plan | The service improvement plan associated with agreement records created with this model. | . | Click the Discussions tab to view any relevant conversations about the model. For more information about discussions, see Discussions. | To view changes or updates made to the record, click the History tab. For more information, see History. | Click Save. | . ",
    "url": "/pages/write/edit_agreement_model.html",
    
    "relUrl": "/pages/write/edit_agreement_model.html"
  },"50": {
    "doc": "Edit an agreement model",
    "title": "Related topics",
    "content": ". | Create an agreement model | Retire an agreement model | . ",
    "url": "/pages/write/edit_agreement_model.html#related-topics",
    
    "relUrl": "/pages/write/edit_agreement_model.html#related-topics"
  },"51": {
    "doc": "Audit of FinOps documentation",
    "title": "Audit of FinOps documentation",
    "content": " ",
    "url": "/pages/design/finops_audit.html",
    
    "relUrl": "/pages/design/finops_audit.html"
  },"52": {
    "doc": "Integrate with AWS for CMP FinOps",
    "title": "Integrate with AWS for CMP FinOps",
    "content": "On this page . | Complete prerequisite tasks in AWS . | Set up IAM user and permissions | Enable rightsizing recommendations | Configure data export | . | Set up an integration via Integration Studio to import cloud accounts . | Prepare an integration user | Configure an endpoint | Create an integration | Configure a scenario | Schedule the import of cloud accounts | Check the scenario execution status | . | Configure a cloud cost data provider to retrieve billing data . | Create a cost reporting integration | . | . Before you can use the Cloud Management Platform (CMP) FinOps solution, you must set up integrations with the cloud platforms from which you want to pull cloud data. We support integrations with Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP). The CMP FinOps solution uses three types of cloud data. You will need to set up a separate integration to retrieve Cloud Account Data, but for Cloud Billing and Cloud Recommendations, this data is retrieved within the same integration. | Cloud account data – The CMP FinOps account management functionality enables you to centrally manage cloud accounts of supported cloud platforms. You must set up an Integration Studio-based integration to import cloud account data. | Cloud billing data – The CMP FinOps’ cloud cost visualization and optimization functionalities, including cost governance reports, cloud insights, and cloud cost limits, are all based on cloud billing data. You must set up a Cloud Cost Data Provider-based integration to retrieve cloud billing data. | Cloud recommendations data - The CMP FinOps’ Optimization functionalities, including cloud insights, are based on cloud recommendations that are collected from the different cloud providers. You must set up a Cloud Cost Data Provider-based integration to retrieve these cloud recommendations. Setting up a Cloud Cost Data Provider-based integration with the appropriate permissions (as described below) will give you both cloud billing data and cloud recommendations data within the same integration. | . This topic describes how to set up the two AWS integrations that support the CMP FinOps capability: the cloud accounts import integration and the billing data and recommendations retrieval integration. ",
    "url": "/pages/write/finops_aws.html",
    
    "relUrl": "/pages/write/finops_aws.html"
  },"53": {
    "doc": "Integrate with AWS for CMP FinOps",
    "title": "Complete prerequisite tasks in AWS",
    "content": "To integrate with AWS, you must complete the following tasks in AWS at the management account level. Set up IAM user and permissions . Generate the security credentials required by the integrations and then grant the required permissions to the corresponding user: . | Log in to the AWS management account as an administrator or the root user. | Navigate to the IAM service and then open the Policies page. | Add a policy with the following services and permissions: . | Service | Permissions | . | Organizations | DescribeOrganizationDescribeAccountListAccounts | . | Cost Explorer Service | ce:Describe*ce:List*ce:Get*ce:StartSavingsPlansPurchaseRecommendationGeneration | . | Savings Plans | savingsplans:Describe*savingsplans:List* | . | EC2 | ec2:DescribeReservedInstancesec2:DescribeReservedInstancesOfferingsec2:DescribeInstancesec2:DescribeVolumesec2:DescribeRegions | . | Trusted Advisor | trustedadvisor:Describe*trustedadvisor:List*trustedadvisor:Get*trustedadvisor:DownloadRisk | . | Compute Optimizer | compute-optimizer:Get*compute-optimizer:Describe* | . | Cost and Usage Report | cur:DescribeReportDefinitions | . | Elastic Container Service | ecs:ListServicesecs:ListClusters | . | Lambda | lambda:ListProvisionedConcurrencyConfigslambda:ListFunctions | . * You’ll need this policy for the cloud accounts import integration. | Navigate to the Users page, and then create an IAM user. | Make sure that the user’s access type is programmatic access. | Enter some meaningful tags for the user. The CMP FinOps capability will make use of this information. | . | Attach the policy added earlier and the following AWS managed policies to an appropriate principal entity (user, group, or role), depending on how you manage user permissions in AWS: . | AmazonEC2ReadOnlyAccess | AmazonS3ReadOnlyAccess | AWSOrganizationsReadOnlyAccess | AWSPriceListServiceFullAccessIf you attach the policies to a group or role, assign the created IAM user to the group or assign the role to the user. | . | Note down the security credentials (Access key ID and Secret access key) for the user. You will need to enter the credentials when configuring the billing data and cloud accounts import integrations. | . For more information, see the AWS documentation. Enable rightsizing recommendations . | Log in to the AWS management account as an administrator or the root user. | Navigate to the AWS Billing and Cost Management console, and then open the Cost Management Preferences page. | On the General tab, select the Rightsizing Recommendations and Enable Allow linked accounts to see recommendations options. | Save your changes. | . For more information, see the AWS Rightsizing Recommendations. Configure data export . To enable the billing data retrieval integration, you need to configure a legacy CUR type data export by following the instructions in the AWS documentation. Once configured, AWS will automatically generate reports and store them in the designated S3 bucket. The integration will then import these reports for further processing. This table lists the essential parameters required during the report configuration process. For other parameters, configure values that align with your specific requirements. | Configuration parameter | Required value | . | Export type | Legacy CUR export | . | Include resource IDs | Select this option | . | Split cost allocation data | Select this option | . | Refresh automatically | Select this option | . | Report data time granularity | Daily | . | Report versioning | Create new report version | . | Report data integration | Don’t select any option | . | Compression type | GZIP | . Note down the S3 bucket name and the report path prefix displayed on the Review page of the report creation wizard. You will need these values later when you configure the cloud cost data provider. After configuring the cost usage report, verify that you can generate the report in this directory of the S3 bucket: &lt;bucket name&gt;\\&lt;report path prefix&gt;. The actual report path will contain an actual date range instead of the date-range value. Note that it might take longer than 24 hours to generate reports in the S3 bucket after you configure the cost and usage report. ",
    "url": "/pages/write/finops_aws.html#complete-prerequisite-tasks-in-aws",
    
    "relUrl": "/pages/write/finops_aws.html#complete-prerequisite-tasks-in-aws"
  },"54": {
    "doc": "Integrate with AWS for CMP FinOps",
    "title": "Set up an integration via Integration Studio to import cloud accounts",
    "content": "This section describes how to set up an integration based on Integration Studio for AWS cloud accounts import. Prepare an integration user . Create a dedicated integration user for each integration. | Log in to Suite Administration (https://&lt;External Access Host&gt;/bo) with Suite Admin credentials. | Create an integration user with the Integration user role. | Log in to Agent Interface with Tenant Admin credentials. | Navigate to Administration &gt; Master Data &gt; People &gt; Roles, and create a role (with the Application value SMAX) with the Create, View, and Update permissions for the Cloud Account record type. | Navigate to Administration &gt; Master Data &gt; People, locate the integration user, and then assign the created role to the user. | . Configure an endpoint . Create an endpoint using the Rest Executor 2.0 endpoint type: . | Log in to Agent Interface as the cloud integration administrator. | Navigate to Administration &gt; Utilities &gt; Integration &gt; Endpoints. | Click Add. | Select Rest Executor 2.0 as the endpoint type, enter a name for the endpoint (for example, AWS), and then click Add. The system creates the endpoint. | Click Configure and set the following fields as described: . | Field | Value | . | Agent | Select Agentless | . | Authentication type | Select AWS Signature | . | Access key ID | Enter the access key ID obtained in the prerequisite section | . | Secret access key | Enter the secret access key obtained in the prerequisite section | . | Session token | Leave the field empty | . | Certificate type | Leave the field empty | . | Certificate format | Leave the field empty | . | Server certificate | Leave the field empty | . | Click Test connection to validate the configuration. | Click Save. | . Create an integration . In the following steps, you’ll create an integration with the predefined template. | Navigate to Administration &gt; Utilities &gt; Integration &gt; Integration studio. | Click New, enter the name of your integration, and then click Save. The system displays the Integration page. | In the Details section, configure the integration user you created for the current SMAX system. | Select the Active option to activate the integration. | In the Connector-Endpoint mappings section, click Add to add a mapping. | Select Management &amp; Governance(under Amazon Web Services) as the connector, select the AWS endpoint that you created earlier, and then enter an alias for the mapping. | Click Save. | . Configure a scenario . Perform the following steps to add and configure a scenario: . | With the integration selected, click Add scenario. | Enter a name for the scenario. For example, AWS account sync. | In the Template field, select AWS accounts sync process (under Amazon Web Services Management &amp; Governance). | Click Save to add the scenario. | Manually run the scenario. If you don’t do this, CMP FinOps functionality that depends on cloud account data won’t work as expected until the first scheduled cloud account sync occurs. | . Schedule the import of cloud accounts . Use the scheduler to periodically import cloud accounts from the cloud platform. Create a schedule that executes the cloud accounts import scenario at the scheduled time (for example, trigger the import at 2:00 every day). To optimize system performance, schedule cloud account imports with intervals of at least 24 hours. Check the scenario execution status . To check the scenario execution status, navigate to Administration &gt; Utilities &gt; Integration &gt; Endpoints, and then select the AWS endpoint in the left pane. The system displays the execution history for the AWS integration. Each record displayed in this grid corresponds to one scenario rule. Make sure all rules in the cloud accounts import scenario have the green Success status. | You can check the start and end time (and duration) for two main activities of the scenario: cloud accounts syncing and stale accounts status setting. To do this, click the hyperlinked ID for these rules (Capture sync time and Clean up sync time end), and check the value in the Output details field. The field displays both the start time and end time for the corresponding activity. | The Connector column indicates the system where the action of the rule is applied: Management &amp; Governance (AWS), SMAX, or Common. Unlike other values, Common means that this rule doesn’t call any APIs, instead, it just sets some values or performs some calculations. | Records whose Connector value isn’t Common contain detailed response data in the Response details field. Check this field for error codes or error messages for records with a warning or error status. | If you find an error code or message in error or warning records for AWS rules (with the Connector value Amazon Web Services Management &amp; Governance), check the Amazon documentation to see if you can find more information about the error. | . ",
    "url": "/pages/write/finops_aws.html#set-up-an-integration-via-integration-studio-to-import-cloud-accounts",
    
    "relUrl": "/pages/write/finops_aws.html#set-up-an-integration-via-integration-studio-to-import-cloud-accounts"
  },"55": {
    "doc": "Integrate with AWS for CMP FinOps",
    "title": "Configure a cloud cost data provider to retrieve billing data",
    "content": "With this integration, you can configure retrieval of historical and current billing information for expenditures associated with your cloud provider. This information will be available within the reporting interface. Create a cost reporting integration . | Log in to Agent Interface as the cloud integration administrator. | Go to Administration &gt; Providers &gt; Cloud Cost Data Providers. | On the Integrations page, click either Add Integration (if no integrations exist) or the icon on the right pane to add an additional integration. | Select Amazon AWS and click Next. | Specify these fields: . | Connection Name: Identifier for the AWS integration, for example, “My AWS DEV Account”. | Access Key ID: AWS access key obtained in the prerequisite section | Secret Access Key: AWS secret access key obtained in the prerequisite section | Billing Bucket Name: The name of the S3 bucket that stores billing data and that you designated when you configured the data export in AWS. | Billing Report Path: The report path prefix (excluding the /date-range/ value at the end) that you specified when you configured the data export in AWS. | . If you forgot to make a note of the billing bucket name and billing report path when you configured the data export, follow these steps to identify them: . | Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/. | In the left navigation pane, select Buckets, and then navigate to the bucket associated with the integration. | Click the Copy S3 URI button, and then paste the URI to a text editor. The URI is formatted as follows: . s3://&lt;billing bucket name&gt;/&lt;billing report path&gt; . Important: The billing report path doesn’t include the forward slash that’s between it and the billing bucket name. For example, if the S3 URI is s3://bucket3///reportPath1/, the billing report path is //reportPath1/ (with one less forward slash). | . | Check the I require a proxy server to connect to this provider box to use a proxy server to connect to the cloud provider. You can either use the system default proxy server (configured by a suite admin) or use a custom proxy server here. | Leave the I require a custom base URL to connect to this provider checkbox unchecked to contact the cloud provider at its default endpoint. | Click Next. | Specify the following information: . | Collection History Cutoff: By default, the integration will fetch all available billing data from the cloud provider. For AWS, Amazon introduced CUR in 2015, and the availability of data depends on the creation date of the configured CUR reports. If, in either case, you wish to limit the amount of data, select a collection start date so that your collection only gathers data from the specified date. For example, selecting January 2020 will fetch data from that point to the present day and into the future. There is no end date. | Cloud Tags: To enable cloud tags for the selected provider, specify the name and key. Cloud tags help you to filter, search, and manage the AWS resources. For more information, see Configure Cloud Tags. | . | Click Finish. | . After you create the integration, a full data collection (including historical data) will occur. This may result in a longer-than-usual data collection time, and will cause issues if the collection is still running when the next scheduled collection is due to start. To avoid this issue, we strongly recommend that you disable the integration until the initial full data collection is complete. You can then enable the integration again to resume scheduled data collection. To do this, click the toggle switch under COLLECTION SETTINGS so that it says Collection is disabled. When the full data collection is complete, click the toggle switch again so that it says Collection is enabled. ",
    "url": "/pages/write/finops_aws.html#configure-a-cloud-cost-data-provider-to-retrieve-billing-data",
    
    "relUrl": "/pages/write/finops_aws.html#configure-a-cloud-cost-data-provider-to-retrieve-billing-data"
  },"56": {
    "doc": "Integrate with AWS for CMP FinOps",
    "title": "Related topics",
    "content": ". | For general information about the Integration Studio, see Integration Studio. | For information on triggering integration scenarios based on scheduler, see Scheduler. | For information on how to export and import integrations and scenarios, see Export and import integrations. | For information on Cost and Usage Report, see the AWS documentation Getting Started with CUR. | For information on cloud tags, see Configure cloud tags. | . ",
    "url": "/pages/write/finops_aws.html#related-topics",
    
    "relUrl": "/pages/write/finops_aws.html#related-topics"
  },"57": {
    "doc": "Integrate with Azure for CMP FinOps",
    "title": "Integrate with Azure for CMP FinOps",
    "content": "On this page . | Complete prerequisite tasks in AWS . | Set up IAM user and permissions | Enable rightsizing recommendations | Configure data export | . | Set up an integration via Integration Studio to import cloud accounts . | Prepare an integration user | Configure endpoint | Create an integration | Configure a scenario . | Use IdleDaysToCleanUp to control the stale cloud account cleanup duration | . | Schedule the import of cloud accounts | Check the scenario execution status | Troubleshooting tips | . | Configure a cloud cost data provider to retrieve billing data . | Create a cost reporting integration | . | . Before you can use the Cloud Management Platform (CMP) FinOps solution, you must set up integrations with the cloud platforms from which you want to pull cloud data. We support integrations with Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP). The CMP FinOps solution uses three types of cloud data. You will need to set up a separate integration to retrieve Cloud Account Data, but for Cloud Billing and Cloud Recommendations, this data is retrieved within the same integration. | Cloud account data – The CMP FinOps account management functionality enables you to centrally manage cloud accounts of supported cloud platforms. You must set up an Integration Studio-based integration to import cloud account data. | Cloud billing data – The CMP FinOps’ cloud cost visualization and optimization functionalities, including cost governance reports, cloud insights, and cloud cost limits, are all based on cloud billing data. You must set up a Cloud Cost Data Provider-based integration to retrieve cloud billing data. | Cloud recommendations data - The CMP FinOps’ Optimization functionalities, including cloud insights, are based on cloud recommendations that are collected from the different cloud providers.  You must set up a Cloud Cost Data Provider-based integration to retrieve these cloud recommendations. Setting up a Cloud Cost Data Provider-based integration with the appropriate permissions (as described below) will give you both cloud billing data and cloud recommendations data within the same integration. | . This topic describes how to set up the two AWS integrations that support the CMP FinOps capability: the cloud accounts import integration and the billing data and recommendations retrieval integration. ",
    "url": "/pages/write/finops_azure.html",
    
    "relUrl": "/pages/write/finops_azure.html"
  },"58": {
    "doc": "Integrate with Azure for CMP FinOps",
    "title": "Complete prerequisite tasks in AWS",
    "content": "To integrate with AWS, you must complete the following tasks in AWS at the management account level. Set up IAM user and permissions . Generate the security credentials required by the integrations and then grant the required permissions to the corresponding user: . | Log in to the AWS management account as an administrator or the root user. | Navigate to the IAM service and then open the Policies page. | Add a policy with the following services and permissions: . Service . Permissions . Organizations . | DescribeOrganization | DescribeAccount | ListAccounts | . Cost Explorer Service . | ce:Describe* | ce:List* | ce:Get* | ce:StartSavingsPlansPurchaseRecommendationGeneration | . Savings Plans . | savingsplans:Describe* | savingsplans:List* | . EC2 . | ec2:DescribeReservedInstances | ec2:DescribeReservedInstancesOfferings | ec2:DescribeInstances | ec2:DescribeVolumes | ec2:DescribeRegions | . Trusted Advisor . | trustedadvisor:Describe* | trustedadvisor:List* | trustedadvisor:Get* | trustedadvisor:DownloadRisk | . Compute Optimizer . | compute-optimizer:Get* | compute-optimizer:Describe* | . Cost and Usage Report . | cur:DescribeReportDefinitions | . Elastic Container Service . | ecs:ListServices | ecs:ListClusters | . Lambda . | lambda:ListProvisionedConcurrencyConfigs | lambda:ListFunctions | . You’ll need this policy for the cloud accounts import integration. | Navigate to the Users page, and then create an IAM user. | Make sure that the user’s access type is programmatic access. | Enter some meaningful tags for the user. The CMP FinOps capability will make use of this information. | . | Attach the policy added earlier and the following AWS managed policies to an appropriate principal entity (user, group, or role), depending on how you manage user permissions in AWS: . | AmazonEC2ReadOnlyAccess | AmazonS3ReadOnlyAccess | AWSOrganizationsReadOnlyAccess | AWSPriceListServiceFullAccessIf you attach the policies to a group or role, assign the created IAM user to the group or assign the role to the user. | . | Note down the security credentials (Access key ID and Secret access key) for the user. You will need to enter the credentials when configuring the billing data and cloud accounts import integrations. | . For details, see the AWS documentation. Enable rightsizing recommendations . | Log in to the AWS management account as an administrator or the root user. | Navigate to the AWS Billing and Cost Management console, and then open the Cost Management Preferences page. | On the General tab, select the Rightsizing Recommendations and Enable Allow linked accounts to see recommendations options. | Save your changes. | . For more information, see AWS Rightsizing Recommendations. Configure data export . To enable the billing data retrieval integration, you need to configure a legacy CUR type data export by following the instructions in the AWS documentation. Once configured, AWS will automatically generate reports and store them in the designated S3 bucket. The integration will then import these reports for further processing. This table lists the essential parameters required during the report configuration process. For other parameters, configure values that align with your specific requirements. Configuration parameter . Required value . Export type . Legacy CUR export . Include resource IDs . Select the option . Split cost allocation data . Select the option . Refresh automatically . Select the option . Report data time granularity . Daily . Report versioning . Create new report version . Report data integration . Don’t select any option . Compression type . GZIP . Note down the S3 bucket name and the report path prefix displayed on the Review page of the report creation wizard. You will need these values later when you configure the cloud cost data provider. After configuring the cost usage report, verify that you can generate the report in this directory of the S3 bucket: &lt;bucket name&gt;\\&lt;report path prefix&gt;. The actual report path will contain an actual date range instead of the date-range value. Note that it might take longer than 24 hours to generate reports in the S3 bucket after you configure the cost and usage report. ",
    "url": "/pages/write/finops_azure.html#complete-prerequisite-tasks-in-aws",
    
    "relUrl": "/pages/write/finops_azure.html#complete-prerequisite-tasks-in-aws"
  },"59": {
    "doc": "Integrate with Azure for CMP FinOps",
    "title": "Set up an integration via Integration Studio to import cloud accounts",
    "content": "This section describes how to set up an integration based on Integration Studio for AWS cloud accounts import. Prepare an integration user . Section begin - CreateIntUser . Create a dedicated integration user for each integration.  . | Log in to Suite Administration (https:///bo) as suite admin and create an integration user with the **Integration user** role for the integration. | Log in to Agent Interface as the tenant admin and do the following: . | Navigate to Administration &gt; Master Data &gt; People &gt; Roles, and create a role (with the Application value SMAX) with the Create, View, and Update permissions for the Cloud Account record type.  | Navigate to Administration &gt; Master Data &gt; People, locate the integration user, and then assign the created role to the user. | . | . Section end - CreateIntUser . Configure endpoint . Create an endpoint using the Rest Executor 2.0 endpoint type: . | Log in to Agent Interface as the cloud integration administrator. | Navigate to Administration &gt; Utilities &gt; Integration &gt; Endpoints. | Click Add. | Select Rest Executor 2.0 as the endpoint type, enter a name for the endpoint (for example, AWS), and then click Add. The system creates the endpoint. | Click Configure and set these fields: . Field . Value . Agent . Select Agentless. Authentication type . Select AWS Signature. Access key ID . Enter the access key ID obtained in the prerequisite section. Secret access key . Enter the secret access key obtained in the prerequisite section. Session token . Leave the field empty. Certificate type . Leave the field empty. Certificate format . Leave the field empty. Server certificate . Leave the field empty. | After completing the configuration, click Test connection to validate the configuration. | Click Save. | . Create an integration . In the following steps, you’ll create an integration with the predefined template. | Navigate to Administration &gt; Utilities &gt; Integration &gt; Integration studio.  | Click New, enter the name of your integration, and then click Save. The system displays the Integration page. | In the Details section, configure the integration user you created for the current SMAX system. | Select the Active option to activate the integration. | In the Connector-Endpoint mappings section, click Add to add a mapping. | Select Management &amp; Governance (under Amazon Web Services) as the connector, select the AWS endpoint that you created earlier, and then enter an alias for the mapping. | Click Save. | . Configure a scenario . Perform the following steps to add and configure a scenario: . | With the integration selected, click Add scenario.  | Enter a name for the scenario. For example, AWS account sync. | In the Template field, select AWS accounts sync process (under Amazon Web Services Management &amp; Governance). | Click Save to add the scenario. | (Optional) Expand the Set sync parameters rule in the Process logic section of the scenario, and update the value for the IdleDaysToCleanUp key. See below for the explanation of this parameter. | Click Save. | Manually run the scenario. If you don’t do this, CMP FinOps functionality that depends on cloud account data won’t work as expected until the first scheduled cloud account sync occurs. | . Use IdleDaysToCleanUp to control the stale cloud account cleanup duration . Service Management moves cloud accounts removed from the cloud platform to the Archived phase by using transition rules. To do this, it uses the “PENDING_CLOSURE” account status set by the integration scenario to decide what are the removed accounts. By default, the integration scenario sets the “PENDING_CLOSURE” status to cloud accounts removed more than 30 days ago. You can update the IdleDaysToCleanUp parameter’s value (in days) to change this duration. Section begin - scheduling . Schedule the import of cloud accounts . Use the scheduler to periodically import cloud accounts from the cloud platform. Create a schedule that executes the cloud accounts import scenario at the scheduled time (for example, trigger the import at 2:00 every day). To optimize system performance, schedule cloud account imports with intervals of at least 24 hours. Section end - scheduling . Check the scenario execution status . To check the scenario execution status, navigate to Administration &gt; Utilities &gt; Integration &gt; Endpoints, and then select the AWS endpoint in the left pane. The system displays the execution history for the AWS integration. Each record displayed in this grid corresponds to one scenario rule. Make sure all rules in the cloud accounts import scenario have the green Success status. | You can check the start and end time (and duration) for two main activities of the scenario: cloud accounts syncing and stale accounts status setting. To do this, click the hyperlinked ID for these rules (Capture sync time and Clean up sync time end), and check the value in the Output details field. The field displays both the start time and end time for the corresponding activity. | The Connector column indicates the system where the action of the rule is applied: Management &amp; Governance (AWS), SMAX, or Common. Unlike other values, Common means that this rule doesn’t call any APIs, instead, it just sets some values or performs some calculations. | Records whose Connector value isn’t Common contain detailed response data in the Response details field. Check this field for error codes or error messages for records with a warning or error status. | If you find an error code or message in error or warning records for AWS rules (with the Connector value Amazon Web Services Management &amp; Governance), check the Amazon documentation to see if you can find more information about the error. | . Troubleshooting tips . For rules that correspond to API calls (as opposed to Common actions), if the error code or message in the Response details field indicates connection or credential errors, take these actions: . | Click Run scenario in the Scenario details page to run the scenario again, and then check the execution history for that rule to see if it succeeds. Use this method to check if temporary network issues are the cause of the error. | Open the AWS endpoint’s configuration page, and click Test connection. If the test connection succeeds, it indicates that invalid AWS security credentials aren’t the cause of the error. | . ",
    "url": "/pages/write/finops_azure.html#set-up-an-integration-via-integration-studio-to-import-cloud-accounts",
    
    "relUrl": "/pages/write/finops_azure.html#set-up-an-integration-via-integration-studio-to-import-cloud-accounts"
  },"60": {
    "doc": "Integrate with Azure for CMP FinOps",
    "title": "Configure a cloud cost data provider to retrieve billing data",
    "content": "With this integration, you can configure retrieval of historical and current billing information for expenditures associated with your cloud provider. This information will be available within the reporting interface. Create a cost reporting integration . | Log in to Agent Interface as the cloud integration administrator. | Go to Administration &gt; Providers &gt; Cloud Cost Data Providers. | On the Integrations page, click either Add Integration (if no integrations exist) or the icon on the right pane to add an additional integration. | Select Amazon AWS and click Next. | Specify these fields: . | Connection Name: Identifier for the AWS integration, for example, “My AWS DEV Account”. | Access Key ID: AWS access key obtained in the prerequisite section | Secret Access Key: AWS secret access key obtained in the prerequisite section | Billing Bucket Name: The name of the S3 bucket that stores billing data and that you designated when you configured the data export in AWS. | Billing Report Path: The report path prefix (excluding the /date-range/ value at the end) that you specified when you configured the data export in AWS. | . If you forgot to make a note of the billing bucket name and billing report path when you configured the data export, follow these steps to identify them: . | Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/. | In the left navigation pane, select Buckets, and then navigate to the bucket associated with the integration. | Click the Copy S3 URI button, and then paste the URI to a text editor. The URI is formatted as follows: . s3://&lt;billing bucket name&gt;/&lt;billing report path&gt; . Important: The billing report path doesn’t include the forward slash that’s between it and the billing bucket name. For example, if the S3 URI is s3://bucket3///reportPath1/, the billing report path is //reportPath1/ (with one less forward slash). | . | Check the I require a proxy server to connect to this provider box to use a proxy server to connect to the cloud provider. You can either use the system default proxy server (configured by a suite admin) or use a custom proxy server here. | Leave the I require a custom base URL to connect to this provider checkbox unchecked to contact the cloud provider at its default endpoint. | Click Next. | Specify the following information: . | Collection History Cutoff: By default, the integration will fetch all available billing data from the cloud provider. For AWS, Amazon introduced CUR in 2015, and the availability of data depends on the creation date of the configured CUR reports. If, in either case, you wish to limit the amount of data, select a collection start date so that your collection only gathers data from the specified date. For example, selecting January 2020 will fetch data from that point to the present day and into the future. There is no end date. | Cloud Tags: To enable cloud tags for the selected provider, specify the name and key. Cloud tags help you to filter, search, and manage the AWS resources. For more information, see Configure Cloud Tags. | . | Click Finish. | . After you create the integration, a full data collection (including historical data) will occur. This may result in a longer-than-usual data collection time, and will cause issues if the collection is still running when the next scheduled collection is due to start. To avoid this issue, we strongly recommend that you disable the integration until the initial full data collection is complete. You can then enable the integration again to resume scheduled data collection. To do this, click the toggle switch under COLLECTION SETTINGS so that it says Collection is disabled. When the full data collection is complete, click the toggle switch again so that it says Collection is enabled. ",
    "url": "/pages/write/finops_azure.html#configure-a-cloud-cost-data-provider-to-retrieve-billing-data",
    
    "relUrl": "/pages/write/finops_azure.html#configure-a-cloud-cost-data-provider-to-retrieve-billing-data"
  },"61": {
    "doc": "Integrate with Azure for CMP FinOps",
    "title": "Related topics",
    "content": ". | For general information about the Integration Studio, see Integration Studio. | For information on triggering integration scenarios based on scheduler, see Scheduler. | For information on how to export and import integrations and scenarios, see Export and import integrations. | For information on Cost and Usage Report, see the AWS documentation Getting Started with CUR. | For information on cloud tags, see Configure cloud tags. | . ",
    "url": "/pages/write/finops_azure.html#related-topics",
    
    "relUrl": "/pages/write/finops_azure.html#related-topics"
  },"62": {
    "doc": "Integrate with cloud platforms to enable CMP FinOps",
    "title": "Integrate with cloud platforms to enable CMP FinOps",
    "content": "On this page . | Cloud account data | Cloud cost data | . Cloud Management Platform (CMP) FinOps uses two types of data: cloud account data and cloud cost data. Before you can use CMP FinOps, you must set up two integrations with your cloud provider to retrieve this data. ",
    "url": "/pages/write/finops_landing.html",
    
    "relUrl": "/pages/write/finops_landing.html"
  },"63": {
    "doc": "Integrate with cloud platforms to enable CMP FinOps",
    "title": "Cloud account data",
    "content": "CMP FinOps uses cloud account data to help you manage and configure your cloud accounts. You can set up the integration to retrieve cloud account data using Integration Studio, Service Management’s built-in integration platform. ",
    "url": "/pages/write/finops_landing.html#cloud-account-data",
    
    "relUrl": "/pages/write/finops_landing.html#cloud-account-data"
  },"64": {
    "doc": "Integrate with cloud platforms to enable CMP FinOps",
    "title": "Cloud cost data",
    "content": "CMP FinOps uses cloud cost data to provide cost visualizations and suggest improvements. This includes cloud cost governance reports, cloud insights, and cloud cost limits. You can’t set up an integration to retrieve cloud cost data using Integration Studio, as CMP FinOps retrieves this data differently for each cloud provider. You must instead set up a Cloud Cost Data Provider in the Admin &amp; Providers section of Service Management. ",
    "url": "/pages/write/finops_landing.html#cloud-cost-data",
    
    "relUrl": "/pages/write/finops_landing.html#cloud-cost-data"
  },"65": {
    "doc": "Integrate with cloud platforms to enable CMP FinOps",
    "title": "Related topics",
    "content": ". | To learn how to set up these integrations with AWS, see Integrate with AWS for CMP FinOps. | To learn how to set up these integrations with Azure, see Integrate with Azure for CMP FinOps. | To learn how to set up these integrations with GCP, see Integrate with GCP for CMP FinOp. | For general information about Integration Studio, see Integration Studio. | For general information about Cloud Cost Data Providers, see Cloud cost data providers. | For an introduction to FinOps, see Get started with CMP FinOps. | . ",
    "url": "/pages/write/finops_landing.html#related-topics",
    
    "relUrl": "/pages/write/finops_landing.html#related-topics"
  },"66": {
    "doc": "Integrate with OCI for CMP FinOps",
    "title": "Integrate with OCI for CMP FinOps",
    "content": "On this page . | Complete prerequisite tasks in OCI . | Create a user and generate a secret key | Set up and configure a group | Create a policy | . | Set up a Cloud Cost Data Provider-based integration | . Before you can use the Cloud Management Platform (CMP) FinOps solution, you must set up integrations with the cloud platforms from which you want to pull cloud data. We support integrations with Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI). The CMP FinOps solution currently supports only the collection of cloud billing data from OCI (integrations with AWS, Azure, and GCP also support the collection of cloud account and cloud recommendations data). This topic describes how to set up a Cloud Cost Data Provider-based integration to retrieve cloud billing data from OCI. To view this data, you will need to set up an integration with Power BI. ",
    "url": "/pages/write/finops_oci.html",
    
    "relUrl": "/pages/write/finops_oci.html"
  },"67": {
    "doc": "Integrate with OCI for CMP FinOps",
    "title": "Complete prerequisite tasks in OCI",
    "content": "OCI generates Cost and Usage (CUR) reports, which contain the cloud billing data that CMP FinOps uses, by default. You can’t modify these reports. Therefore, you don’t have to set up or configure CUR reports to set up an integration with OCI. However, you do have to perform some initial setup to access the reports. Create a user and generate a secret key . OCI does not have service accounts. In order to grant CMP FinOps API access to the OCI cloud data, you must create a real user- no service users in OCI, so create a real user and use “customer secret key” to grant API access to CMP FinOps. | Log in to OCI Console as a member of the Administrators group. | In the navigation menu, click Identity &amp; Security &gt; Domains, and then select the identity domain that you want to work in. You might need to change the compartment to find the domain that you want. | Click Users &gt; Create user. | In the First name and Last name fields enter the user’s name. For example, you name this user “FinOps Admin”. | Leave the Use the email address as the username option selected. | In the Username / Email field, enter an email address for the user account, and then click Create. | In the left-hand Resources menu, click Customer secret keys, and then click Generate secret key. | Enter a name for the key, and then click Generate secret key. | Make a note of the values in the Name and Access key columns. You will need these values when configuring the Cloud Cost Data Provider-based integration. | . Set up and configure a group . | In the navigation menu, click Identity &amp; Security &gt; Domains, and then select the identity domain that you want to work in. You might need to change the compartment to find the domain that you want. | Click Groups &gt; Create group. | In the Name and Description fields of the Create group window, enter the name of the group (for example, “oci_finops”) and a description. | Add the user that you created earlier to the group. | Click Create. | . Create a policy . | In the navigation menu, click Identity &amp; Security &gt; Identity &gt; Create Policy. | In the Name and Description fields of the Create policy window, enter the name of the policy (for example, “oci_cur_reports”) and a description. | If you want to attach the policy to a compartment other than the one you’re viewing, select it from the Compartment list.  | Click Show manual editor, and then copy the following text to the Policy Builder text box. `define tenancy usage-report as ocid1.tenancy.oc1..aaaaaaaaned4fkpkisbwjlr56u7cj63lf3wffbilvqknstgtvzub7vhqkggq endorse group &lt;group&gt; to read objects in tenancy usage-report` . Update the &lt;group&gt; placeholder in this text with the name of the group that you created earlier. For example, “oci_finops”. | Click Create. | . ",
    "url": "/pages/write/finops_oci.html#complete-prerequisite-tasks-in-oci",
    
    "relUrl": "/pages/write/finops_oci.html#complete-prerequisite-tasks-in-oci"
  },"68": {
    "doc": "Integrate with OCI for CMP FinOps",
    "title": "Set up a Cloud Cost Data Provider-based integration",
    "content": "This integration enables you to retrieve historical and current cloud billing data for expenditures associated with your cloud provider. This information will be available within the reporting interface. | Log in to Agent Interface as the cloud integration administrator. | Go to Administration &gt; Providers &gt; Cloud Cost Data Providers. | On the Integrations page, click Add Integration (if no integrations exist) or the icon on the right pane (to add an additional integration). | Select Oracle Cloud Infrastructure, click NEXT, and then enter the following information: . Field . Description . Connection Name . Identifier for the OCI integration, for example, “My OCI”. Access Key ID . The name of the customer secret key that you created in OCI Console earlier. Secret Access Key . The access key of the customer secret key that you created in OCI Console earlier. Billing Bucket Name . The Oracle Cloud ID (OCID) of your root compartment. To see the OCID of all your compartments (including the root compartment), log in to OCI Console as a member of the Administrators group, and then click Identity &amp; Security &gt; Compartments in the navigation menu. Billing Report Path . Enter reports/cost-csv. Bucket Endpoint URL . Enter https://bling.compat.objectstorage.&lt;OCI region&gt;.oraclecloud.com.   . Replace the &lt;OCI region&gt; placeholder with your OCI home region. For example, you enter https://bling.compat.objectstorage.us-ashburn-1.oraclecloud.com. For information about OCI home regions, see Managing Regions. | Leave I require a proxy server to connect to this provider unselected if you want to use a proxy server to connect to the cloud provider. You can use either the system default proxy server (configured by a suite administrator) or a custom proxy server. | Select I require a custom base URL to connect to this provider if you don’t want to contact the cloud provider at its default endpoint. | Click NEXT. | (Optional) Specify the Collection History Cutoff. By default, all available cloud billing data is fetched from the cloud provider. If you wish to limit the amount of data, select a collection start date so that your collection only gathers data from the specified date. For example, selecting January 2022 will fetch data from that point to the present day and into the future. There is no end date. | Click FINISH. | . After you create the integration, a full data collection (including historical data) will occur. This may result in a longer-than-usual data collection time, and will cause issues if the collection is still running when the next scheduled collection is due to start. To avoid this issue, we strongly recommend that you disable the integration until the initial full data collection is complete. You can then enable the integration again to resume scheduled data collection. To do this, click the toggle switch under COLLECTION SETTINGS so that it says Collection is disabled. When the full data collection is complete, click the toggle switch again so that it says Collection is enabled. ",
    "url": "/pages/write/finops_oci.html#set-up-a-cloud-cost-data-provider-based-integration",
    
    "relUrl": "/pages/write/finops_oci.html#set-up-a-cloud-cost-data-provider-based-integration"
  },"69": {
    "doc": "Integrate with OCI for CMP FinOps",
    "title": "Related topics",
    "content": ". | For more information about cost and usage reports in OCI, see Cost and Usage Reports. | For more information about users in OCI, see Managing Users. | For more information about groups in OCI, see Managing Groups. | For more information about policies in OCI, see Managing Policies. | For more information about Power BI, see Integrate with Power BI to create FinOps reports. | . ",
    "url": "/pages/write/finops_oci.html#related-topics",
    
    "relUrl": "/pages/write/finops_oci.html#related-topics"
  },"70": {
    "doc": "Check the firewall settings",
    "title": "Check the firewall settings",
    "content": "On this page . | Add firewall rules for the inbound connections | Add firewall rules for the outbound connections | Enable VRRP protocol for Keepalived in a multiple master node deployment | . If you haven’t enabled a firewall, skip this topic. There are many firewall management tools to manage your firewall. For example, firewalld or iptables. The following steps are based on a firewalld managed firewall. If you are using another firewall management tool, contact your network administrator for detailed steps on how to add the related firewall rules. To check whether firewalld manages your firewall, run the following command: . systemctl status firewalld . If your terminal resembles the following, firewalld manages your firewall: . [root@sh ~\\]# systemctl status firewalld firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2020-06-22 04:36:32 CST; 2 weeks 0 days ago Docs: man:firewalld(1) Main PID: 878 (firewalld) Tasks: 2 CGroup: /system.slice/firewalld.service └─878 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid . ",
    "url": "/pages/write/firewall_settings.html",
    
    "relUrl": "/pages/write/firewall_settings.html"
  },"71": {
    "doc": "Check the firewall settings",
    "title": "Add firewall rules for the inbound connections",
    "content": "For the NFS servers . | Make sure the default policy for the INPUT chain is ACCEPT. To check the default policy, run the following command: . iptables -S | grep -- '-P INPUT' . If the default policy isn’t ACCEPT, contact your IT system administrator to change the policy. | Run the following commands on the NFS server: . systemctl start firewalld; systemctl enable firewalld firewall-cmd --permanent --add-port=111/udp firewall-cmd --permanent --add-port=111/tcp firewall-cmd --permanent --add-port=22/tcp firewall-cmd --permanent --add-port=2049/tcp firewall-cmd --permanent --add-port=20048/tcp firewall-cmd --reload . | . For the master nodes and worker nodes . If you’ve enabled firewalld, OMT will add firewall rules automatically on the master and worker nodes with your confirmation. ",
    "url": "/pages/write/firewall_settings.html#add-firewall-rules-for-the-inbound-connections",
    
    "relUrl": "/pages/write/firewall_settings.html#add-firewall-rules-for-the-inbound-connections"
  },"72": {
    "doc": "Check the firewall settings",
    "title": "Add firewall rules for the outbound connections",
    "content": "You must make sure you’ve added the related firewall rules for the required outbound ports to ensure the connection. For example, run the following commands on the worker nodes and master nodes to configure the firewall outbound port 5432 to connect an external PostgreSQL database: . systemctl start firewalld; systemctl enable firewalld firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 0 -p tcp -m tcp --dport=5432 -j ACCEPT -m comment --comment \"connect PostgreSQL\" firewall-cmd --reload . ",
    "url": "/pages/write/firewall_settings.html#add-firewall-rules-for-the-outbound-connections",
    
    "relUrl": "/pages/write/firewall_settings.html#add-firewall-rules-for-the-outbound-connections"
  },"73": {
    "doc": "Check the firewall settings",
    "title": "Enable VRRP protocol for Keepalived in a multiple master node deployment",
    "content": "If you want to set up a multiple master node (HA) environment by setting the HA_VIRTUAL_IP parameter in the install.properties file, make sure you’ve enabled the vrrp protocol when you have enabled the firewall on the node. Keepalived will the vrrp protocol to support virtual IP. In most default settings, the vrrp protocol is enabled. If you have used some custom settings for the server or if Keepalived doesn’t work well, run the following command on the node to enable the vrrp protocol: . firewall-cmd --add-protocol vrrp --permanent firewall-cmd --reload . ",
    "url": "/pages/write/firewall_settings.html#enable-vrrp-protocol-for-keepalived-in-a-multiple-master-node-deployment",
    
    "relUrl": "/pages/write/firewall_settings.html#enable-vrrp-protocol-for-keepalived-in-a-multiple-master-node-deployment"
  },"74": {
    "doc": "Check the firewall settings",
    "title": "Related topics",
    "content": "When you have finished, return to Set up prerequisites (embedded K8s) to continue. ",
    "url": "/pages/write/firewall_settings.html#related-topics",
    
    "relUrl": "/pages/write/firewall_settings.html#related-topics"
  },"75": {
    "doc": "Online help restructure",
    "title": "Online help restructure",
    "content": " ",
    "url": "/pages/design/help_restructure.html",
    
    "relUrl": "/pages/design/help_restructure.html"
  },"76": {
    "doc": "Home",
    "title": "Home",
    "content": "I’m a lead tech writer, formerly at OpenText, with 15 years of experience in technical documentation. I mainly worked on IT Service Management products and a Kubernetes-based container orchestration platform. Also an occasional cycling coach and one-time archaeologist. You can contact me at edcork@gmail.com and find me on LinkedIn. ",
    "url": "/",
    
    "relUrl": "/"
  },"77": {
    "doc": "Install OMT",
    "title": "Install OMT",
    "content": "On this page . | Step 1: Plan your deployment . | Decide which capabilities to enable | . | Step 2: Prepare a Docker Hub account | Step 3: Prepare infrastructure | Step 4: Download the OMT installation package | Step 5: Set up prerequisites | Step 6: Deploy | Step 7: Complete the post installation tasks | . This section describes how to install OMT. The following is a high level view of the process. ",
    "url": "/pages/write/install_landing.html",
    
    "relUrl": "/pages/write/install_landing.html"
  },"78": {
    "doc": "Install OMT",
    "title": "Step 1: Plan your deployment",
    "content": "Refer to the Sizing and directory structure, and System requirements topics. ",
    "url": "/pages/write/install_landing.html#step-1-plan-your-deployment",
    
    "relUrl": "/pages/write/install_landing.html#step-1-plan-your-deployment"
  },"79": {
    "doc": "Install OMT",
    "title": "Decide which capabilities to enable",
    "content": "OMT includes a number of capabilities that you can enable or disable during installation. Some capabilities help with the installation process itself, and others add functionality when you use OMT. The following table describes the capabilities. When installing OMT with its embedded Kubernetes, all capabilities are enabled by default. | Capability | Description | Default setting | . | Cluster management | The cluster management capability provides the necessary components for managing the Kubernetes cluster. | Enabled | . | Deployment management | The deployment management capability provides the necessary components for deploying a product. | Enabled | . | Log collection | The logging capability collects logs from OMT infrastructure and applications. By default, the logs are saved to an NFS volume. However, you can also forward the logs to an external receiver, such as Elasticsearch Server or Splunk.If you install OMT with the log collection capability disabled, logs remain wherever they’re generated. | Enabled | . | Monitoring | The monitoring capability uses the Prometheus and Grafana open source projects. Prometheus collects container based metrics associated with the OMT cluster, and streams them to applications (including Operations Bridge) and many third-party software solutions (known as exporters). Grafana is the default visualization tool for Prometheus.If you install OMT with the monitoring and NFS provisioner capabilities enabled, the NFS provisioner capability automatically configures an additional NFS persistent volume for the monitoring capability.If you install OMT with the NFS provisioner capability disabled, you must manually configure an additional persistent volume for the monitoring capability. | Enabled | . | Monitoring content | The monitoring content capability provides a set of out of the box Grafana dashboards to use with the monitoring capability. | Enabled | . | NFS provisioner | The NFS provisioner is an infrastructure capability that creates all the required NFS PVs automatically when you install OMT. You need only to set up an NFS server and create a volume on it. | Enabled | . | Tools | The tools capability provides a set of CLI tools (scripts) to help you administer the Kubernetes cluster and deploy applications. | Enabled | . | Kubernetes backup | The Kubernetes backup capability uses Velero to back up and restore Kubernetes application deployments. | Enabled | . ",
    "url": "/pages/write/install_landing.html#decide-which-capabilities-to-enable",
    
    "relUrl": "/pages/write/install_landing.html#decide-which-capabilities-to-enable"
  },"80": {
    "doc": "Install OMT",
    "title": "Step 2: Prepare a Docker Hub account",
    "content": "You need a valid Docker Hub account that we’ve authorized to download application images from our Docker Hub image registry. If you don’t already have one, you must set up the Docker account and then contact us with your account details. For more information about how to do this, see Activate your Docker Hub account. ",
    "url": "/pages/write/install_landing.html#step-2-prepare-a-docker-hub-account",
    
    "relUrl": "/pages/write/install_landing.html#step-2-prepare-a-docker-hub-account"
  },"81": {
    "doc": "Install OMT",
    "title": "Step 3: Prepare infrastructure",
    "content": "To deploy OMT with its embedded Kubernetes you must prepare the required infrastructure for OMT and its Kubernetes cluster. For more information, see Prepare the infrastructure for OMT. ",
    "url": "/pages/write/install_landing.html#step-3-prepare-infrastructure",
    
    "relUrl": "/pages/write/install_landing.html#step-3-prepare-infrastructure"
  },"82": {
    "doc": "Install OMT",
    "title": "Step 4: Download the OMT installation package",
    "content": "Download the installation package from the Software Licenses and Downloads portal. ",
    "url": "/pages/write/install_landing.html#step-4-download-the-omt-installation-package",
    
    "relUrl": "/pages/write/install_landing.html#step-4-download-the-omt-installation-package"
  },"83": {
    "doc": "Install OMT",
    "title": "Step 5: Set up prerequisites",
    "content": "Perform the prerequisite tasks to configure your environment for the installation. For more information, see Set up prerequisites for OMT. ",
    "url": "/pages/write/install_landing.html#step-5-set-up-prerequisites",
    
    "relUrl": "/pages/write/install_landing.html#step-5-set-up-prerequisites"
  },"84": {
    "doc": "Install OMT",
    "title": "Step 6: Deploy",
    "content": "Deploy OMT and its embedded Kubernetes. For more information, see Deploy OMT. ",
    "url": "/pages/write/install_landing.html#step-6-deploy",
    
    "relUrl": "/pages/write/install_landing.html#step-6-deploy"
  },"85": {
    "doc": "Install OMT",
    "title": "Step 7: Complete the post installation tasks",
    "content": "Some deployment scenarios require further configuration after you install OMT and its embedded Kubernetes. For more information, see Post installation tasks for OMT. ",
    "url": "/pages/write/install_landing.html#step-7-complete-the-post-installation-tasks",
    
    "relUrl": "/pages/write/install_landing.html#step-7-complete-the-post-installation-tasks"
  },"86": {
    "doc": "Prepare an environment to install OMT",
    "title": "Prepare an environment to install OMT",
    "content": "On this page . | Prepare to deploy the embedded Kubernetes | Request certificates | Set up persistent volumes | Create external databases | Decide what happens to log files | Configure on-access security scans | Create and configure a config.json file | . Before you install OPTIC Management Toolkit (OMT), you must perform some prerequisite tasks to configure the environment. ",
    "url": "/pages/write/install_prereq_tasks.html",
    
    "relUrl": "/pages/write/install_prereq_tasks.html"
  },"87": {
    "doc": "Prepare an environment to install OMT",
    "title": "Prepare to deploy the embedded Kubernetes",
    "content": "You must ensure that the cluster is correctly configured for the install script to deploy the Kubernetes that is embedded in OMT. This comprises the following tasks. | Task | Required? | Description | Detailed steps | . | Enable a regular user to install OMT | Optional | You need only do this if you will use a regular user to perform the installation. | Enable a regular user to install OMT | . | Update the system configuration | Mandatory | Update the system configuration to meet the deployment requirements. This includes ensuring the localhost resolves to 127.0.0.1, setting the required system parameters, disabling swap space, making sure all required Linux packages are installed, checking the SSH connection, checking default gateway, checking the host name on every node, and synchronizing time. A script is available to automate this process. | Make required system configurations | . | Check the firewall settings | Mandatory (if you’ve set a firewall) | If you have enabled a firewall in your network, you must check that the firewall settings meet the deployment requirements. | Check the firewall settings | . | Check that the required ports are open | Mandatory | Check that all the ports that OMT required for network communication are open. | Check that the required ports are open (embedded K8s) | . | Configure High Availability (HA) | Optional | If you have more than one master node, you can configure HA. To do this, you can either use Keepalived (included in OMT) or set up your own load balancer(s). | Configure Keepalived for High Availability Configure an internal load balancer | . | Configure the install.properties file. | Optional | Configure the installation of the master and worker nodes in the install.properties file. You can also use command options to do this when you run the install command. | Configure the install.properties file | . | Run a preliminary check of the nodes | Optional | You can run a script (pre-check.sh) that checks the readiness of the nodes for the deployment. This step is optional. However, it’s highly advisable that you perform a preliminary check on all the master nodes and worker nodes before you perform the remaining preparatory tasks to check whether your prepared nodes meet the basic requirements to install OPTIC Management Toolkit (OMT). | Run a preliminary check | . ",
    "url": "/pages/write/install_prereq_tasks.html#prepare-to-deploy-the-embedded-kubernetes",
    
    "relUrl": "/pages/write/install_prereq_tasks.html#prepare-to-deploy-the-embedded-kubernetes"
  },"88": {
    "doc": "Prepare an environment to install OMT",
    "title": "Request certificates",
    "content": "Certificates protect network traffic between OMT and external services. The network traffic flows include: . | Browser -&gt; OMT The OMT installer will create certificate authorities (CAs) to generate and sign server certificates for the ingress controller. However, the browsers in your organization won’t be able to validate these certificates, as they’re not in your organization’s trust store. If you want the browsers in your organization to connect to OMT securely, you must contact your IT administrator and request a server certificate pair (including a server certificate, a server key, and the CA cert which signed the server certificate). The server certificate pair is generated for the external access host name of OMT. For more information, see Request server certificates (embedded K8s). | OMT -&gt; external database If you are installing OMT with an external database, connect the database with TLS mode. In this case, contact the database administrator to request the CA certificate to validate the database server certificate. You’ll use the CA certificate when you configure the config.json file later. | OMT -&gt; external image registry If you are installing OMT with an external image registry, if the registry server certificate is already trusted on the operating system level, the CA cert of the registry isn’t required. Otherwise, contact the external registry administrator to request the CA cert to validate the registry server certificate. The CA cert will be used while running the install command. | . These certificates are required for OMT installation. For the certificates required by applications, please check the applications’ documentation. ",
    "url": "/pages/write/install_prereq_tasks.html#request-certificates",
    
    "relUrl": "/pages/write/install_prereq_tasks.html#request-certificates"
  },"89": {
    "doc": "Prepare an environment to install OMT",
    "title": "Set up persistent volumes",
    "content": "If a container stops or restarts, all changes made inside the container are lost. To save information such as configuration files and databases, the information must be stored outside of the container in a persistent volume (PV). When you install OMT with the embedded Kubernetes, the NFS provisioner capability (which is enabled by default) creates the required PVs automatically. To use this capability, you must create a single volume on the NFS server. When you run the install command, you will specify the NFS server URL and the path to this volume in command options. For more information about how to set up persistent volumes, see Set up persistent volumes (embedded K8s). ",
    "url": "/pages/write/install_prereq_tasks.html#set-up-persistent-volumes",
    
    "relUrl": "/pages/write/install_prereq_tasks.html#set-up-persistent-volumes"
  },"90": {
    "doc": "Prepare an environment to install OMT",
    "title": "Create external databases",
    "content": "OMT uses an IdM database. Unless you will use the PostgreSQL instance that’s embedded in OMT, you must create it on the database server that you prepared earlier. For more information about how to do this, see Configure external databases (embedded K8s). If you want to use the embedded database instance, you don’t need to create the databases. The install script will deploy the required databases automatically if you don’t specify any database options when you run the install command. ",
    "url": "/pages/write/install_prereq_tasks.html#create-external-databases",
    
    "relUrl": "/pages/write/install_prereq_tasks.html#create-external-databases"
  },"91": {
    "doc": "Prepare an environment to install OMT",
    "title": "Decide what happens to log files",
    "content": "OMT infrastructure and applications produce log files. By default, OMT collects these logs on an NFS volume (itom-logging-vol). However, you can also forward the logs to an external receiver, such as Elasticsearch Server or Splunk. For more information about how to do this, see Forward application logs to an external receiver (embedded K8s). ",
    "url": "/pages/write/install_prereq_tasks.html#decide-what-happens-to-log-files",
    
    "relUrl": "/pages/write/install_prereq_tasks.html#decide-what-happens-to-log-files"
  },"92": {
    "doc": "Prepare an environment to install OMT",
    "title": "Configure on-access security scans",
    "content": "OMT installation may fail if you have enabled on-access scanning by security products such as McAfee Endpoint Security, Microsoft Defender, or Trend Micro Deep Security Agent in your environment. To prevent this, you must exclude certain directories from the on-access scan. Further information is available in the Security products can’t scan files before they’re deleted troubleshooting topic. ",
    "url": "/pages/write/install_prereq_tasks.html#configure-on-access-security-scans",
    
    "relUrl": "/pages/write/install_prereq_tasks.html#configure-on-access-security-scans"
  },"93": {
    "doc": "Prepare an environment to install OMT",
    "title": "Create and configure a config.json file",
    "content": "Before you run the install command, you must Create and configure a config.json file (embedded K8s) that contains all the necessary parameters for the installation script. This file will capture information about many of the decisions you have made while following the steps in this topic. ",
    "url": "/pages/write/install_prereq_tasks.html#create-and-configure-a-configjson-file",
    
    "relUrl": "/pages/write/install_prereq_tasks.html#create-and-configure-a-configjson-file"
  },"94": {
    "doc": "Service Management record models",
    "title": "Service Management record models",
    "content": "A model simplifies the creation of records in Service Management. A model is a template that end users can select when creating a new record. Service Management automatically populates fields in the new record using information in the model. For certain record types, a model can even create the tasks necessary to complete a process. Models also help to standardize the process of record creation. This is useful if you frequently create records with similar information. For example, if you have many similar incidents, an incident model reduces the amount of effort required to create the numerous similar incident records. Service Management provides a set of default models. You can also add models as required. If you no longer require a model, you can retire that model. For more information about working with models in each Service Management module, see the following topics: . | Module | Topics | . | Service Level Management | Create an agreement modelEdit an agreement modelRetire an agreement model | . | Change Management | Create a change modelEdit a change model | . | Incident Management | Create an incident modelEdit an incident model | . | Knowledge Management | Create an article modelEdit an article model | . | Release Management | Create a release modelEdit a release model | . | Service Asset &amp; Configuration Management | Create an asset modelEdit an asset model | . | Software Asset Management | Create a license modelEdit a license model | . ",
    "url": "/pages/write/models.html",
    
    "relUrl": "/pages/write/models.html"
  },"95": {
    "doc": "Retire an agreement model",
    "title": "Retire an agreement model",
    "content": "Retiring an agreement model means changing its status from Active to Inactive. The current status is displayed in the workflow snapshot at the top of the model. You must have appropriate permissions to retire an agreement model. If you retire an agreement model, this has no effect on existing service level agreement records that were created using the model. | From the main menu, select Plan &gt; Service Level &gt; Agreement Models. | In the ID column, click the number of the agreement model that you want to retire. Service Management displays the model details. | On the toolbar, click Retired, and then click Save. | . ",
    "url": "/pages/write/retire_agreement_model.html",
    
    "relUrl": "/pages/write/retire_agreement_model.html"
  },"96": {
    "doc": "Retire an agreement model",
    "title": "Related topics",
    "content": ". | Create an agreement model | Edit an agreement model | . ",
    "url": "/pages/write/retire_agreement_model.html#related-topics",
    
    "relUrl": "/pages/write/retire_agreement_model.html#related-topics"
  },"97": {
    "doc": "Update the system configuration",
    "title": "Update the system configuration",
    "content": "On this page . | Check the SSH configurations . | Check whether you have enabled SSH | Check MAC and Cipher algorithms | Check the password or key authentication setting | Enable the password or key authentication setting | Check whether you have enabled SCP | . | Use the script to automate the system configuration . | Copy the script to each node | Run the script | . | Update the system configurations manually . | Ensure localhost is resolved to 127.0.0.1 | Set the required system parameters | Disable swap space | Install the required Linux packages | Set the default gateway settings | Configure the hosts file in the etc directory | Synchronize time | . | . Before you can deploy OMT with the embedded Kubernetes, you must: . | Check the SSH configurations | Ensure the localhost is resolved to 127.0.0.1 | Set the required system parameters | Disable swap space | Install the required Linux packages | Check the default gateway | Check the host name on every node | Synchronize time | . All of these tasks (excpet SSH) can be automated by running the node_prereq script automates the system configuration process, refer to Use the script to automate the system configuration for more information. If you decide not to use the node_prereq script and configure the parameters manually, follow the steps in the Update the system configurations manually section. ",
    "url": "/pages/write/system_config.html",
    
    "relUrl": "/pages/write/system_config.html"
  },"98": {
    "doc": "Update the system configuration",
    "title": "Check the SSH configurations",
    "content": "OMT supports default SSH configurations. If you are using default SSH configurations, ignore the steps in this topic. If you don’t have SSH enabled, or aren’t aware of the current SSH configurations, follow the steps in this topic. OpenSSH provides secure and encrypted connections between machines. Enable SSH on all master and worker nodes (cluster nodes) in your deployment. You can run the following command to check whether SSH is enabled and running as the root user or the regular user with elevated permissions: . systemctl is-active sshd . If the output is active, SSH is enabled on this node. However, if you have customized the SSH configurations, perform the following steps on all master nodes and worker nodes to check whether your SSH configuration meets the installation requirements. Check whether you have enabled SSH . Check the output of the systemctl is-active sshd command to make sure that SSH is enabled on this node. An SSH server is installed and enabled on most operating systems, and the configuration file is /etc/ssh/sshd_config. | To check whether the SSH server has been installed, run the following command: . systemctl -t service|grep sshd . | If the SSH isn’t installed on your node, run the following commands to install and start the SSH server: . yum install openssh-server systemctl enable sshd systemctl start sshd . | . Check MAC and Cipher algorithms . For security reasons, the IT administrator may allow only limited algorithms for SSH client connection. Make sure the /etc/ssh/sshd\\_config files on all the master nodes and worker nodes are configured with at least one of the following values. If no MAC/Cipher algorithm is configured in the /etc/ssh/sshd\\_config file, the server uses the default MAC/Cipher algorithm whose default value contains the MAC and Cipher requested by OMT. In that case, you can ignore this “Check MAC and Cipher algorithms” section. | For MAC algorithms: hmac-sha1,hmac-sha2-256,hmac-sha2-512,hmac-sha1-96 | For Cipher algorithms: 3des-cbc,aes128-cbc,aes192-cbc,aes256-cbc,aes128-ctr,aes192-ctr,aes256-ctr,arcfour128,arcfour256,blowfish-cbc . For example, add the following lines to the /etc/ssh/sshd_config files on all the master nodes and worker nodes: . MACs hmac-sha2-256,hmac-sha2-512 Ciphers aes128-cbc,aes192-cbc,aes256-cbc,aes128-ctr,aes192-ctr,aes256-ctr . | . Check the password or key authentication setting . During the installation, you will use either user name and password authentication or a user name and key authentication to add master nodes and worker nodes. To use a user name and password authentication to add nodes for the installation, make sure the PasswordAuthentication parameter in the /etc/ssh/sshd_config file is set to yes. You can run the following command to check whether the PasswordAuthentication parameter is set to yes. Replace &lt;cluster node&gt; with the IPv4 address or FQDN of any of the cluster nodes. ssh root@&lt;cluster node&gt; echo \"It\\'s working\\!\" . The command line will prompt you to enter the password of the cluster node. If your terminal resembles the following, the PasswordAuthentication parameter is set to yes. \\[root ~\\]# ssh root@192.0.2.0 echo \"It\\\\'s working\\\\!\" root@192.0.2.0's password: It's working! . To add the cluster nodes by using a user name and key authentication, make sure the PubkeyAuthentication parameter in the /etc/ssh/sshd_config file is set to yes. Enable the password or key authentication setting . If the password or key authentication setting isn’t enabled, follow these steps on all master and worker nodes to enable the setting: . | Log on to the cluster node as the root user. | Open the /etc/ssh/sshd_config file with a supported editor. | To enable the password or key authentication, make sure the value of the related parameter is set to yes. To enable both, set the value of both these parameters to yes. To enable password authentication: Check that the value of the PasswordAuthentication parameter is yes. If not, set the value of the parameter to yes, as follows: . PasswordAuthentication yes . To enable the key authentication: Check that the value of the PubkeyAuthentication parameter is yes. If not, set the value of the parameters to yes, as follows: . PubkeyAuthentication yes . | Set the value of the PermitRootLogin parameter. To enable both password and key authentication or only password authentication, set the value of the parameter to yes, as follows . PermitRootLogin yes . To enable key authentication only, set the value of the parameter to prohibit-passsword, as follows: . PermitRootLogin prohibit-password . | Save the /etc/ssh/sshd_config file. | Run the following command to restart the sshd service: . systemctl restart sshd.service . | . Check whether you have enabled SCP . To check if you’ve enabled SCP, follow these steps: . | Ensure that the /etc/ssh/disable_scp file doesn’t exist. | Ensure that the PermitTTY option in the /etc/ssh/sshd_config configuration file is set to yes: PermitTTY yes . | Ensure that the /etc/ssh/sshd_config configuration file doesn’t contain the following settings: ForceCommand internal-sftp . | . ",
    "url": "/pages/write/system_config.html#check-the-ssh-configurations",
    
    "relUrl": "/pages/write/system_config.html#check-the-ssh-configurations"
  },"99": {
    "doc": "Update the system configuration",
    "title": "Use the script to automate the system configuration",
    "content": "The node_prereq script automates this process. You will need to run the script on all your master nodes, worker nodes, and NFS servers. The node_prereq script depends on the yum command to install the related packages. Make sure a yum repository has been set up correctly on the server. Contact your IT administrator for help if one hasn’t. Copy the script to each node . | Copy the &lt;OMT_Embedded_K8s_2x.x-xxx&gt;/node_prereq script from the first master node to the /tmp directory on each of the remaining nodes (master nodes, worker nodes, and NFS server). | Navigate to the directory that contains the node_prereq script, and then run the following command to add the executive permission: . chmod +x node_prereq . | . Run the script . On each node, navigate to the directory that contains the script, and then run the appropriate command. This automates all the required configurations. | On the master nodes, run: ./node_prereq -T master --all . | On the worker nodes, run: ./node_prereq -T worker --all . | On the NFS servers, run: ./node_prereq -T nfs --all . | . If you want to automate only some required configurations, you can use command options to specify which tasks the script will perform: . | The --etc-hosts command option ensures the localhost is resolved to 127.0.0.1. | The --sys-param command option checks and sets the required system parameters. | The --disable-swap command option disable swap space. | The --install-pkg command option installs the required Linux packages. | . ",
    "url": "/pages/write/system_config.html#use-the-script-to-automate-the-system-configuration",
    
    "relUrl": "/pages/write/system_config.html#use-the-script-to-automate-the-system-configuration"
  },"100": {
    "doc": "Update the system configuration",
    "title": "Update the system configurations manually",
    "content": "ADD TEXT!!!!!! . Ensure localhost is resolved to 127.0.0.1 . Flannel uses the default gateway to create packet routing for communication. To enable network communication across all the cluster nodes, you must ensure localhost resolves to 127.0.0.1 on all master and worker nodes. To do this, follow these steps: . | Run the following command to check your localhost setting: . grep -v '^\\s*#' /etc/hosts 2&gt;/dev/null | grep -E '\\slocalhost$|\\slocalhost\\s' . If there is no return value, run the following command to set the default route setting: . echo \"127.0.0.1 localhost\" &gt;&gt; /etc/hosts . | Open the /etc/hosts file with a supported editor and check the configurations in the file. Make sure that localhost resolves to 127.0.0.1. For example: . 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 . If you want to enable IPv4/IPv6 dual stack for OMT installation, make sure the localhost resolves to ::1 in the /etc/hosts file. For example: . ::1 localhost localhost6 localhost6.localdomain6 . | . Set the required system parameters . OMT uses the br_netfilter module to enable transparent masquerading and to ease Virtual Extensible LAN (VxLAN) traffic for communication between Kubernetes pods across the cluster nodes. Therefore, you must make sure the br_netfilter module is installed on all master and worker nodes before you set the system parameters. To do this, follow these steps: . | Log in to the node. | Run the following command to check whether the br_netfilter module is enabled: . lsmod |grep br_netfilter . If there is no return value, the br_netfilter module isn’t installed. Run the following commands to install it: . modprobe br_netfilter echo \"br_netfilter\" &gt; /etc/modules-load.d/br_netfilter.conf . | Open the /etc/sysctl.conf file in a supported editor. The sysctl.conf file contains the following instructions: . # To override a whole file, create a new file with the same in # /etc/sysctl.d/ and put new settings there. To override # only specific settings, add a file with a lexically later # name in /etc/sysctl.d/ and put new settings there. Ignore these instructions and update the sysctl.conf file directly with the settings described below. | Run the following command to check if the fs.may_detach_mounts parameter exists: . # sysctl -n fs.may_detach_mounts . If you receive the following error message or the integer 1, you don’t need to configure the parameter. [root@testVM ~]# sysctl -n fs.may_detach_mounts sysctl: cannot stat /proc/sys/fs/may_detach_mounts: No such file or directory . [root@testVM ~]# sysctl -n fs.may_detach_mounts 1 . If you receive the integer 0, you must configure the parameter and set it to 1 in the sysctl.conf file. [root@testVM ~]# sysctl -n fs.may_detach_mounts 0 . | Set the following system parameters according to the operating system that’s installed on the node. Redhat 8.1 and later versions . net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip\\_forward = 1 kernel.sem=50100 128256000 50100 2560 . Oracle Linux 7.9 and later versions . net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip\\_forward = 1 kernel.sem=50100 128256000 50100 2560 . Rocky Linux versions . net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip\\_forward = 1 kernel.sem=50100 128256000 50100 2560 . Other older operating systems, including supported versions of CentOS . net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip\\_forward = 1 net.ipv4.tcp\\_tw\\_recycle = 0 fs.may\\_detach\\_mounts = 1 kernel.sem=50100 128256000 50100 2560 . | Save the /etc/sysctl.conf file, and then run the following commands to apply the updates to the node: . /sbin/sysctl -p . | . Disable swap space . Complete this task on all master and worker nodes. | Run the following command to disable the swap process: . swapoff -a . | By default, the swap configuration is in the /etc/fstab file. You can check and permanently disable the swap process from that configuration file. Open the /etc/fstab file in a supported editor, and then comment out the lines that display swap as the disk type. For example: . #/dev/mapper/centos_shcentos72x64-swap swap . | . Install the required Linux packages . The installation depends on various packages that are included in standard yum repositories. To install the packages on any of the servers, follow these steps: . | Make sure a yum repository has been set up correctly on the server. Contact your IT administrator for help if a yum repository hasn’t been set up correctly on your server. | Run the following command on the NFS servers to install the required packages: yum install nfs-utils rpcbind . | Run the following commnand on all master and worker nodes to install the required packages: yum install device-mapper-libs libgcrypt libtool-ltdl net-tools nfs-utils rpcbind systemd-libs unzip conntrack-tools curl lvm2 socat checkpolicy policycoreutils container-selinux bind-utils tar rng-tools iptables . | Run the following commnand on the master nodes only to install the required packages: yum install java-1.8.0-openjdk . | . Set the default gateway settings . If you get an error message Default gateway not set after the node_prereq script checks the default gateway, run the following command to add a default gateway on that node: . route add default gw &lt;IP address&gt; &lt;interface name&gt; . For example: . route add default gw 192.0.2.24 eth0 . Configure the hosts file in the etc directory . If you get an error message after the node_prereq script checks the node host name, you’ll have to configure the host name. If you have configured Domain Name Service (DNS) in your environment, and the master and worker nodes can resolve the FQDN of all cluster nodes, load balancer host, NFS server, and external databases, skip this section. If the Domain Name Service (DNS) isn’t configured in your environment, configure the /etc/hosts file on every node (master and worker) in the cluster. List all the master nodes, worker nodes, external database servers, external access host (HA virtual IP, load balancer), and NFS servers in the /etc/hosts file. In an environment that doesn’t have DNS configured, you must also configure the KUBE_DNS_HOSTS parameter in the install.properties file as described in the “Configure the install.properties file” topic. To set up the host name resolution after the installation, follow the steps listed in the “Update the DNS entries” topic. Add the IP address and FQDN details of all the nodes in the cluster. This includes the external access host (HA virtual IP, load balancer), external database servers, and NFS server. Add an entry for each server using the following syntax: . &lt;IP address&gt; &lt;FQDN&gt; . For example, you can add the following entries to the /etc/hosts file: . 192.0.2.0 external-accesshost.mycompany.com 192.0.2.1 control1.mycompany.com 192.0.2.2 control2.mycompany.com 192.0.2.3 control3.mycompany.com 192.0.2.4 worker1.mycompany.com 192.0.2.5 worker2.mycompany.com 192.0.2.6 worker3.mycompany.com 192.0.2.7 externalbalancer.mycompany.com 192.0.2.8 externaldb.mycompany.com 192.0.2.9 nfs.mycompany.com . Synchronize time . OMT components require the time on all nodes to be synchronized. If you receive a warning message after the node_prereq script checks the node host name, make sure that the Kubernetes cluster nodes can reach the Network Time Protocol (NTP) servers (either internal organization based or external internet based NTP servers). The following example uses the chrony tool to synchronize time across operating systems. You must have a time server prepared for the time synchronization. | On the first master node, run the following command to install the chrony client: . yum install chrony -y . | Run the following commands to create the drift file: . mkdir -p /var/lib/chrony echo &gt; /var/lib/chrony/driftfile . | Run the following commands to configure the chrony client. Replace the &lt;Time Server Name or IP Address&gt; placeholder with the host name or IPv4 address of your time server. You can use a public time server if your cluster can access the Internet. Otherwise, use an existing time server of the organization or create a time server. cat &lt;&lt;ENDFILE &gt;/etc/chrony.conf server &lt;Time Server Name or IP Address&gt; iburst driftfile /var/lib/chrony/driftfile stratumweight 0 rtcsync makestep 0.1 3 ENDFILE . | Run the following commands to enable and start the chrony service: . systemctl enable chronyd systemctl start chronyd . | Run the following command to synchronize the operating system time with the NTP server: . chronyc -a makestep . | Run the following command to restart the chrony daemon: . systemctl restart chronyd . | Run the following command to check the server time synchronization: . timedatectl . If your terminal resembles the following with the NTP synchronized set to yes, then you have successfully synchronized the time on the host with the time server: . \\[root@ ~\\]# timedatectl status Local time: Thu 2020-07-21 13:05:21 CST Universal time: Thu 2020-07-21 05:05:21 UTC RTC time: Thu 2020-07-21 05:05:21 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yes NTP synchronized: yes RTC in local TZ: no DST active: n/a . | Run the following command to synchronize the hardware time from the current system time: . hwclock -w . | Repeat these steps on all other master nodes (if any) and worker nodes. You must also synchronize the time on the NFS servers, load balancers, and database servers once you have created them. | . ",
    "url": "/pages/write/system_config.html#update-the-system-configurations-manually",
    
    "relUrl": "/pages/write/system_config.html#update-the-system-configurations-manually"
  },"101": {
    "doc": "Update the system configuration",
    "title": "Related topics",
    "content": "When you have finished, return to Set up prerequisites (embedded K8s) to continue. ",
    "url": "/pages/write/system_config.html#related-topics",
    
    "relUrl": "/pages/write/system_config.html#related-topics"
  },"102": {
    "doc": "Writing",
    "title": "Writing",
    "content": "This section contains examples of different types of topic and writing for different audiences. ",
    "url": "/pages/write.html",
    
    "relUrl": "/pages/write.html"
  },"103": {
    "doc": "Writing",
    "title": "Install OMT",
    "content": "This example aims to demonstrate different levels of detail aimed at different levels of experience. | “Install OMT” is a reference topic that guides users through the (long) installation process. | “Prepare an environment to install OMT” describes the how to configure the environment at a very high level. This topic is aimed at experienced users who have installed OMT before. They need to know what to do, but not how to do it. | The three child topics are examples of task topics. They’re aimed at less experienced users who need more support to configure the environment. The topics contain step-by-step instructions for the tasks described in “Prepare an environment to install OMT.” | . OMT is a Kubernetes-based platform, on which other products (such as Service Management) are deployed and managed. ",
    "url": "/pages/write.html#install-omt",
    
    "relUrl": "/pages/write.html#install-omt"
  },"104": {
    "doc": "Writing",
    "title": "Service Management record models",
    "content": "This example includes a concept landing page and a number of task-based child topics. Models are essentially templates that can be applied when creating different types of record in Service Management. The content and function of these models differ according to the module (and record type) in Service Management. These examples are from the Service Level Agreement module. ",
    "url": "/pages/write.html#service-management-record-models",
    
    "relUrl": "/pages/write.html#service-management-record-models"
  },"105": {
    "doc": "Writing",
    "title": "Dev2Prod REST API",
    "content": "An example of reference API documentation. The Dev2Prod REST API enables users to synchronize two Service Management environments (typically, development and production environments) without using the GUI. ",
    "url": "/pages/write.html#dev2prod-rest-api",
    
    "relUrl": "/pages/write.html#dev2prod-rest-api"
  }
}
