{"0": {
    "doc": "Page Not Found",
    "title": "Page Not Found",
    "content": "This is probably my fault. Sorry. Please try browsing to the page you were looking for, otherwise shoot me an email to complain. ",
    "url": "/404.html",
    
    "relUrl": "/404.html"
  },"1": {
    "doc": "Create an agreement model",
    "title": "Method 1: create an agreement model from an existing record",
    "content": ". | From the main menu, select Plan&nbsp;&gt; Service Level, and then select Service Agreements, Support Agreements, or Human Resources Agreements from the drop-down menu. Service Management displays a list of the selected type of agreement record. | Select the record you want to use as the model. Click the record identifier in the ID column to display the selected record. | Click More &gt; Create model from record. Service Management displays the new model form with the values copied from the original record. Every agreement model created this way takes all the field values from the existing record, including user options, but excluding: . | Service offerings . | Workflow status . | . Note that the title and description of the record are copied to the default values of section the model. You must enter a new title and description for the model itself. | Edit the model as required. | Click Save&nbsp;on the toolbar. | . ",
    "url": "/pages/write/create_agreement_model.html#Method_1:_create_a_change_model_from_an_existing_record",
    
    "relUrl": "/pages/write/create_agreement_model.html#Method_1:_create_a_change_model_from_an_existing_record"
  },"2": {
    "doc": "Create an agreement model",
    "title": "Method 2: create an agreement model from scratch",
    "content": "The record fields displayed are the same as those detailed in Create a Service Level Agreement record. However, out-of-the-box, Service Management disables some fields when creating an agreement model. If you have the appropriate rights, you can customize this behavior. | From the main menu, select Plan&nbsp;&gt; Service Level&nbsp;&gt; Agreement&nbsp;Models&nbsp;&gt;&nbsp;New. Service Management displays the New Model form. | Complete the General model details section. | Field | Description | . | Title | A name for the model. Best practice: Choose a meaningful, descriptive, and relatively short name. The name is often the only identification used in selection lists and in other areas to identify components. | . | Description | A description that captures the details of the model. | . | Agreement flavor | Choose whether this is a model for service agreements, support agreements, or human resources agreements. | . | Agreement type | The type of agreement record sets the workflow type. | . | Click Attachments &gt; Add attachment&nbsp;to upload a file to the agreement model. | The supported attachment file formats and the maximum file size of an attachment are defined in the tenant's Application settings tab in Suite Administration. | If the Attachments field has been defined as encrypted for this record type and you are a member of an encryption domain, click Add encrypted attachments to attach an encrypted file to the record. | Attachments aren't visible in the Service Portal. | . | Click the Approvals tab to add approvals to the model. For more information about approvals, see Task plans. | Click the&nbsp;Default values&nbsp;tab and complete the information relevant for the model. | Details . | Field | Description | . | Title | Type a word or phrase that's a unique identifier for the Service Level Agreements created with this model. It should be a value that makes it easy for the end user to understand the purpose of the Service Level Agreement. Example: Initial review for priority&nbsp;1 incidents. | . | Default agreement | Select this check box to make the Service Level Agreement records created with this model the default. The default Service Level Agreement (for incidents, changes, support requests, tasks, problems, or custom record types as appropriate) is automatically applied to all incidents and support requests which don't have a matching Service Level target Definition. | . | Owner | Choose the Service Level Agreement owner for records created with this model. Example: The owner might be the Service Level Manager. If no value is selected, the owner is set to the current user by default. | . | Technical group | Choose the technical group to which records created with this model are assigned. | . | Financial group | Choose the financial group to which records created with this model are assigned. | . | Description | Enter a description for the Service Level Agreement records created with this model. It's helpful for other users to understand the purpose and objectives of this Service Level Agreement. | . | Requirement . | Field | Description | . | Cost | The cost associated with Service Level Agreements created with this model. | . | Effort | The effort associated with Service Level Agreements created with this model. | . | Validity start date | The start date of the period of validity of Service Level Agreements created with this model. | . | Validity end date | The end date of the period of validity of&nbsp;Service Level Agreements created with this model. | . | Review and improvement . | Field | Description | . | Next review date | The date when Service Level Agreements created with this model must next be reviewed. | . | Service quality report | The service quality report associated with Service Level Agreements created with this model. | . | Improvement measures | The improvement measures associated with Service Level Agreements created with this model. | . | Service improvement plan | The service improvement plan associated with Service Level Agreements created with this model. | . | . | Click Save&nbsp;on the toolbar. | . When you create a model, Service Management automatically creates the workflow and displays the model in draft status. | Click the model's workflow tab to see the workflow and status. | You may edit the agreement type only when the model is in draft status. | To change the status to Active, you must have the appropriate rights.&nbsp; | In order for the model to be available for selection when creating Service Level Agreement records, its status must be Active. | . ",
    "url": "/pages/write/create_agreement_model.html#method-2-create-an-agreement-model-from-scratch",
    
    "relUrl": "/pages/write/create_agreement_model.html#method-2-create-an-agreement-model-from-scratch"
  },"3": {
    "doc": "Create an agreement model",
    "title": "Related topics",
    "content": ". | How to edit an agreement model | How to retire an agreement model | . ",
    "url": "/pages/write/create_agreement_model.html#Related_topics",
    
    "relUrl": "/pages/write/create_agreement_model.html#Related_topics"
  },"4": {
    "doc": "Create an agreement model",
    "title": "Create an agreement model",
    "content": "On this page . | Method 2: create an agreement model from scratch | . There are two methods to create an agreement model. ",
    "url": "/pages/write/create_agreement_model.html",
    
    "relUrl": "/pages/write/create_agreement_model.html"
  },"5": {
    "doc": "Enable a regular user to install OMT",
    "title": "Delegate the authority to install OMT to a regular user",
    "content": "To configure sudo permission, you must run the node_prereq script on the cluster node with a root user and the regular user must already exist on the node. Perform the following steps to delegate authority to install OMT to a regular user: . | Download the OMT installation package to the first control plane node and unzip it. | Change directory into the OMT installation package directory and run the following command to delegate authority to install OMT to a regular user: ./node_prereq --sudo-user &lt;username&gt; --sudo-for install . | After finishing the permission configuration on the first node, copy the node_prereq script to any directory (For example, /tmp) on other nodes and ensure that it has executable permission. Then run the following command: ./node_prereq --sudo-user &lt;username&gt; --sudo-for install . | . | If you’ll specify the OMT installation directory using the --cdf-home option when running the ./install script, specify the same cdf home with --cdf-home option when running node_prereq: ./node_prereq --sudo-user &lt;username&gt; --sudo-for install --cdf-home &lt;cdf_home&gt; . | If you’ll specify the temporary directory using the --tmp-folder option when running the ./install script, specify the same temporary directory with --tmp-folder option when running node_prereq: ./node_prereq --sudo-user &lt;username&gt; --sudo-for install --tmp-folder &lt;tmp_folder&gt; . | If you’ll perform administrative tasks with this regular user after installation, add administer to --sudo-for parameter as below or use the default value to delegate the install and administer authorities to the user: ./node_prereq --sudo-user &lt;username&gt; --sudo-for 'install,administer' . | . ",
    "url": "/pages/write/create_sudouser.html#delegate-the-authority-to-install-omt-to-a-regular-user",
    
    "relUrl": "/pages/write/create_sudouser.html#delegate-the-authority-to-install-omt-to-a-regular-user"
  },"6": {
    "doc": "Enable a regular user to install OMT",
    "title": "Install OMT as a regular user",
    "content": "After the regular user got the required authorities, run the command to get a list of commands you can run as a regular user: . sudo -n -ll -U &lt;username&gt; . To run the commands as a regular user, prefix the command with sudo. For example, if you’ll run the install command: ./install -c /tmp/config.json --nfsprov-server 192.0.2.0 --nfsprov-folder var/vols/itom/data as a regular user, run sudo ./install -c /tmp/config.json --nfsprov-server 192.0.2.0 --nfsprov-folder var/vols/itom/data instead. Embedded Kubernetes minimal installation (regular user) . Use this command to install OMT as a regular user. sudo ./install \\ -c &lt;config.json file&gt; \\ --nfsprov-server &lt;nfs-server&gt; \\ --nfsprov-folder &lt;nfs-directory&gt; . For example: . sudo ./install \\ -c /tmp/config.json \\ --nfsprov-server 192.0.2.0 \\ --nfsprov-folder /var/vols/itom/data . ",
    "url": "/pages/write/create_sudouser.html#install-omt-as-a-regular-user",
    
    "relUrl": "/pages/write/create_sudouser.html#install-omt-as-a-regular-user"
  },"7": {
    "doc": "Enable a regular user to install OMT",
    "title": "Related topics",
    "content": ". | When you have finished, return to Set up prerequisites (embedded K8s) to continue. | For how to revoke authority after OMT installation, refer to Revoke authority to install OMT. | If you don’t want to delegate authority to a regular user with the node_prereq script, refer to Delegate authority to a regular user without using the script. | . ",
    "url": "/pages/write/create_sudouser.html#related-topics",
    
    "relUrl": "/pages/write/create_sudouser.html#related-topics"
  },"8": {
    "doc": "Enable a regular user to install OMT",
    "title": "Enable a regular user to install OMT",
    "content": "On this page . | Delegate the authority to install OMT to a regular user | Install OMT as a regular user . | Embedded Kubernetes minimal installation (regular user) | . | Related topics | . sudo is a command line utility for Unix and Unix based operating systems. The utility provides an efficient way for system administrators to temporarily delegate certain regular users or user groups privileged access to system resources. Therefore, the users can run commands that they can’t run under their regular accounts.  . OMT requires root user permissions to install OMT. If the root users have security concerns to give a regular user the root password, you can delegate authority to the user before installing OMT. You can revoke the permissions after OMT installation. The node_prereq script can grant a specified regular user authority to run all the commands required to install OMT. You can also grant authority to administer OMT at the same or simply use the default value. But you will have to run the script again when they want to upgrade, as the regular user needs permission to run the upgrade command from the upgrade package. For a full command list that a regular user can run after granting permissions, refer to Use sudo to enable a regular user to install, upgrade, or administer OMT. ",
    "url": "/pages/write/create_sudouser.html",
    
    "relUrl": "/pages/write/create_sudouser.html"
  },"9": {
    "doc": "Design",
    "title": "Design",
    "content": "Examples of doc design. CV has stats on help restrtucture . ",
    "url": "/pages/design.html",
    
    "relUrl": "/pages/design.html"
  },"10": {
    "doc": "Dev2Prod REST API",
    "title": "Dev2Prod REST API",
    "content": "On this page . | Reference | Use case: Use API calls to perform a synchronization | . Service Management provides a Dev2Prod API to &quot;copy&quot; configuration or data from a development tenant to a production tenant (or between two development tenants) to synchronize and align environments. ",
    "url": "/pages/write/dev2prod_api.html",
    
    "relUrl": "/pages/write/dev2prod_api.html"
  },"11": {
    "doc": "Dev2Prod REST API",
    "title": "Reference",
    "content": "The following endpoints are available: . | /export/types/{package_id} | /export/{id} | /import/{id} | /import/ | . ",
    "url": "/pages/write/dev2prod_api.html#reference",
    
    "relUrl": "/pages/write/dev2prod_api.html#reference"
  },"12": {
    "doc": "Dev2Prod REST API",
    "title": "/export/types/{package_id}",
    "content": "Exports a customized package (created in Package Manager) on the source tenant to&nbsp;the File Repository Service (FRS).&nbsp; . URL structure . https://{serverAddress}/rest/{tenant_id}/dev2prod/export/types/{package_id} . Method . POST . Parameters . | {tenant_id}: The tenant ID of the Service Management tenant.&nbsp; | {package_id}:&nbsp;The ID of the export package. | . Example call . https://company.net/rest/123456789/dev2prod/export/types/54321 . Example request body . {} . Example response . { \"Id\": \"66fabdf1e4b06908b04d4051\" } . Error codes . | 400: Invalid Request | 401: Unauthorized | 403:&nbsp;Forbidden | 404:&nbsp;Page/Resource was not found | 500:&nbsp;Internal Server Error | . ",
    "url": "/pages/write/dev2prod_api.html",
    
    "relUrl": "/pages/write/dev2prod_api.html"
  },"13": {
    "doc": "Dev2Prod REST API",
    "title": "/export/{id}",
    "content": "Gets the status of an export process.&nbsp; . URL structure . https://{serverAddress}/rest/{tenant_id}/dev2prod/export/{id} . Method . GET . Parameters . | {tenant_id}: The tenant ID of the Service Management tenant.&nbsp; | {id}:&nbsp;The ID of the export process. | . Example call . https://company.net/rest/123456789/dev2prod/export/66fabdf1e4b06908b04d4051 . Example response . { \"id\": \"6797556be4b0c05084915394\", \"PackageBundleName\": \"Package-10204-555500000-2025-01-27 09-44\", \"Status\": \"COMPLETED\", \"FileId\": \"c7e87b89-f214-4766-8032-bba6967e1c49\", \"StartTime\": 1737971051843, \"LastUpdateTime\": 1737971053134, \"ItemCount\": 352, \"Statistics\": [ { \"PackageName\": \"metadata\", \"ItemCount\": 0 }, { \"PackageName\": \"workflow\", \"ItemCount\": 24 }, { \"PackageName\": \"formLayouts\", \"ItemCount\": 117 } ], \"OperationType\": \"Export\" } . Error codes . | 400: Invalid Request | 401: Unauthorized | 403:&nbsp;Forbidden | 404:&nbsp;Page/Resource was not found | 500:&nbsp;Internal Server Error | . ",
    "url": "/pages/write/dev2prod_api.html",
    
    "relUrl": "/pages/write/dev2prod_api.html"
  },"14": {
    "doc": "Dev2Prod REST API",
    "title": "/import/{id}",
    "content": "Gets the status of an import process. URL structure . https://{serverAddress}/rest/{tenant_id}/dev2prod/import/{id} . Method . GET . Parameters . | {tenant_id}: The tenant ID of the Service Management tenant.&nbsp; | {id}:&nbsp;The ID of the import process. | . Example call . https://company.net/rest/123456789/dev2prod/import/66fabdf1e4b06908b04d4051 . Example response - success . { \"TimeToLive\": 9946986, \"StudioLocked\": false, \"SystemLocked\": false, \"id\": \"67975a80e4b0c050849153ae\", \"FileId\": \"9bf82520-f087-4763-af10-cf1ed38d893d\", \"Status\": \"COMPLETED\", \"DryRun\": true, \"PackageManagerId\": \"67975a8be4b0c050849153b2\", \"StartTime\": 1737972352849, \"LastUpdateTime\": 1737972406603, \"ProcessingServerName\": \"company-server\", \"Results\": [ { \"PackageName\": \"metadata\", \"Statistics\": { \"Modified\": 300, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] }, { \"PackageName\": \"SLTTenantSetting\", \"Statistics\": { \"Modified\": 0, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] }, { \"PackageName\": \"indexConfiguration\", \"Statistics\": { \"Modified\": 26, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] } ], \"Totals\": { \"Added\": 0, \"Modified\": 326, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"OperationType\": \"Import\" } . Error codes . | 400: Invalid Request | 401: Unauthorized | 403:&nbsp;Forbidden | 404:&nbsp;Page/Resource was not found | 500:&nbsp;Internal Server Error | . ",
    "url": "/pages/write/dev2prod_api.html",
    
    "relUrl": "/pages/write/dev2prod_api.html"
  },"15": {
    "doc": "Dev2Prod REST API",
    "title": "/import/",
    "content": "Imports a Dev2Prod package to the current tenant. URL structure . https://{serverAddress}/rest/{tenant_id}/dev2prod/import . Method . POST . Parameters . | {tenant_id}: The tenant ID of the Service Management tenant.&nbsp; | dryRun:&nbsp;Simulates the import process. Helps to find&nbsp;any conflicts or errors without comitting any changes. | . Example call . https://company.net/rest/123456789/dev2prod/import?dryRun=true . Example request body . { \"FileId\": \"9bf82520-f087-4763-af10-cf1ed38d893d\" } . Example response . { \"Id\": \"67975cd8e4b0c0508491541f\" } . Error codes . | 400: Invalid Request | 401: Unauthorized | 403:&nbsp;Forbidden | 404:&nbsp;Page/Resource was not found | 500:&nbsp;Internal Server Error | . ",
    "url": "/pages/write/dev2prod_api.html",
    
    "relUrl": "/pages/write/dev2prod_api.html"
  },"16": {
    "doc": "Dev2Prod REST API",
    "title": "Use case: Use API calls to perform a synchronization",
    "content": "Service Management provides a&nbsp;REST API&nbsp;that, together with the Case Exchange API, you can use to perform Dev2Prod synchronization. Reference information these APIs is available in the following topics: . | Dev2Prod REST API | Case Exchange REST API | . Prerequisites . You have already generated a full or granular export package on the source tenant, as described above. Workflow and examples . The workflow is as follows: . | Run the https://{serverAddress}/rest/{tenant_id}/dev2prod/export/types/{id}&nbsp;POST method API to export the package to&nbsp;the File Repository Service (FRS). The expected response is the export ID. Example This example exports package &quot;54321&quot; on tenant 123456789. API: . https://company.net/rest/123456789/dev2prod/export/types/54321 . Response body: { \"Id\": \"66fabdf1e4b06908b04d4051\" } . | Use the export ID returned in the previous step to run the https://{serverAddress}/rest/{tenant_id}/dev2prod/export/{id}&nbsp;GET method API and&nbsp;view the export details. Example This example queries export &quot;66fabdf1e4b06908b04d4051&quot; on tenant 123456789. API: . https://company.net/rest/123456789/dev2prod/export/66fabdf1e4b06908b04d4051 . Response body: { \"id\": \"6797556be4b0c05084915394\", \"PackageBundleName\": \"Package-54321-123456789-2025-01-27 09-44\", \"Status\": \"COMPLETED\", \"FileId\": \"c7e87b89-f214-4766-8032-bba6967e1c49\", \"StartTime\": 1737971051843, \"LastUpdateTime\": 1737971053134, \"ItemCount\": 352, \"Statistics\": [ { \"PackageName\": \"metadata\", \"ItemCount\": 0 }, { \"PackageName\": \"workflow\", \"ItemCount\": 24 }, { \"PackageName\": \"formLayouts\", \"ItemCount\": 117 }, { \"PackageName\": \"customAction\", \"ItemCount\": 0 }, { \"PackageName\": \"indexConfiguration\", \"ItemCount\": 0 }, { \"PackageName\": \"featureInstances\", \"ItemCount\": 26 }, { \"PackageName\": \"uiComponentConfig\", \"ItemCount\": 0 }, { \"PackageName\": \"notificationTemplateDefinitions\", \"ItemCount\": 88 }, { \"PackageName\": \"notificationTemplateBodies\", \"ItemCount\": 97 }, { \"PackageName\": \"SLTTenantSetting\", \"ItemCount\": 0 }, { \"PackageName\": \"data\", \"ItemCount\": 0 }, { \"PackageName\": \"userOptionMetadata\", \"ItemCount\": 0 }, { \"PackageName\": \"rulesInEntity\", \"ItemCount\": 0 }, { \"PackageName\": \"many2manyRelations\", \"ItemCount\": 0 }, { \"PackageName\": \"attachments\", \"ItemCount\": 0 }, { \"PackageName\": \"customAppSltSettings\", \"ItemCount\": 0 }, { \"PackageName\": \"resourceBundles\", \"ItemCount\": 0 } ], \"OperationType\": \"Export\" } . | Run the https://{serverAddress}/rest/{tenant_id}/ces/attachment/{attachment_id}&nbsp;GET method API to download the exported package to the source tenant. Use the FileID value from the previous response as the attachment ID. The expected response is ? &nbsp; Example This example downloads package &quot;c7e87b89-f214-4766-8032-bba6967e1c49&quot; on tenant 123456789. API: . https://company.net/rest/123456789/dev2prod/ces/attachment/c7e87b89-f214-4766-8032-bba6967e1c49 . | Run the https://{serverAddress}/rest/{tenant_id}/ces/attachment&nbsp;POST method API to upload the package to FRS. &nbsp; Example This example uploads package &quot;??&quot; on tenant 123456789. API: . https://company.net/rest/123456789/ces/attachment . Request body: { ?? } . Response body: { \"Id\": \"66fabdf1e4b06908b04d4051\" } . | Run the https://{serverAddress}/rest/{tenant_id}/dev2prod/import?dryRun=true&nbsp;POST method API to simulate importing the package to the target tenant. &nbsp; Example This example simulates importing package &quot;c7e87b89-f214-4766-8032-bba6967e1c49&quot; on tenant 123456789. API: . https://company.net/rest/123456789/dev2prod/import?dryRun=true . Request body: { \"FileId\": \"c7e87b89-f214-4766-8032-bba6967e1c49\" } . Response body: { \"Id\": \"66fabdf1e4b06908b04d4051\" } . | Run the https://{serverAddress}/rest/{tenant_id}/dev2prod/import/{id}&nbsp;GET method API to check the import status. If there are no errors, the import dry-run was successful, and you can import the package. &nbsp; Example This example checks the status of import &quot;66fabdf1e4b06908b04d4051&quot; on tenant 123456789. API: . https://company.net/rest/123456789/dev2prod/import/66fabdf1e4b06908b04d4051 . Response body: { \"TimeToLive\": 9946986, \"StudioLocked\": false, \"SystemLocked\": false, \"id\": \"66fabdf1e4b06908b04d4051\", \"FileId\": \"c7e87b89-f214-4766-8032-bba6967e1c49\", \"Status\": \"COMPLETED\", \"DryRun\": true, \"PackageManagerId\": \"67975a8be4b0c050849153b2\", \"StartTime\": 1737972352849, \"LastUpdateTime\": 1737972406603, \"ProcessingServerName\": \"company-server\", \"Results\": [ { \"PackageName\": \"metadata\", \"Statistics\": { \"Modified\": 300, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] }, { \"PackageName\": \"SLTTenantSetting\", \"Statistics\": { \"Modified\": 0, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] }, { \"PackageName\": \"indexConfiguration\", \"Statistics\": { \"Modified\": 26, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"Errors\": [], \"Warnings\": [] } ], \"Totals\": { \"Added\": 0, \"Modified\": 326, \"Removed\": 0, \"Failed\": 0, \"Skipped\": 0 }, \"OperationType\": \"Import\" } . | Run the https://{serverAddress}/rest/{tenant_id}/dev2prod/import&nbsp;POST method API to import the package to the source tenant. &nbsp; Example This example imports package &quot;c7e87b89-f214-4766-8032-bba6967e1c49&quot; on tenant 123456789. API: . https://company.net/rest/123456789/dev2prod/import . Request body: { \"FileId\": \"c7e87b89-f214-4766-8032-bba6967e1c49\" } . Response body: { \"Id\": \"66fabdf1e4b06908b04d4051\" } . | . ",
    "url": "/pages/write/dev2prod_api.html#use-case-use-api-calls-to-perform-a-synchronization",
    
    "relUrl": "/pages/write/dev2prod_api.html#use-case-use-api-calls-to-perform-a-synchronization"
  },"17": {
    "doc": "Dummy link",
    "title": "Dummy link",
    "content": "The linked page does not exist in the portfolio- sorry! . ",
    "url": "/pages/dummy.html",
    
    "relUrl": "/pages/dummy.html"
  },"18": {
    "doc": "Editing",
    "title": "Editing",
    "content": "This section contains some examples of edits I’ve made while updating older topics. ",
    "url": "/pages/edit.html",
    
    "relUrl": "/pages/edit.html"
  },"19": {
    "doc": "Editing",
    "title": "Sample 1: Troubleshooting topic 1",
    "content": "This troubleshooting topic is an example of fixing generally poor language. ",
    "url": "/pages/edit.html#sample-1-troubleshooting-topic-1",
    
    "relUrl": "/pages/edit.html#sample-1-troubleshooting-topic-1"
  },"20": {
    "doc": "Editing",
    "title": "Sample 1: Troubleshooting topic 2",
    "content": "Another troubleshooting topic, this time containing numerous technical inaccuracies. ",
    "url": "/pages/edit.html#sample-1-troubleshooting-topic-2",
    
    "relUrl": "/pages/edit.html#sample-1-troubleshooting-topic-2"
  },"21": {
    "doc": "Editing",
    "title": "Sample 3: Integration guide",
    "content": "In this parent topic that introduces some integration guides, poor structure makes the basic message unclear. ",
    "url": "/pages/edit.html#sample-3-integration-guide",
    
    "relUrl": "/pages/edit.html#sample-3-integration-guide"
  },"22": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Editing sample 1: Troubleshooting topic 1",
    "content": "On this page . | ORIGINAL TEXT: Kubelet gets stuck with massive “runtime service failed: rpc error: code = Unknown” error messages . | Cause | Solution | . | EDITED TEXT: “runtime service failed: rpc error: code = Unknown” error messages and the kubelet enters a restart loop . | Cause | Solution | . | . Source: UCMDB 2021.08 documentation. Summary of issues . | Non-native language mistakes | Including: “Massive” error messages, “a known issue of Kubernetes”, “see details from”, and “then run below command.” | . | Formatting issues | The link to the Kubernetes documentation uses the URL as title text. The “Terminating” state is formatted inconsistently. There is no need to format “Kubelet” with code tags. | . | Technical accuracy | “Kubelet” should be “the kubelet”, as defined by the Kubernetes glossary. | . | Structure | The first sentence of the Cause section is actually part of the symptoms. | . | Redundancies | “On a node that has CDF installed” is redundant: this is documentation about the product CDF (therefore, all nodes have CDF installed on them). | . ",
    "url": "/pages/edit/edit1.html#editing-sample-1-troubleshooting-topic-1",
    
    "relUrl": "/pages/edit/edit1.html#editing-sample-1-troubleshooting-topic-1"
  },"23": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "ORIGINAL TEXT: Kubelet gets stuck with massive “runtime service failed: rpc error: code = Unknown” error messages",
    "content": "You receive massive error messages on a node with CDF installed that resemble the following: . b83ace3b5e0870284e554502e8922563a4d9587b800b6b699dc2c2acfcc9b7cc\" from runtime service failed: rpc error: code = Unknown desc = unable to inspect docker image \"sha256:a950dd441cee8f60ce4ee325799c62e5fe444fa8e851b5c96b9172da0ced8d28\" while inspecting docker container \"b83ace3b5e0870284e554502e8922563a4d9587b800b6b699dc2c2acfcc9b7cc\": no such image: \"sha256:a950dd441cee8f60ce4ee325799c62e5fe444fa8e851b5c96b9172da0ced8d28\" . When you check the pod status, some pods are stuck in Terminating state. ",
    "url": "/pages/edit/edit1.html#original-text-kubelet-gets-stuck-with-massive-runtime-service-failed-rpc-error-code--unknown-error-messages",
    
    "relUrl": "/pages/edit/edit1.html#original-text-kubelet-gets-stuck-with-massive-runtime-service-failed-rpc-error-code--unknown-error-messages"
  },"24": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Cause",
    "content": "This issue occurs because Kubelet loops when trying to inspect a Docker container for a pod whose image has been deleted or cleaned up. This is a known issue of Kubernetes. See details from https://github.com/kubernetes/kubernetes/issues/84214. ",
    "url": "/pages/edit/edit1.html#cause",
    
    "relUrl": "/pages/edit/edit1.html#cause"
  },"25": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Solution",
    "content": ". | Log on to the node where you receive these error messages. | Run the following command to check the pod status: kubectl get pods -n core -o wide | Identify the pods that are stuck in the “Terminating” state on this node. Then run below command to delete the pods. You need to replace the &lt;pod name&gt; placeholder with the name of the pod that is in the “Terminating” state. Run the following command for all the “terminating” pods: kubectl delete pod &lt;pod name&gt; -n core --force --grace-period=0 | Run the following command to restart CDF: K8S_HOME/bin/kube-restart.sh | . ",
    "url": "/pages/edit/edit1.html#solution",
    
    "relUrl": "/pages/edit/edit1.html#solution"
  },"26": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "EDITED TEXT: “runtime service failed: rpc error: code = Unknown” error messages and the kubelet enters a restart loop",
    "content": "When the kubelet tries to inspect the Docker container of a pod whose image was deleted or cleaned up, the kubelet enters a cyclical restart loop. When this issue occurs, the pod becomes stuck in the “Terminating” state, and you receive error messages that resemble the following: . b83ace3b5e0870284e554502e8922563a4d9587b800b6b699dc2c2acfcc9b7cc\" from runtime service failed: rpc error: code = Unknown desc = unable to inspect docker image \"sha256:a950dd441cee8f60ce4ee325799c62e5fe444fa8e851b5c96b9172da0ced8d28\" while inspecting docker container \"b83ace3b5e0870284e554502e8922563a4d9587b800b6b699dc2c2acfcc9b7cc\": no such image: \"sha256:a950dd441cee8f60ce4ee325799c62e5fe444fa8e851b5c96b9172da0ced8d28\"` . ",
    "url": "/pages/edit/edit1.html#edited-text-runtime-service-failed-rpc-error-code--unknown-error-messages-and-the-kubelet-enters-a-restart-loop",
    
    "relUrl": "/pages/edit/edit1.html#edited-text-runtime-service-failed-rpc-error-code--unknown-error-messages-and-the-kubelet-enters-a-restart-loop"
  },"27": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Cause",
    "content": "This is a known issue in Kubernetes. For more information, see Kubelet gets stuck trying to inspect a container whose image has been cleaned up. ",
    "url": "/pages/edit/edit1.html#cause-1",
    
    "relUrl": "/pages/edit/edit1.html#cause-1"
  },"28": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Solution",
    "content": ". | Log on to the node where you receive the error messages. | Run the following command to check the pod status, and then identify the pods that are stuck in the “Terminating” state: kubectl get pods -n core -o wide | Run the following command to delete the pods. Replace the &lt;pod name&gt; placeholder with the name of the pod that is in the “Terminating” state. Do this for all pods stuck in the “Terminating” state. kubectl delete pod &lt;pod name&gt; -n core --force --grace-period=0 | Run the following command to restart CDF: $K8S_HOME/bin/kube-restart.sh | . ",
    "url": "/pages/edit/edit1.html#solution-1",
    
    "relUrl": "/pages/edit/edit1.html#solution-1"
  },"29": {
    "doc": "Sample 1: Troubleshooting topic 1",
    "title": "Sample 1: Troubleshooting topic 1",
    "content": " ",
    "url": "/pages/edit/edit1.html",
    
    "relUrl": "/pages/edit/edit1.html"
  },"30": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Editing sample 2: Troubleshooting topic",
    "content": "On this page . | ORIGINAL TEXT: RabbitMQ isn’t ready . | Cause | Solution | . | EDITED TEXT: RabbitMQ pod isn’t ready . | Cause | Solution | . | . Source: SMAX 2021.11 documentation. Summary of issues . | Technical accuracy | You do not power off an environment. The note says “If you… fail to remove these folders,” but it’s not the user that fails to do this. Further, there are no instructions at the linked Azure files topic, as the described issue was fixed 2.5 years earlier. | . | Clarity | The connection between the first sentence and the first step of the Solution is lost due to poor structure. | . | Product naming and consistency | The terms “environment” and “system” are used interchangeably. The term “suite” is used instead of the correct product name. Inconsistent formatting of directory paths. | . ",
    "url": "/pages/edit/edit2.html#editing-sample-2-troubleshooting-topic",
    
    "relUrl": "/pages/edit/edit2.html#editing-sample-2-troubleshooting-topic"
  },"31": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "ORIGINAL TEXT: RabbitMQ isn’t ready",
    "content": "The infra-rabbitmq-&lt;n&gt; (&lt;n&gt;=0, 1, or 2) pod isn’t ready. The pod’s readiness state is stuck in 1/2. The following is an example: . NAME READY STATUS RESTARTS AGE infra-rabbitmq-0 1/2 Running 0 16h . ",
    "url": "/pages/edit/edit2.html#original-text-rabbitmq-isnt-ready",
    
    "relUrl": "/pages/edit/edit2.html#original-text-rabbitmq-isnt-ready"
  },"32": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Cause",
    "content": "There are many causes for this issue. Here are some examples. | The suite environment wasn’t shut down gracefully. For example, you powered off the environment directly without shutting down the suite and OMT pods first. | Your system has insufficient hardware resources. | There are issues with the network connectivity between the NFS server and worker nodes. | . ",
    "url": "/pages/edit/edit2.html#cause",
    
    "relUrl": "/pages/edit/edit2.html#cause"
  },"33": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Solution",
    "content": "When RabbitMQ fails to start twice, the system will automatically perform a fresh start of RabbitMQ. When this issue occurs: . | Wait up to 15 minutes, the issue is probably gone. | If the problem still persists, check your system resources and network connectivity. | If it’s not a resource or network issue, manually restart RabbitMQ. | . How to manually restart RabbitMQ: . | Run the following command on a master node (embedded Kubernetes) or the bastion node (managed Kubernetes) to stop RabbitMQ: . kubectl scale statefulset infra-rabbitmq -n &amp;lt;suite namespace&amp;gt; --replicas=0 . | Wait until all RabbitMQ pods are terminated. | Remove the &lt;rabbitmq-infra-rabbitmq-n&gt;/data/xservices/rabbitmq/x.x.x.xx/mnesia folders on the NFS server or the bastion node (if managed NFS is used, including EFS, Azure Files, Azure NetApp Files, and Filestore). For example, remove the following folders: . /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-0/data/xservices/rabbitmq/x.x.x.xx/mnesia /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-1/data/xservices/rabbitmq/x.x.x.xx/mnesia /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-2/data/xservices/rabbitmq/x.x.x.xx/mnesia . Note If you use Azure Files as the storage service and fail to remove these folders, follow the instructions at Fail to delete files except that you don’t need to install Azure Powershell Module. Instead, you can run all the commands with Azure Cloud Shell from the Azure portal. | Run the following command on a master node (embedded Kubernetes) or the bastion node (managed Kubernetes) to restart RabbitMQ: . kubectl scale statefulset infra-rabbitmq -n &lt;suite namespace&gt; --replicas=3 . | . ",
    "url": "/pages/edit/edit2.html#solution",
    
    "relUrl": "/pages/edit/edit2.html#solution"
  },"34": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "EDITED TEXT: RabbitMQ pod isn’t ready",
    "content": "The RabbitMQ pod (for example, infra-rabbitmq-&lt;n&gt;, where &lt;n&gt; is 0, 1, or 2) isn’t ready. The pod’s readiness state is stuck at 1/2. For example: . NAME READY STATUS RESTARTS AGE infra-rabbitmq-0 1/2 Running 0 16h . ",
    "url": "/pages/edit/edit2.html#edited-text-rabbitmq-pod-isnt-ready",
    
    "relUrl": "/pages/edit/edit2.html#edited-text-rabbitmq-pod-isnt-ready"
  },"35": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Cause",
    "content": "This issue may occur because: . | The system was shut down incorrectly. For example, you powered off the system without first shutting down Service Management and OMT. | Your system doesn’t have enough hardware resources. | There are network connectivity issues between the NFS server and the worker nodes. | . ",
    "url": "/pages/edit/edit2.html#cause-1",
    
    "relUrl": "/pages/edit/edit2.html#cause-1"
  },"36": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Solution",
    "content": "OMT automatically restarts RabbitMQ if it fails to start twice. Therefore, first wait 15 minutes, and then check if the issue was resolved automatically. If the issue still exists, check the system resources and network connectivity. If there are no resource or network issues, manually restart RabbitMQ. To do this, follow these steps: . | Run the following command on a control plane node (embedded Kubernetes) or the bastion node (managed Kubernetes) to stop RabbitMQ: . kubectl scale statefulset infra-rabbitmq -n &lt;Service Management namespace&gt; --replicas=0 . | Wait until all RabbitMQ pods have stopped. | Remove the rabbitmq-infra-rabbitmq-&lt;n&gt;/data/xservices/rabbitmq/x.x.x.xx/mnesia folders from the NFS server (or the bastion node if you use a managed NFS, including: EFS, Azure NetApp Files, and Filestore). For example, remove the following folders: . | /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-0/data/xservices/rabbitmq/x.x.x.xx/mnesia | /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-1/data/xservices/rabbitmq/x.x.x.xx/mnesia | /var/vols/itom/itsma/rabbitmq-infra-rabbitmq-2/data/xservices/rabbitmq/x.x.x.xx/mnesia | . | Restart RabbitMQ. To do this, run the following command on a control plane node (embedded Kubernetes) or on the bastion node (managed Kubernetes): . kubectl scale statefulset infra-rabbitmq -n &lt; Service Management namespace&gt; --replicas=3 . | . ",
    "url": "/pages/edit/edit2.html#solution-1",
    
    "relUrl": "/pages/edit/edit2.html#solution-1"
  },"37": {
    "doc": "Sample 2: Troubleshooting topic 2",
    "title": "Sample 2: Troubleshooting topic 2",
    "content": " ",
    "url": "/pages/edit/edit2.html",
    
    "relUrl": "/pages/edit/edit2.html"
  },"38": {
    "doc": "Sample 3: Integration guide",
    "title": "Editing sample 3: Integration guide",
    "content": "On this page . | ORIGINAL TEXT: Integrate with cloud platforms for CMP FinOps . | Related topics | . | EDITED TEXT: Integrations for CMP FinOps . | Cloud account data | Cloud cost data | Related topics | . | . Source: Service Management 25.1 documentation. Summary of issues . | Clarity and structure | The topic is a series of lists, which obscure what should be a simple message: the product relies on 2 types of data, and each type of data is retrieved a different way. | . | Non-native language mistakes | “Firstly you need to…” | . ",
    "url": "/pages/edit/edit3.html#editing-sample-3-integration-guide",
    
    "relUrl": "/pages/edit/edit3.html#editing-sample-3-integration-guide"
  },"39": {
    "doc": "Sample 3: Integration guide",
    "title": "ORIGINAL TEXT: Integrate with cloud platforms for CMP FinOps",
    "content": "To use the Cloud Management Platform (CMP) FinOps solution, firstly you need to retrieve cloud data from the cloud platforms that we support: Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP). The CMP FinOps solution uses two types of cloud data: . | Cloud billing data - The CMP FinOps’ cloud cost visualization and optimization functionalities, including cost governance reports, cloud insights, and cloud cost limits, are all based on the retrieved billing data. | Cloud account data - The CMP FinOps account management functionality enables you to centrally manage cloud accounts of supported cloud platforms. | . You need to set up two types of integrations to retrieve the cloud data: . | Cloud Cost Data Provider-based integrations that retrieve cloud billing data | Integration Studio-based integrations that import cloud account data | . See these topics for how to set up the integrations to retrieve cloud data from each cloud platform: . | Integrate with AWS for CMP FinOps | Integrate with Azure for CMP FinOps | Integrate with GCP for CMP FinOps | . ",
    "url": "/pages/edit/edit3.html#original-text-integrate-with-cloud-platforms-for-cmp-finops",
    
    "relUrl": "/pages/edit/edit3.html#original-text-integrate-with-cloud-platforms-for-cmp-finops"
  },"40": {
    "doc": "Sample 3: Integration guide",
    "title": "Related topics",
    "content": ". | Integration Studio | Cloud cost data providers | Cloud account management | Cost governance reports | . ",
    "url": "/pages/edit/edit3.html#related-topics",
    
    "relUrl": "/pages/edit/edit3.html#related-topics"
  },"41": {
    "doc": "Sample 3: Integration guide",
    "title": "EDITED TEXT: Integrations for CMP FinOps",
    "content": "Cloud Management Platform (CMP) FinOps uses two types of data: cloud account data and cloud cost data. Before you can use CMP FinOps, you must set up two integrations with your cloud provider to retrieve this data. ",
    "url": "/pages/edit/edit3.html#edited-text-integrations-for-cmp-finops",
    
    "relUrl": "/pages/edit/edit3.html#edited-text-integrations-for-cmp-finops"
  },"42": {
    "doc": "Sample 3: Integration guide",
    "title": "Cloud account data",
    "content": "CMP FinOps uses cloud account data to help you manage and configure your cloud accounts. You can set up the integration to retrieve cloud account data using Integration Studio, Service Management’s built-in integration platform. ",
    "url": "/pages/edit/edit3.html#cloud-account-data",
    
    "relUrl": "/pages/edit/edit3.html#cloud-account-data"
  },"43": {
    "doc": "Sample 3: Integration guide",
    "title": "Cloud cost data",
    "content": "CMP FinOps uses cloud cost data to provide cost visualizations and suggest improvements. This includes cloud cost governance reports, cloud insights, and cloud cost limits. You can’t set up an integration to retrieve cloud cost data using Integration Studio, as CMP FinOps retrieves this data differently for each cloud provider. You must instead set up a Cloud Cost Data Provider in the Admin &amp; Providers section of Service Management. ",
    "url": "/pages/edit/edit3.html#cloud-cost-data",
    
    "relUrl": "/pages/edit/edit3.html#cloud-cost-data"
  },"44": {
    "doc": "Sample 3: Integration guide",
    "title": "Related topics",
    "content": ". | To learn how to set up these integrations with AWS, see Integrate with AWS for CMP FinOps. | To learn how to set up these integrations with Azure, see Integrate with Azure for CMP FinOps. | To learn how to set up these integrations with GCP, see Integrate with GCP for CMP FinOp. | For general information about Integration Studio, see Integration Studio. | For general information about Cloud Cost Data Providers, see Cloud cost data providers. | For an introduction to FinOps, see Get started with CMP FinOps. | . ",
    "url": "/pages/edit/edit3.html#related-topics-1",
    
    "relUrl": "/pages/edit/edit3.html#related-topics-1"
  },"45": {
    "doc": "Sample 3: Integration guide",
    "title": "Sample 3: Integration guide",
    "content": " ",
    "url": "/pages/edit/edit3.html",
    
    "relUrl": "/pages/edit/edit3.html"
  },"46": {
    "doc": "Edit an agreement model",
    "title": "Related topics",
    "content": ". | How to create an agreement model | How to retire an agreement model | . ",
    "url": "/pages/write/edit_agreement_model.html#Related_topics",
    
    "relUrl": "/pages/write/edit_agreement_model.html#Related_topics"
  },"47": {
    "doc": "Edit an agreement model",
    "title": "Edit an agreement model",
    "content": "On this page . If you edit an agreement model, the changes have no effect on existing service level records, but affect all new service level records created after the edit. You can edit multiple records simultaneously by selecting them in the grid and updating them in the Preview pane on the right. For more information, see Mass update. | From the main menu, select Plan &gt; Service Level &gt; Agreement Models. | Click the record identifier in the ID column to display the selected record. To filter the record list, click the Add filter&nbsp; button. For more information, see Filters. | Edit the General model details section as required. | Field | Description | . | Title | A name for the model. Best practice: Choose a meaningful, descriptive, and relatively short name. The name is often the only identification used in selection lists and in other areas to identify components. | . | Description | A description that captures the details of the model. | . | Agreement flavor | You can't edit this field once the model is created. | . | Agreement type | You can't edit this field once the model is created. | . | Click Attachments &gt; Add attachment to upload a file to the agreement model. | The supported attachment file formats and the maximum file size of an attachment are defined in the tenant's Application settings tab in Suite Administration. | If the Attachments field has been defined as encrypted for this record type and you are a member of an encryption domain, click Add encrypted attachments to attach an encrypted file to the record. | Attachments aren't visible in the Service Portal. | . | Click the Workflow tab, to view the workflow for the model. Note that to change the status, you must have the appropriate rights.&nbsp; . | Click the Approvals tab to add approvals to the model. For more information about approvals, see&nbsp;Task plans. | Click the Default values tab and complete the information relevant to the model. The record fields displayed are the same as those detailed in Create a Service Level Agreement record. However, out-of-the-box, Service Management disables some fields when creating an agreement model. If you have the appropriate rights, you can customize this behavior. | Details . | Field | Description | . | Title | Type a word or phrase that's a unique identifier for this Service Level Agreement. It should be a value that makes it easy for the end user to understand the purpose of the Service Level Agreement. Example: Initial review for priority&nbsp;1 incidents. | . | Default agreement | Select this check box to make this Service Level Agreement record the default. The default Service Level Agreement (for incidents, changes, support requests, tasks, problems, or custom record types as appropriate) is automatically applied to all incidents and support requests which don't have a matching Service Level target Definition. | . | Owner | Choose the Service Level Agreement owner. Example: the owner might be the Service Level Manager. If no value is selected, the owner is set to the current user by default. | . | Technical group | Choose the technical group to which records created with this model are assigned. | . | Financial group | Choose the financial group to which records created with this model are assigned. | . | Description | Enter a description for the Service Level Agreement. It's helpful for other users to understand the purpose and objectives of this Service Level Agreement. | . | Requirement . | Field | Description | . | Cost | The cost associated with Service Level Agreements created with this model. | . | Effort | The effort associated with Service Level Agreements created with this model. | . | Validity start date | The start date of the period of validity of Service Level Agreements created with this model. | . | Validity end date | The end date of the period of validity of&nbsp;Service Level Agreements created with this model. | . | Review and improvement . | Field | Description | . | Next review date | The date when Service Level Agreements created with this model must next be reviewed. | . | Service quality report | The service quality report associated with Service Level Agreements created with this model. | . | Improvement measures | The improvement measures associated with Service Level Agreements created with this model. | . | Service improvement plan | The service improvement plan associated with Service Level Agreements created with this model. | . | . | Click the Group target sets&nbsp;tab to add target sets to the model. | Section | Description | . | Target Sets&nbsp; | The target set selected for the agreement is the same one selected in the Target Sets section in the General tab. You can select it from here also. The list of available target sets includes only target sets with a matching module: . | For Support OLAs, only the target sets of Type OLA with Incident, Support request, Change, Task, Problem, or customized record type selected as Module are available. | For Service&nbsp;OLAs, only target sets of module Service Request of type OLA are available. | . | . | Group override rules | In the Group override rules section, you can define override rules to override the default target set for specified groups. | To add an override rule, click Add. In the New group override rule dialog box, enter the following information: . | Name. Enter a name for the rule | Groups. Click in the Groups by name box to display a drop-down list of groups. Select the required groups to add. You can't select the same group in more than one override rule in an agreement. | Target sets. Select the target sets. The target sets selected should be with a same Module . | For Support OLAs, only the target sets of Type OLA with Incident, Support request, Change, Task, Problem, or customized record type selected as Module are available. | For Service&nbsp;OLAs, only target sets of module Service Request of type OLA are available. | . | Click Save to save the rule. | . | To edit an override rule, select the required rule and click Edit. | To delete an override rule, select the rule and click Delete. | . The maximum number of target sets that can be added is 500. | . | Click the Discussions tab to view any relevant conversations about the model. For more information about discussions, see Discussions. | To view changes or updates made to the record, click the History tab. For more information, see History. | Click Save. | . You can edit multiple records simultaneously by selecting them in the grid and updating them in the Preview pane on the right. For more information, see Mass update. ",
    "url": "/pages/write/edit_agreement_model.html",
    
    "relUrl": "/pages/write/edit_agreement_model.html"
  },"48": {
    "doc": "Audit of FinOps documentation",
    "title": "Audit of FinOps documentation",
    "content": " ",
    "url": "/pages/design/finops_audit.html",
    
    "relUrl": "/pages/design/finops_audit.html"
  },"49": {
    "doc": "Integrate with AWS for CMP FinOps",
    "title": "Integrate with AWS for CMP FinOps",
    "content": "On this page . | Complete prerequisite tasks in AWS . | Set up IAM user and permissions | Enable rightsizing recommendations | Configure data export | . | Set up an integration via Integration Studio to import cloud accounts . | Prepare an integration user | Configure endpoint | Create an integration | Configure a scenario . | Use IdleDaysToCleanUp to control the stale cloud account cleanup duration | . | Schedule the import of cloud accounts | Check the scenario execution status | Troubleshooting tips | . | Configure a cloud cost data provider to retrieve billing data . | Create a cost reporting integration | . | Related topics | . Before you can use the Cloud Management Platform (CMP) FinOps solution, you must set up integrations with the cloud platforms from which you want to pull cloud data. We support integrations with Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP). The CMP FinOps solution uses three types of cloud data. You will need to set up a separate integration to retrieve Cloud Account Data, but for Cloud Billing and Cloud Recommendations, this data is retrieved within the same integration. | Cloud account data – The CMP FinOps account management functionality enables you to centrally manage cloud accounts of supported cloud platforms. You must set up an Integration Studio-based integration to import cloud account data. | Cloud billing data – The CMP FinOps’ cloud cost visualization and optimization functionalities, including cost governance reports, cloud insights, and cloud cost limits, are all based on cloud billing data. You must set up a Cloud Cost Data Provider-based integration to retrieve cloud billing data. | Cloud recommendations data - The CMP FinOps’ Optimization functionalities, including cloud insights, are based on cloud recommendations that are collected from the different cloud providers.  You must set up a Cloud Cost Data Provider-based integration to retrieve these cloud recommendations. Setting up a Cloud Cost Data Provider-based integration with the appropriate permissions (as described below) will give you both cloud billing data and cloud recommendations data within the same integration. | . This topic describes how to set up the two AWS integrations that support the CMP FinOps capability: the cloud accounts import integration and the billing data and recommendations retrieval integration. ",
    "url": "/pages/write/finops_aws.html",
    
    "relUrl": "/pages/write/finops_aws.html"
  },"50": {
    "doc": "Integrate with AWS for CMP FinOps",
    "title": "Complete prerequisite tasks in AWS",
    "content": "To integrate with AWS, you must complete the following tasks in AWS at the management account level.  . Set up IAM user and permissions . Generate the security credentials required by the integrations and then grant the required permissions to the corresponding user: . | Log in to the AWS management account as an administrator or the root user. | Navigate to the IAM service and then open the Policies page. | Add a policy with the following services and permissions: . Service . Permissions . Organizations . | DescribeOrganization | DescribeAccount | ListAccounts | . Cost Explorer Service . | ce:Describe* | ce:List* | ce:Get* | ce:StartSavingsPlansPurchaseRecommendationGeneration | . Savings Plans . | savingsplans:Describe* | savingsplans:List* | . EC2 . | ec2:DescribeReservedInstances | ec2:DescribeReservedInstancesOfferings | ec2:DescribeInstances | ec2:DescribeVolumes | ec2:DescribeRegions | . Trusted Advisor . | trustedadvisor:Describe* | trustedadvisor:List* | trustedadvisor:Get* | trustedadvisor:DownloadRisk | . Compute Optimizer . | compute-optimizer:Get* | compute-optimizer:Describe* | . Cost and Usage Report . | cur:DescribeReportDefinitions | . Elastic Container Service . | ecs:ListServices | ecs:ListClusters | . Lambda . | lambda:ListProvisionedConcurrencyConfigs | lambda:ListFunctions | . You’ll need this policy for the cloud accounts import integration. | Navigate to the Users page, and then create an IAM user. | Make sure that the user’s access type is programmatic access. | Enter some meaningful tags for the user. The CMP FinOps capability will make use of this information. | . | Attach the policy added earlier and the following AWS managed policies to an appropriate principal entity (user, group, or role), depending on how you manage user permissions in AWS: . | AmazonEC2ReadOnlyAccess | AmazonS3ReadOnlyAccess | AWSOrganizationsReadOnlyAccess | AWSPriceListServiceFullAccessIf you attach the policies to a group or role, assign the created IAM user to the group or assign the role to the user. | . | Note down the security credentials (Access key ID and Secret access key) for the user. You will need to enter the credentials when configuring the billing data and cloud accounts import integrations. | . For details, see the AWS documentation. Enable rightsizing recommendations . | Log in to the AWS management account as an administrator or the root user. | Navigate to the AWS Billing and Cost Management console, and then open the Cost Management Preferences page. | On the General tab, select the Rightsizing Recommendations and Enable Allow linked accounts to see recommendations options. | Save your changes. | . For more information, see AWS Rightsizing Recommendations. Configure data export . To enable the billing data retrieval integration, you need to configure a legacy CUR type data export by following the instructions in the AWS documentation. Once configured, AWS will automatically generate reports and store them in the designated S3 bucket. The integration will then import these reports for further processing. This table lists the essential parameters required during the report configuration process. For other parameters, configure values that align with your specific requirements. Configuration parameter . Required value . Export type . Legacy CUR export . Include resource IDs . Select the option . Split cost allocation data . Select the option . Refresh automatically . Select the option . Report data time granularity . Daily . Report versioning . Create new report version . Report data integration . Don’t select any option . Compression type . GZIP . Note down the S3 bucket name and the report path prefix displayed on the Review page of the report creation wizard. You will need these values later when you configure the cloud cost data provider. After configuring the cost usage report, verify that you can generate the report in this directory of the S3 bucket: &lt;bucket name&gt;\\&lt;report path prefix&gt;. The actual report path will contain an actual date range instead of the date-range value. Note that it might take longer than 24 hours to generate reports in the S3 bucket after you configure the cost and usage report. ",
    "url": "/pages/write/finops_aws.html#complete-prerequisite-tasks-in-aws",
    
    "relUrl": "/pages/write/finops_aws.html#complete-prerequisite-tasks-in-aws"
  },"51": {
    "doc": "Integrate with AWS for CMP FinOps",
    "title": "Set up an integration via Integration Studio to import cloud accounts",
    "content": "This section describes how to set up an integration based on Integration Studio for AWS cloud accounts import. Prepare an integration user . Section begin - CreateIntUser . Create a dedicated integration user for each integration.  . | Log in to Suite Administration (https:///bo) as suite admin and create an integration user with the **Integration user** role for the integration. | Log in to Agent Interface as the tenant admin and do the following: . | Navigate to Administration &gt; Master Data &gt; People &gt; Roles, and create a role (with the Application value SMAX) with the Create, View, and Update permissions for the Cloud Account record type.  | Navigate to Administration &gt; Master Data &gt; People, locate the integration user, and then assign the created role to the user. | . | . Section end - CreateIntUser . Configure endpoint . Create an endpoint using the Rest Executor 2.0 endpoint type: . | Log in to Agent Interface as the cloud integration administrator. | Navigate to Administration &gt; Utilities &gt; Integration &gt; Endpoints. | Click Add. | Select Rest Executor 2.0 as the endpoint type, enter a name for the endpoint (for example, AWS), and then click Add. The system creates the endpoint. | Click Configure and set these fields: . Field . Value . Agent . Select Agentless. Authentication type . Select AWS Signature. Access key ID . Enter the access key ID obtained in the prerequisite section. Secret access key . Enter the secret access key obtained in the prerequisite section. Session token . Leave the field empty. Certificate type . Leave the field empty. Certificate format . Leave the field empty. Server certificate . Leave the field empty. | After completing the configuration, click Test connection to validate the configuration. | Click Save. | . Create an integration . In the following steps, you’ll create an integration with the predefined template. | Navigate to Administration &gt; Utilities &gt; Integration &gt; Integration studio.  | Click New, enter the name of your integration, and then click Save. The system displays the Integration page. | In the Details section, configure the integration user you created for the current SMAX system. | Select the Active option to activate the integration. | In the Connector-Endpoint mappings section, click Add to add a mapping. | Select Management &amp; Governance (under Amazon Web Services) as the connector, select the AWS endpoint that you created earlier, and then enter an alias for the mapping. | Click Save. | . Configure a scenario . Perform the following steps to add and configure a scenario: . | With the integration selected, click Add scenario.  | Enter a name for the scenario. For example, AWS account sync. | In the Template field, select AWS accounts sync process (under Amazon Web Services Management &amp; Governance). | Click Save to add the scenario. | (Optional) Expand the Set sync parameters rule in the Process logic section of the scenario, and update the value for the IdleDaysToCleanUp key. See below for the explanation of this parameter. | Click Save. | Manually run the scenario. If you don’t do this, CMP FinOps functionality that depends on cloud account data won’t work as expected until the first scheduled cloud account sync occurs. | . Use IdleDaysToCleanUp to control the stale cloud account cleanup duration . Service Management moves cloud accounts removed from the cloud platform to the Archived phase by using transition rules. To do this, it uses the “PENDING_CLOSURE” account status set by the integration scenario to decide what are the removed accounts. By default, the integration scenario sets the “PENDING_CLOSURE” status to cloud accounts removed more than 30 days ago. You can update the IdleDaysToCleanUp parameter’s value (in days) to change this duration. Section begin - scheduling . Schedule the import of cloud accounts . Use the scheduler to periodically import cloud accounts from the cloud platform. Create a schedule that executes the cloud accounts import scenario at the scheduled time (for example, trigger the import at 2:00 every day). To optimize system performance, schedule cloud account imports with intervals of at least 24 hours. Section end - scheduling . Check the scenario execution status . To check the scenario execution status, navigate to Administration &gt; Utilities &gt; Integration &gt; Endpoints, and then select the AWS endpoint in the left pane. The system displays the execution history for the AWS integration. Each record displayed in this grid corresponds to one scenario rule. Make sure all rules in the cloud accounts import scenario have the green Success status. | You can check the start and end time (and duration) for two main activities of the scenario: cloud accounts syncing and stale accounts status setting. To do this, click the hyperlinked ID for these rules (Capture sync time and Clean up sync time end), and check the value in the Output details field. The field displays both the start time and end time for the corresponding activity. | The Connector column indicates the system where the action of the rule is applied: Management &amp; Governance (AWS), SMAX, or Common. Unlike other values, Common means that this rule doesn’t call any APIs, instead, it just sets some values or performs some calculations. | Records whose Connector value isn’t Common contain detailed response data in the Response details field. Check this field for error codes or error messages for records with a warning or error status. | If you find an error code or message in error or warning records for AWS rules (with the Connector value Amazon Web Services Management &amp; Governance), check the Amazon documentation to see if you can find more information about the error. | . Troubleshooting tips . For rules that correspond to API calls (as opposed to Common actions), if the error code or message in the Response details field indicates connection or credential errors, take these actions: . | Click Run scenario in the Scenario details page to run the scenario again, and then check the execution history for that rule to see if it succeeds. Use this method to check if temporary network issues are the cause of the error. | Open the AWS endpoint’s configuration page, and click Test connection. If the test connection succeeds, it indicates that invalid AWS security credentials aren’t the cause of the error. | . ",
    "url": "/pages/write/finops_aws.html#set-up-an-integration-via-integration-studio-to-import-cloud-accounts",
    
    "relUrl": "/pages/write/finops_aws.html#set-up-an-integration-via-integration-studio-to-import-cloud-accounts"
  },"52": {
    "doc": "Integrate with AWS for CMP FinOps",
    "title": "Configure a cloud cost data provider to retrieve billing data",
    "content": "With this integration, you can configure retrieval of historical and current billing information for expenditures associated with your cloud provider. This information will be available within the reporting interface. Create a cost reporting integration . | Log in to Agent Interface as the cloud integration administrator. | Go to Administration &gt; Providers &gt; Cloud Cost Data Providers. | On the Integrations page, click either Add Integration (if no integrations exist) or the icon on the right pane to add an additional integration. | Select Amazon AWS and click Next. | Specify these fields: . | Connection Name: Identifier for the AWS integration, for example, “My AWS DEV Account”. | Access Key ID: AWS access key obtained in the prerequisite section | Secret Access Key: AWS secret access key obtained in the prerequisite section | Billing Bucket Name: The name of the S3 bucket that stores billing data and that you designated when you configured the data export in AWS. | Billing Report Path: The report path prefix (excluding the /date-range/ value at the end) that you specified when you configured the data export in AWS. | . If you forgot to make a note of the billing bucket name and billing report path when you configured the data export, follow these steps to identify them: . | Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/. | In the left navigation pane, select Buckets, and then navigate to the bucket associated with the integration. | Click the Copy S3 URI button, and then paste the URI to a text editor. The URI is formatted as follows: . s3://&lt;billing bucket name&gt;/&lt;billing report path&gt; . Important: The billing report path doesn’t include the forward slash that’s between it and the billing bucket name. For example, if the S3 URI is s3://bucket3///reportPath1/, the billing report path is //reportPath1/ (with one less forward slash). | . | Check the I require a proxy server to connect to this provider box to use a proxy server to connect to the cloud provider. You can either use the system default proxy server (configured by a suite admin) or use a custom proxy server here. | Leave the I require a custom base URL to connect to this provider checkbox unchecked to contact the cloud provider at its default endpoint. | Click Next. | Specify the following information: . | Collection History Cutoff: By default, the integration will fetch all available billing data from the cloud provider. For AWS, Amazon introduced CUR in 2015, and the availability of data depends on the creation date of the configured CUR reports. If, in either case, you wish to limit the amount of data, select a collection start date so that your collection only gathers data from the specified date. For example, selecting January 2020 will fetch data from that point to the present day and into the future. There is no end date. | Cloud Tags: To enable cloud tags for the selected provider, specify the name and key. Cloud tags help you to filter, search, and manage the AWS resources. For more information, see Configure Cloud Tags. | . | Click Finish. | . After you create the integration, a full data collection (including historical data) will occur. This may result in a longer-than-usual data collection time, and will cause issues if the collection is still running when the next scheduled collection is due to start. To avoid this issue, we strongly recommend that you disable the integration until the initial full data collection is complete. You can then enable the integration again to resume scheduled data collection. To do this, click the toggle switch under COLLECTION SETTINGS so that it says Collection is disabled. When the full data collection is complete, click the toggle switch again so that it says Collection is enabled. ",
    "url": "/pages/write/finops_aws.html#configure-a-cloud-cost-data-provider-to-retrieve-billing-data",
    
    "relUrl": "/pages/write/finops_aws.html#configure-a-cloud-cost-data-provider-to-retrieve-billing-data"
  },"53": {
    "doc": "Integrate with AWS for CMP FinOps",
    "title": "Related topics",
    "content": ". | For general information about the Integration Studio, see Integration Studio. | For information on triggering integration scenarios based on scheduler, see Scheduler. | For information on how to export and import integrations and scenarios, see Export and import integrations. | For information on Cost and Usage Report, see the AWS documentation Getting Started with CUR. | For information on cloud tags, see Configure cloud tags. | . ",
    "url": "/pages/write/finops_aws.html#related-topics",
    
    "relUrl": "/pages/write/finops_aws.html#related-topics"
  },"54": {
    "doc": "Integrate with Azure for CMP FinOps",
    "title": "Integrate with Azure for CMP FinOps",
    "content": "On this page . | Complete prerequisite tasks in AWS . | Set up IAM user and permissions | Enable rightsizing recommendations | Configure data export | . | Set up an integration via Integration Studio to import cloud accounts . | Prepare an integration user | Configure endpoint | Create an integration | Configure a scenario . | Use IdleDaysToCleanUp to control the stale cloud account cleanup duration | . | Schedule the import of cloud accounts | Check the scenario execution status | Troubleshooting tips | . | Configure a cloud cost data provider to retrieve billing data . | Create a cost reporting integration | . | Related topics | . Before you can use the Cloud Management Platform (CMP) FinOps solution, you must set up integrations with the cloud platforms from which you want to pull cloud data. We support integrations with Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP). The CMP FinOps solution uses three types of cloud data. You will need to set up a separate integration to retrieve Cloud Account Data, but for Cloud Billing and Cloud Recommendations, this data is retrieved within the same integration. | Cloud account data – The CMP FinOps account management functionality enables you to centrally manage cloud accounts of supported cloud platforms. You must set up an Integration Studio-based integration to import cloud account data. | Cloud billing data – The CMP FinOps’ cloud cost visualization and optimization functionalities, including cost governance reports, cloud insights, and cloud cost limits, are all based on cloud billing data. You must set up a Cloud Cost Data Provider-based integration to retrieve cloud billing data. | Cloud recommendations data - The CMP FinOps’ Optimization functionalities, including cloud insights, are based on cloud recommendations that are collected from the different cloud providers.  You must set up a Cloud Cost Data Provider-based integration to retrieve these cloud recommendations. Setting up a Cloud Cost Data Provider-based integration with the appropriate permissions (as described below) will give you both cloud billing data and cloud recommendations data within the same integration. | . This topic describes how to set up the two AWS integrations that support the CMP FinOps capability: the cloud accounts import integration and the billing data and recommendations retrieval integration. ",
    "url": "/pages/write/finops_azure.html",
    
    "relUrl": "/pages/write/finops_azure.html"
  },"55": {
    "doc": "Integrate with Azure for CMP FinOps",
    "title": "Complete prerequisite tasks in AWS",
    "content": "To integrate with AWS, you must complete the following tasks in AWS at the management account level. Set up IAM user and permissions . Generate the security credentials required by the integrations and then grant the required permissions to the corresponding user: . | Log in to the AWS management account as an administrator or the root user. | Navigate to the IAM service and then open the Policies page. | Add a policy with the following services and permissions: . Service . Permissions . Organizations . | DescribeOrganization | DescribeAccount | ListAccounts | . Cost Explorer Service . | ce:Describe* | ce:List* | ce:Get* | ce:StartSavingsPlansPurchaseRecommendationGeneration | . Savings Plans . | savingsplans:Describe* | savingsplans:List* | . EC2 . | ec2:DescribeReservedInstances | ec2:DescribeReservedInstancesOfferings | ec2:DescribeInstances | ec2:DescribeVolumes | ec2:DescribeRegions | . Trusted Advisor . | trustedadvisor:Describe* | trustedadvisor:List* | trustedadvisor:Get* | trustedadvisor:DownloadRisk | . Compute Optimizer . | compute-optimizer:Get* | compute-optimizer:Describe* | . Cost and Usage Report . | cur:DescribeReportDefinitions | . Elastic Container Service . | ecs:ListServices | ecs:ListClusters | . Lambda . | lambda:ListProvisionedConcurrencyConfigs | lambda:ListFunctions | . You’ll need this policy for the cloud accounts import integration. | Navigate to the Users page, and then create an IAM user. | Make sure that the user’s access type is programmatic access. | Enter some meaningful tags for the user. The CMP FinOps capability will make use of this information. | . | Attach the policy added earlier and the following AWS managed policies to an appropriate principal entity (user, group, or role), depending on how you manage user permissions in AWS: . | AmazonEC2ReadOnlyAccess | AmazonS3ReadOnlyAccess | AWSOrganizationsReadOnlyAccess | AWSPriceListServiceFullAccessIf you attach the policies to a group or role, assign the created IAM user to the group or assign the role to the user. | . | Note down the security credentials (Access key ID and Secret access key) for the user. You will need to enter the credentials when configuring the billing data and cloud accounts import integrations. | . For details, see the AWS documentation. Enable rightsizing recommendations . | Log in to the AWS management account as an administrator or the root user. | Navigate to the AWS Billing and Cost Management console, and then open the Cost Management Preferences page. | On the General tab, select the Rightsizing Recommendations and Enable Allow linked accounts to see recommendations options. | Save your changes. | . For more information, see AWS Rightsizing Recommendations. Configure data export . To enable the billing data retrieval integration, you need to configure a legacy CUR type data export by following the instructions in the AWS documentation. Once configured, AWS will automatically generate reports and store them in the designated S3 bucket. The integration will then import these reports for further processing. This table lists the essential parameters required during the report configuration process. For other parameters, configure values that align with your specific requirements. Configuration parameter . Required value . Export type . Legacy CUR export . Include resource IDs . Select the option . Split cost allocation data . Select the option . Refresh automatically . Select the option . Report data time granularity . Daily . Report versioning . Create new report version . Report data integration . Don’t select any option . Compression type . GZIP . Note down the S3 bucket name and the report path prefix displayed on the Review page of the report creation wizard. You will need these values later when you configure the cloud cost data provider. After configuring the cost usage report, verify that you can generate the report in this directory of the S3 bucket: &lt;bucket name&gt;\\&lt;report path prefix&gt;. The actual report path will contain an actual date range instead of the date-range value. Note that it might take longer than 24 hours to generate reports in the S3 bucket after you configure the cost and usage report. ",
    "url": "/pages/write/finops_azure.html#complete-prerequisite-tasks-in-aws",
    
    "relUrl": "/pages/write/finops_azure.html#complete-prerequisite-tasks-in-aws"
  },"56": {
    "doc": "Integrate with Azure for CMP FinOps",
    "title": "Set up an integration via Integration Studio to import cloud accounts",
    "content": "This section describes how to set up an integration based on Integration Studio for AWS cloud accounts import. Prepare an integration user . Section begin - CreateIntUser . Create a dedicated integration user for each integration.  . | Log in to Suite Administration (https:///bo) as suite admin and create an integration user with the **Integration user** role for the integration. | Log in to Agent Interface as the tenant admin and do the following: . | Navigate to Administration &gt; Master Data &gt; People &gt; Roles, and create a role (with the Application value SMAX) with the Create, View, and Update permissions for the Cloud Account record type.  | Navigate to Administration &gt; Master Data &gt; People, locate the integration user, and then assign the created role to the user. | . | . Section end - CreateIntUser . Configure endpoint . Create an endpoint using the Rest Executor 2.0 endpoint type: . | Log in to Agent Interface as the cloud integration administrator. | Navigate to Administration &gt; Utilities &gt; Integration &gt; Endpoints. | Click Add. | Select Rest Executor 2.0 as the endpoint type, enter a name for the endpoint (for example, AWS), and then click Add. The system creates the endpoint. | Click Configure and set these fields: . Field . Value . Agent . Select Agentless. Authentication type . Select AWS Signature. Access key ID . Enter the access key ID obtained in the prerequisite section. Secret access key . Enter the secret access key obtained in the prerequisite section. Session token . Leave the field empty. Certificate type . Leave the field empty. Certificate format . Leave the field empty. Server certificate . Leave the field empty. | After completing the configuration, click Test connection to validate the configuration. | Click Save. | . Create an integration . In the following steps, you’ll create an integration with the predefined template. | Navigate to Administration &gt; Utilities &gt; Integration &gt; Integration studio.  | Click New, enter the name of your integration, and then click Save. The system displays the Integration page. | In the Details section, configure the integration user you created for the current SMAX system. | Select the Active option to activate the integration. | In the Connector-Endpoint mappings section, click Add to add a mapping. | Select Management &amp; Governance (under Amazon Web Services) as the connector, select the AWS endpoint that you created earlier, and then enter an alias for the mapping. | Click Save. | . Configure a scenario . Perform the following steps to add and configure a scenario: . | With the integration selected, click Add scenario.  | Enter a name for the scenario. For example, AWS account sync. | In the Template field, select AWS accounts sync process (under Amazon Web Services Management &amp; Governance). | Click Save to add the scenario. | (Optional) Expand the Set sync parameters rule in the Process logic section of the scenario, and update the value for the IdleDaysToCleanUp key. See below for the explanation of this parameter. | Click Save. | Manually run the scenario. If you don’t do this, CMP FinOps functionality that depends on cloud account data won’t work as expected until the first scheduled cloud account sync occurs. | . Use IdleDaysToCleanUp to control the stale cloud account cleanup duration . Service Management moves cloud accounts removed from the cloud platform to the Archived phase by using transition rules. To do this, it uses the “PENDING_CLOSURE” account status set by the integration scenario to decide what are the removed accounts. By default, the integration scenario sets the “PENDING_CLOSURE” status to cloud accounts removed more than 30 days ago. You can update the IdleDaysToCleanUp parameter’s value (in days) to change this duration. Section begin - scheduling . Schedule the import of cloud accounts . Use the scheduler to periodically import cloud accounts from the cloud platform. Create a schedule that executes the cloud accounts import scenario at the scheduled time (for example, trigger the import at 2:00 every day). To optimize system performance, schedule cloud account imports with intervals of at least 24 hours. Section end - scheduling . Check the scenario execution status . To check the scenario execution status, navigate to Administration &gt; Utilities &gt; Integration &gt; Endpoints, and then select the AWS endpoint in the left pane. The system displays the execution history for the AWS integration. Each record displayed in this grid corresponds to one scenario rule. Make sure all rules in the cloud accounts import scenario have the green Success status. | You can check the start and end time (and duration) for two main activities of the scenario: cloud accounts syncing and stale accounts status setting. To do this, click the hyperlinked ID for these rules (Capture sync time and Clean up sync time end), and check the value in the Output details field. The field displays both the start time and end time for the corresponding activity. | The Connector column indicates the system where the action of the rule is applied: Management &amp; Governance (AWS), SMAX, or Common. Unlike other values, Common means that this rule doesn’t call any APIs, instead, it just sets some values or performs some calculations. | Records whose Connector value isn’t Common contain detailed response data in the Response details field. Check this field for error codes or error messages for records with a warning or error status. | If you find an error code or message in error or warning records for AWS rules (with the Connector value Amazon Web Services Management &amp; Governance), check the Amazon documentation to see if you can find more information about the error. | . Troubleshooting tips . For rules that correspond to API calls (as opposed to Common actions), if the error code or message in the Response details field indicates connection or credential errors, take these actions: . | Click Run scenario in the Scenario details page to run the scenario again, and then check the execution history for that rule to see if it succeeds. Use this method to check if temporary network issues are the cause of the error. | Open the AWS endpoint’s configuration page, and click Test connection. If the test connection succeeds, it indicates that invalid AWS security credentials aren’t the cause of the error. | . ",
    "url": "/pages/write/finops_azure.html#set-up-an-integration-via-integration-studio-to-import-cloud-accounts",
    
    "relUrl": "/pages/write/finops_azure.html#set-up-an-integration-via-integration-studio-to-import-cloud-accounts"
  },"57": {
    "doc": "Integrate with Azure for CMP FinOps",
    "title": "Configure a cloud cost data provider to retrieve billing data",
    "content": "With this integration, you can configure retrieval of historical and current billing information for expenditures associated with your cloud provider. This information will be available within the reporting interface. Create a cost reporting integration . | Log in to Agent Interface as the cloud integration administrator. | Go to Administration &gt; Providers &gt; Cloud Cost Data Providers. | On the Integrations page, click either Add Integration (if no integrations exist) or the icon on the right pane to add an additional integration. | Select Amazon AWS and click Next. | Specify these fields: . | Connection Name: Identifier for the AWS integration, for example, “My AWS DEV Account”. | Access Key ID: AWS access key obtained in the prerequisite section | Secret Access Key: AWS secret access key obtained in the prerequisite section | Billing Bucket Name: The name of the S3 bucket that stores billing data and that you designated when you configured the data export in AWS. | Billing Report Path: The report path prefix (excluding the /date-range/ value at the end) that you specified when you configured the data export in AWS. | . If you forgot to make a note of the billing bucket name and billing report path when you configured the data export, follow these steps to identify them: . | Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/. | In the left navigation pane, select Buckets, and then navigate to the bucket associated with the integration. | Click the Copy S3 URI button, and then paste the URI to a text editor. The URI is formatted as follows: . s3://&lt;billing bucket name&gt;/&lt;billing report path&gt; . Important: The billing report path doesn’t include the forward slash that’s between it and the billing bucket name. For example, if the S3 URI is s3://bucket3///reportPath1/, the billing report path is //reportPath1/ (with one less forward slash). | . | Check the I require a proxy server to connect to this provider box to use a proxy server to connect to the cloud provider. You can either use the system default proxy server (configured by a suite admin) or use a custom proxy server here. | Leave the I require a custom base URL to connect to this provider checkbox unchecked to contact the cloud provider at its default endpoint. | Click Next. | Specify the following information: . | Collection History Cutoff: By default, the integration will fetch all available billing data from the cloud provider. For AWS, Amazon introduced CUR in 2015, and the availability of data depends on the creation date of the configured CUR reports. If, in either case, you wish to limit the amount of data, select a collection start date so that your collection only gathers data from the specified date. For example, selecting January 2020 will fetch data from that point to the present day and into the future. There is no end date. | Cloud Tags: To enable cloud tags for the selected provider, specify the name and key. Cloud tags help you to filter, search, and manage the AWS resources. For more information, see Configure Cloud Tags. | . | Click Finish. | . After you create the integration, a full data collection (including historical data) will occur. This may result in a longer-than-usual data collection time, and will cause issues if the collection is still running when the next scheduled collection is due to start. To avoid this issue, we strongly recommend that you disable the integration until the initial full data collection is complete. You can then enable the integration again to resume scheduled data collection. To do this, click the toggle switch under COLLECTION SETTINGS so that it says Collection is disabled. When the full data collection is complete, click the toggle switch again so that it says Collection is enabled. ",
    "url": "/pages/write/finops_azure.html#configure-a-cloud-cost-data-provider-to-retrieve-billing-data",
    
    "relUrl": "/pages/write/finops_azure.html#configure-a-cloud-cost-data-provider-to-retrieve-billing-data"
  },"58": {
    "doc": "Integrate with Azure for CMP FinOps",
    "title": "Related topics",
    "content": ". | For general information about the Integration Studio, see Integration Studio. | For information on triggering integration scenarios based on scheduler, see Scheduler. | For information on how to export and import integrations and scenarios, see Export and import integrations. | For information on Cost and Usage Report, see the AWS documentation Getting Started with CUR. | For information on cloud tags, see Configure cloud tags. | . ",
    "url": "/pages/write/finops_azure.html#related-topics",
    
    "relUrl": "/pages/write/finops_azure.html#related-topics"
  },"59": {
    "doc": "Integrate with cloud platforms for CMP FinOps",
    "title": "Integrate with cloud platforms for CMP FinOps",
    "content": "To use the Cloud Management Platform (CMP) FinOps solution, firstly you need to retrieve cloud data from the cloud platforms that we support: Amazon Web Services (AWS), Azure, and Google Cloud Platform (GCP). The CMP FinOps solution uses two types of cloud data: . | Cloud billing data – The CMP FinOps’ cloud cost visualization and optimization functionalities, including cost governance reports, cloud insights, and cloud cost limits, are all based on the retrieved billing data. | Cloud account data – The CMP FinOps account management functionality enables you to centrally manage cloud accounts of supported cloud platforms. | . You need to set up two types of integrations to retrieve the cloud data: . | Cloud Cost Data Provider-based integrations that retrieve cloud billing data | Integration Studio-based integrations that import cloud account data | . See these topics for how to set up the integrations to retrieve cloud data from each cloud platform: . | Integrate with AWS for CMP FinOps | Integrate with Azure for CMP FinOps | Integrate with GCP for CMP FinOps | . ",
    "url": "/pages/write/finops_landing.html",
    
    "relUrl": "/pages/write/finops_landing.html"
  },"60": {
    "doc": "Integrate with OCI for CMP FinOps",
    "title": "Integrate with OCI for CMP FinOps",
    "content": "On this page . | Complete prerequisite tasks in OCI . | Create a user and generate a secret key | Set up and configure a group | Create a policy | . | Set up a Cloud Cost Data Provider-based integration  | Related topics | . Before you can use the Cloud Management Platform (CMP) FinOps solution, you must set up integrations with the cloud platforms from which you want to pull cloud data. We support integrations with Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI). The CMP FinOps solution currently supports only the collection of cloud billing data from OCI (integrations with AWS, Azure, and GCP also support the collection of cloud account and cloud recommendations data). This topic describes how to set up a Cloud Cost Data Provider-based integration to retrieve cloud billing data from OCI. To view this data, you will need to set up an integration with Power BI. ",
    "url": "/pages/write/finops_oci.html",
    
    "relUrl": "/pages/write/finops_oci.html"
  },"61": {
    "doc": "Integrate with OCI for CMP FinOps",
    "title": "Complete prerequisite tasks in OCI",
    "content": "OCI generates Cost and Usage (CUR) reports, which contain the cloud billing data that CMP FinOps uses, by default. You can’t modify these reports. Therefore, you don’t have to set up or configure CUR reports to set up an integration with OCI. However, you do have to perform some initial setup to access the reports. Create a user and generate a secret key . OCI does not have service accounts. In order to grant CMP FinOps API access to the OCI cloud data, you must create a real user- no service users in OCI, so create a real user and use “customer secret key” to grant API access to CMP FinOps. | Log in to OCI Console as a member of the Administrators group. | In the navigation menu, click Identity &amp; Security &gt; Domains, and then select the identity domain that you want to work in. You might need to change the compartment to find the domain that you want. | Click Users &gt; Create user. | In the First name and Last name fields enter the user’s name. For example, you name this user “FinOps Admin”. | Leave the Use the email address as the username option selected. | In the Username / Email field, enter an email address for the user account, and then click Create. | In the left-hand Resources menu, click Customer secret keys, and then click Generate secret key. | Enter a name for the key, and then click Generate secret key. | Make a note of the values in the Name and Access key columns. You will need these values when configuring the Cloud Cost Data Provider-based integration. | . Set up and configure a group . | In the navigation menu, click Identity &amp; Security &gt; Domains, and then select the identity domain that you want to work in. You might need to change the compartment to find the domain that you want. | Click Groups &gt; Create group. | In the Name and Description fields of the Create group window, enter the name of the group (for example, “oci_finops”) and a description. | Add the user that you created earlier to the group. | Click Create. | . Create a policy . | In the navigation menu, click Identity &amp; Security &gt; Identity &gt; Create Policy. | In the Name and Description fields of the Create policy window, enter the name of the policy (for example, “oci_cur_reports”) and a description. | If you want to attach the policy to a compartment other than the one you’re viewing, select it from the Compartment list.  | Click Show manual editor, and then copy the following text to the Policy Builder text box. `define tenancy usage-report as ocid1.tenancy.oc1..aaaaaaaaned4fkpkisbwjlr56u7cj63lf3wffbilvqknstgtvzub7vhqkggq endorse group &lt;group&gt; to read objects in tenancy usage-report` . Update the &lt;group&gt; placeholder in this text with the name of the group that you created earlier. For example, “oci_finops”. | Click Create. | . ",
    "url": "/pages/write/finops_oci.html#complete-prerequisite-tasks-in-oci",
    
    "relUrl": "/pages/write/finops_oci.html#complete-prerequisite-tasks-in-oci"
  },"62": {
    "doc": "Integrate with OCI for CMP FinOps",
    "title": "Set up a Cloud Cost Data Provider-based integration ",
    "content": "This integration enables you to retrieve historical and current cloud billing data for expenditures associated with your cloud provider. This information will be available within the reporting interface. | Log in to Agent Interface as the cloud integration administrator. | Go to Administration &gt; Providers &gt; Cloud Cost Data Providers. | On the Integrations page, click Add Integration (if no integrations exist) or the icon on the right pane (to add an additional integration). | Select Oracle Cloud Infrastructure, click NEXT, and then enter the following information: . Field . Description . Connection Name . Identifier for the OCI integration, for example, “My OCI”. Access Key ID . The name of the customer secret key that you created in OCI Console earlier. Secret Access Key . The access key of the customer secret key that you created in OCI Console earlier. Billing Bucket Name . The Oracle Cloud ID (OCID) of your root compartment. To see the OCID of all your compartments (including the root compartment), log in to OCI Console as a member of the Administrators group, and then click Identity &amp; Security &gt; Compartments in the navigation menu. Billing Report Path . Enter reports/cost-csv. Bucket Endpoint URL . Enter https://bling.compat.objectstorage.&lt;OCI region&gt;.oraclecloud.com.   . Replace the &lt;OCI region&gt; placeholder with your OCI home region. For example, you enter https://bling.compat.objectstorage.us-ashburn-1.oraclecloud.com. For information about OCI home regions, see Managing Regions. | Leave I require a proxy server to connect to this provider unselected if you want to use a proxy server to connect to the cloud provider. You can use either the system default proxy server (configured by a suite administrator) or a custom proxy server. | Select I require a custom base URL to connect to this provider if you don’t want to contact the cloud provider at its default endpoint. | Click NEXT. | (Optional) Specify the Collection History Cutoff. By default, all available cloud billing data is fetched from the cloud provider. If you wish to limit the amount of data, select a collection start date so that your collection only gathers data from the specified date. For example, selecting January 2022 will fetch data from that point to the present day and into the future. There is no end date. | Click FINISH. | . After you create the integration, a full data collection (including historical data) will occur. This may result in a longer-than-usual data collection time, and will cause issues if the collection is still running when the next scheduled collection is due to start. To avoid this issue, we strongly recommend that you disable the integration until the initial full data collection is complete. You can then enable the integration again to resume scheduled data collection. To do this, click the toggle switch under COLLECTION SETTINGS so that it says Collection is disabled. When the full data collection is complete, click the toggle switch again so that it says Collection is enabled. ",
    "url": "/pages/write/finops_oci.html#set-up-a-cloud-cost-data-provider-based-integration",
    
    "relUrl": "/pages/write/finops_oci.html#set-up-a-cloud-cost-data-provider-based-integration"
  },"63": {
    "doc": "Integrate with OCI for CMP FinOps",
    "title": "Related topics",
    "content": ". | For more information about cost and usage reports in OCI, see Cost and Usage Reports. | For more information about users in OCI, see Managing Users. | For more information about groups in OCI, see Managing Groups. | For more information about policies in OCI, see Managing Policies. | For more information about Power BI, see Integrate with Power BI to create FinOps reports. | . ",
    "url": "/pages/write/finops_oci.html#related-topics",
    
    "relUrl": "/pages/write/finops_oci.html#related-topics"
  },"64": {
    "doc": "Check the firewall settings",
    "title": "Related topics",
    "content": "When you have finished, return to Set up prerequisites (embedded K8s) to continue. ",
    "url": "/pages/write/firewall_settings.html#related-topics",
    
    "relUrl": "/pages/write/firewall_settings.html#related-topics"
  },"65": {
    "doc": "Check the firewall settings",
    "title": "Check the firewall settings",
    "content": "On this page . | Add firewall rules for the inbound connections | Add firewall rules for the outbound connections | Enable VRRP protocol for Keepalived in a multiple control plane node deployment | Related topics | . | Role | Location | Privileges required | . | System administrator | All control plane nodes, worker nodes, and NFS servers | Root | . If you haven’t enabled a firewall, skip this topic. There are many firewall management tools to manage your firewall. For example, firewalld or iptables. The following steps are based on a firewalld managed firewall. If you are using another firewall management tool, contact your network administrator for detailed steps on how to add the related firewall rules. To check whether firewalld manages your firewall, run the following command: . systemctl status firewalld . If your terminal resembles the following, firewalld manages your firewall: . [root@sh ~\\]# systemctl status firewalld firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2020-06-22 04:36:32 CST; 2 weeks 0 days ago Docs: man:firewalld(1) Main PID: 878 (firewalld) Tasks: 2 CGroup: /system.slice/firewalld.service └─878 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid . Add firewall rules for the inbound connections . The following steps apply to a root user. If you are a sudo user, add sudo in front of each command before you run it. For example: . sudo iptables -S | grep -- '-P INPUT' . You can also add the related commands that sudo users need to perform to the /etc/sudoers file. For the NFS servers . | Make sure the default policy for the INPUT chain is ACCEPT. To check the default policy, run the following command: . iptables -S | grep -- '-P INPUT' . If the default policy isn’t ACCEPT, contact your IT system administrator to change the policy. | Run the following commands on the NFS server: . systemctl start firewalld; systemctl enable firewalld firewall-cmd --permanent --add-port=111/udp firewall-cmd --permanent --add-port=111/tcp firewall-cmd --permanent --add-port=22/tcp firewall-cmd --permanent --add-port=2049/tcp firewall-cmd --permanent --add-port=20048/tcp firewall-cmd --reload . | . For the control plane nodes and worker nodes . If you’ve enabled firewalld, OMT will add firewall rules automatically on the control plane and worker nodes with your confirmation. Add firewall rules for the outbound connections . You must make sure you’ve added the related firewall rules for the required outbound ports to ensure the connection. For example, run the following commands on the worker nodes and control plane nodes to configure the firewall outbound port 5432 to connect an external PostgreSQL database: . systemctl start firewalld; systemctl enable firewalld firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 0 -p tcp -m tcp --dport=5432 -j ACCEPT -m comment --comment \"connect PostgreSQL\" firewall-cmd --reload . Enable VRRP protocol for Keepalived in a multiple control plane node deployment . If you want to set up a multiple control plane node (HA) environment by setting the HA_VIRTUAL_IP parameter in the install.properties file, make sure you’ve enabled the vrrp protocol when you have enabled the firewall on the node. Keepalived will the vrrp protocol to support virtual IP. In most default settings, the vrrp protocol is enabled. If you have used some custom settings for the server or if Keepalived doesn’t work well, run the following command on the node to enable the vrrp protocol: . firewall-cmd --add-protocol vrrp --permanent firewall-cmd --reload . ",
    "url": "/pages/write/firewall_settings.html",
    
    "relUrl": "/pages/write/firewall_settings.html"
  },"66": {
    "doc": "Online help restructure",
    "title": "Online help restructure",
    "content": " ",
    "url": "/pages/design/help_restructure.html",
    
    "relUrl": "/pages/design/help_restructure.html"
  },"67": {
    "doc": "Home",
    "title": "Home",
    "content": "I’m a lead tech writer, formerly at OpenText, with 15 years of experience in technical documentation. I mainly worked on IT Service Management products and a Kubernetes-based container orchestration platform. Also an occasional cycling coach and one-time archaeologist. You can contact me at edcork@gmail.com and find me on LinkedIn. ",
    "url": "/",
    
    "relUrl": "/"
  },"68": {
    "doc": "Prepare an environment to install OMT",
    "title": "Prepare an environment to install OMT",
    "content": "On this page . | Prepare to install the embedded Kubernetes | Request certificates | Set up persistent volumes | Create external databases | Decide what happens to log files | Configure on-access security scans | Create and configure a config.json file | . Before you install OPTIC Management Toolkit (OMT), you must perform some prerequisite tasks to configure the environment. ",
    "url": "/pages/write/install_landing.html",
    
    "relUrl": "/pages/write/install_landing.html"
  },"69": {
    "doc": "Prepare an environment to install OMT",
    "title": "Prepare to install the embedded Kubernetes",
    "content": "You must ensure that the cluster is correctly configured for the install script to deploy the Kubernetes that is embedded in OMT. This comprises the following tasks. | Task | Required | Description | Detailed steps | . | Enable a regular user to install OMT | Optional | You need only do this if you will use a regular user to perform the installation. | Enable a regular user to install OMT | . | Update the system configuration | Mandatory | Update the system configuration to meet the deployment requirements. This includes ensuring the localhost resolves to 127.0.0.1, setting the required system parameters, disabling swap space, making sure all required Linux packages are installed, checking the SSH connection, checking default gateway, checking the host name on every node, and synchronizing time. A script is available to automate this process.  | Make required system configurations | . | Check the firewall settings | Mandatory (if you’ve set a firewall) | If you have enabled a firewall in your network, you must check that the firewall settings meet the deployment requirements.  | Check the firewall settings | . | Check that the required ports are open | Mandatory | Check that all the ports that OMT required for network communication are open. | Check that the required ports are open (embedded K8s) | . | Configure High Availability (HA) | Optional | If you have more than one control plane node, you can configure HA. To do this, you can either use Keepalived (included in OMT) or set up your own load balancer(s). | Configure Keepalived for High Availability Configure an internal load balancer | . | Configure the install.properties file. | Optional | Configure the installation of the control plane and worker nodes in the install.properties file. You can also use command options to do this when you run the install command.  | Configure the install.properties file | . | Run a preliminary check of the nodes | Optional | You can run a script (pre-check.sh) that checks the readiness of the nodes for the deployment. This step is optional. However, it’s highly advisable that you perform a preliminary check on all the control plane nodes and worker nodes before you perform the remaining preparatory tasks to check whether your prepared nodes meet the basic requirements to install OPTIC Management Toolkit (OMT).  | Run a preliminary check | . ",
    "url": "/pages/write/install_landing.html#prepare-to-install-the-embedded-kubernetes",
    
    "relUrl": "/pages/write/install_landing.html#prepare-to-install-the-embedded-kubernetes"
  },"70": {
    "doc": "Prepare an environment to install OMT",
    "title": "Request certificates",
    "content": "Certificates protect the network traffics between OMT and external services. The network traffic flows include: . | Browser -&gt; OMT The OMT installer will create certificate authorities (CAs) to generate and sign server certificates for the ingress controller. However, the browsers in your organization won’t be able to validate these certificates, as they’re not in your organization’s trust store. If you want the browsers in your organization to connect to OMT securely, you must contact your IT administrator and request a server certificate pair (including a server certificate, a server key, and the CA cert which signed the server certificate). The server certificate pair is generated for the external access host name of OMT. For more information, see Request server certificates (embedded K8s). | OMT -&gt; external database If you are installing OMT with an external database, connect the database with TLS mode. In this case, contact the database administrator to request the CA certificate to validate the database server certificate. You’ll use the CA certificate when you configure the config.json file later. | OMT -&gt; external image registry If you are installing OMT with an external image registry, if the registry server certificate is already trusted on the operating system level, the CA cert of the registry isn’t required. Otherwise, contact the external registry administrator to request the CA cert to validate the registry server certificate. The CA cert will be used while running the install command. | . These certificates are required for OMT installation. For the certificates required by applications, please check the applications’ documentation. ",
    "url": "/pages/write/install_landing.html#request-certificates",
    
    "relUrl": "/pages/write/install_landing.html#request-certificates"
  },"71": {
    "doc": "Prepare an environment to install OMT",
    "title": "Set up persistent volumes",
    "content": "If a container stops or restarts, all changes made inside the container are lost. To save information such as configuration files and databases, the information must be stored outside of the container in a persistent volume (PV). When you install OMT with the embedded Kubernetes, the NFS provisioner capability (which is enabled by default) creates the required PVs automatically. To use this capability, you must create a single volume on the NFS server. When you run the install command, you will specify the NFS server URL and the path to this volume in command options. For more information about how to set up persistent volumes, see Set up persistent volumes (embedded K8s). ",
    "url": "/pages/write/install_landing.html#set-up-persistent-volumes",
    
    "relUrl": "/pages/write/install_landing.html#set-up-persistent-volumes"
  },"72": {
    "doc": "Prepare an environment to install OMT",
    "title": "Create external databases",
    "content": "OMT uses an IdM database. Unless you will use the PostgreSQL instance that’s embedded in OMT, you must create it on the database server that you prepared earlier. For more information about how to do this, see Configure external databases (embedded K8s). If you want to use the embedded database instance, you don’t need to create the databases. The install script will deploy the required databases automatically if you don’t specify any database options when you run the install command. ",
    "url": "/pages/write/install_landing.html#create-external-databases",
    
    "relUrl": "/pages/write/install_landing.html#create-external-databases"
  },"73": {
    "doc": "Prepare an environment to install OMT",
    "title": "Decide what happens to log files",
    "content": "OMT infrastructure and applications produce log files. By default, OMT collects these logs on an NFS volume (itom-logging-vol). However, you can also forward the logs to an external receiver, such as Elasticsearch Server or Splunk. For more information about how to do this, see Forward application logs to an external receiver (embedded K8s). ",
    "url": "/pages/write/install_landing.html#decide-what-happens-to-log-files",
    
    "relUrl": "/pages/write/install_landing.html#decide-what-happens-to-log-files"
  },"74": {
    "doc": "Prepare an environment to install OMT",
    "title": "Configure on-access security scans",
    "content": "OMT installation may fail if you have enabled on-access scanning by security products such as McAfee Endpoint Security, Microsoft Defender, or Trend Micro Deep Security Agent in your environment. To prevent this, you must exclude certain directories from the on-access scan. Further information is available in the Security products can’t scan files before they’re deleted troubleshooting topic. ",
    "url": "/pages/write/install_landing.html#configure-on-access-security-scans",
    
    "relUrl": "/pages/write/install_landing.html#configure-on-access-security-scans"
  },"75": {
    "doc": "Prepare an environment to install OMT",
    "title": "Create and configure a config.json file",
    "content": "Before you run the install command, you must Create and configure a config.json file (embedded K8s) that contains all the necessary parameters for the installation script. This file will capture information about many of the decisions you have made while following the steps in this topic. ",
    "url": "/pages/write/install_landing.html#create-and-configure-a-configjson-file",
    
    "relUrl": "/pages/write/install_landing.html#create-and-configure-a-configjson-file"
  },"76": {
    "doc": "Record models",
    "title": "Record models",
    "content": "On this page . A model simplifies the creation of a record. For example, when you have common incidents, it's efficient to design an incident model that you can reuse to simplify the amount of effort required to resolve that same type of incident many times. Models standardize the end-to-end process and maximize efficiency. A Service Management model pre-populates common fields to save time when you create a new record. When you create a new record and select an appropriate model, Service Management automatically populates the relevant fields. For certain record types, a model can even create the tasks necessary to complete a process. Service Management provides a set of default models, and you can add models as required. If you no longer require a model, you can retire that model. For more information, see the following: . | Module | Topics | . | Change Management | . | How to create a change model | How to edit a change model | . | . | Incident Management | . | How to create an incident model | How to edit an incident model | . | . | Knowledge Management | . | How to create an article model | How to edit an article model | . | . | Release Management | . | How to create a release model | How to edit a release model | . | . | Service Asset &amp; Configuration Management | . | How to create an asset model | How to edit an asset model | . | . | Software Asset Management | . | How to create a license model | How to edit a license model | . | . | Service Level Management | . | How to create an agreement model | How to edit an agreement model | How to retire an agreement model | . | . ",
    "url": "/pages/write/models.html",
    
    "relUrl": "/pages/write/models.html"
  },"77": {
    "doc": "Retire an agreement model",
    "title": "Related topics",
    "content": ". | How to create an agreement model | How to edit an agreement model | . ",
    "url": "/pages/write/retire_agreement_model.html#related-topics",
    
    "relUrl": "/pages/write/retire_agreement_model.html#related-topics"
  },"78": {
    "doc": "Retire an agreement model",
    "title": "Retire an agreement model",
    "content": "On this page . | Related topics | . To retire an agreement model means changing its status, as shown in the workflow snapshot at the top of the model, from Active to Inactive. You must have the appropriate permissions to be able to retire an agreement model. If you retire an agreement model, this has no effect on existing service level agreement records. | From the main menu, select Plan &gt; Service Level &gt; Agreement Models. | Click the record identifier in the Id column to display the selected model. | Click Retired. | Click Save on the toolbar. | . ",
    "url": "/pages/write/retire_agreement_model.html",
    
    "relUrl": "/pages/write/retire_agreement_model.html"
  },"79": {
    "doc": "Update the system configuration",
    "title": "Check the SSH configurations",
    "content": "OMT supports default SSH configurations. If you are using default SSH configurations, ignore the steps in this topic. If you don’t have SSH enabled, or aren’t aware of the current SSH configurations, follow the steps in this topic. OpenSSH provides secure and encrypted connections between machines. Enable SSH on all control plane and worker nodes (cluster nodes) in your deployment. You can run the following command to check whether SSH is enabled and running as the root user or the regular user with elevated permissions: . systemctl is-active sshd . If the output is active, SSH is enabled on this node. However, if you have customized the SSH configurations, perform the following steps on all control plane nodes and worker nodes to check whether your SSH configuration meets the installation requirements. Check whether you have enabled SSH . Check the output of the systemctl is-active sshd command to make sure that SSH is enabled on this node. An SSH server is installed and enabled on most operating systems, and the configuration file is /etc/ssh/sshd_config.  . | To check whether the SSH server has been installed, run the following command: . systemctl -t service|grep sshd . | If the SSH isn’t installed on your node, run the following commands to install and start the SSH server: . yum install openssh-server systemctl enable sshd systemctl start sshd . | . Check MAC and Cipher algorithms . For security reasons, the IT administrator may allow only limited algorithms for SSH client connection. Make sure the /etc/ssh/sshd_config files on all the control plane nodes and worker nodes are configured with at least one of the following values. If no MAC/Cipher algorithm is configured in the /etc/ssh/sshd_config file, the server uses the default MAC/Cipher algorithm whose default value contains the MAC and Cipher requested by OMT. In that case, you can ignore this “Check MAC and Cipher algorithms” section.  . | For MAC algorithms: hmac-sha1,hmac-sha2-256,hmac-sha2-512,hmac-sha1-96 | For Cipher algorithms: 3des-cbc,aes128-cbc,aes192-cbc,aes256-cbc,aes128-ctr,aes192-ctr,aes256-ctr,arcfour128,arcfour256,blowfish-cbc . For example, add the following lines to the /etc/ssh/sshd_config files on all the control plane nodes and worker nodes: . MACs hmac-sha2-256,hmac-sha2-512 Ciphers aes128-cbc,aes192-cbc,aes256-cbc,aes128-ctr,aes192-ctr,aes256-ctr . | . Check the password or key authentication setting . During the installation, you will use either user name and password authentication or user name and key authentication to add control plane nodes and worker nodes on the installation portal. To use a user name and password authentication to add nodes for the installation, make sure the PasswordAuthentication parameter in the /etc/ssh/sshd_config file is set to yes. You can run the following command to check whether the PasswordAuthentication parameter is set to yes. Replace &lt;cluster node&gt; with the IPv4 address or FQDN of any of the cluster nodes. ssh root@&lt;cluster node&gt; echo \"It\\'s working\\!\" . The command line will prompt you to enter the password of the cluster node. If your terminal resembles the following, the PasswordAuthentication parameter is set to yes. [root ~]# ssh root@192.0.2.0 echo “It\\’s working\\!” root@192.0.2.0’s password: It’s working! . To add the cluster nodes by using a user name and key authentication, make sure the PubkeyAuthentication parameter in the /etc/ssh/sshd_config file is set to yes. Enable the password or key authentication setting . If the password or key authentication setting isn’t enabled, follow these steps on all control plane and worker nodes to enable the setting: . | Log on to the cluster node as the root user. | Open the /etc/ssh/sshd_config file with a supported editor. | To enable the password or key authentication, make sure the value of the related parameter is set to yes. To enable both, set the value of both these parameters to yes. To enable password authentication: Check that the value of the PasswordAuthentication parameter is yes. If not, set the value of the parameter to yes, as follows: . PasswordAuthentication yes . To enable the key authentication: Check that the value of the PubkeyAuthentication parameter is yes. If not, set the value of the parameters to yes, as follows: . PubkeyAuthentication yes . | Set the value of the PermitRootLogin parameter. To enable both password and key authentication or only password authentication, set the value of the parameter to yes, as follows . PermitRootLogin yes . To enable key authentication only, set the value of the parameter to prohibit-passsword, as follows: . PermitRootLogin prohibit-password . | Save the /etc/ssh/sshd_config file. | Run the following command to restart the sshd service: . systemctl restart sshd.service . | . Check whether you have enabled SCP . To check if you’ve enabled SCP, follow these steps:  . | Ensure that the /etc/ssh/disable_scp file doesn’t exist. | Ensure that the PermitTTY option in the /etc/ssh/sshd_config configuration file is set to yes: . PermitTTY yes . | Ensure that the /etc/ssh/sshd_config configuration file doesn’t contain the following settings: . ForceCommand internal-sftp . | . ",
    "url": "/pages/write/system_config.html#check-the-ssh-configurations",
    
    "relUrl": "/pages/write/system_config.html#check-the-ssh-configurations"
  },"80": {
    "doc": "Update the system configuration",
    "title": "Use the script to automate the system configuration",
    "content": "Role . Location . Privileges required . System administrator . All control plane nodes, all worker nodes, and all NFS nodes . Root or sudo . The node_prereq script automates this process. You will need to run the script on all your control plane nodes, worker nodes, and NFS servers. The node_prereq script depends on the yum command to install the related packages. Make sure a yum repository has been set up correctly on the server. Contact your IT administrator for help if one hasn’t. Copy the script to each node . | Copy the &lt;OMT_Embedded_K8s_2x.x-xxx&gt;/node_prereq script from the first control plane node to the /tmp directory on each of the remaining nodes (control plane nodes, worker nodes, and NFS server).  | Navigate to the directory that contains the node_prereq script, and then run the following command to add the executive permission:  . chmod +x node_prereq . | . Run the script . On each node, navigate to the directory that contains the script, and then run the appropriate command. This automates all the required configurations. | On the control plane nodes, run:  ./node_prereq -T master --all . | On the worker nodes, run:  ./node_prereq -T worker --all . | On the NFS servers, run:  ./node_prereq -T nfs --all . | . If you want to automate only some required configurations, you can use command options to specify which tasks the script will perform: . | The --etc-hosts command option ensures the localhost is resolved to 127.0.0.1 | The --sys-param command option checks and sets the required system parameters | The --disable-swap command option disable swap space | The --install-pkg command option installs the required Linux packages | . ",
    "url": "/pages/write/system_config.html#use-the-script-to-automate-the-system-configuration",
    
    "relUrl": "/pages/write/system_config.html#use-the-script-to-automate-the-system-configuration"
  },"81": {
    "doc": "Update the system configuration",
    "title": "Update the system configurations manually",
    "content": "add text . Ensure localhost is resolved to 127.0.0.1 . Role . Location . Privileges required . System administrator . All control plane nodes and worker nodes . Root or sudo . Flannel uses the default gateway to create packet routing for communication. To enable network communication across all the cluster nodes, you must ensure localhost resolves to 127.0.0.1 on all control plane and worker nodes. To do this, follow these steps:   . | Run the following command to check your localhost setting:   . grep -v '^\\s*#' /etc/hosts 2&gt;/dev/null | grep -E '\\slocalhost$|\\slocalhost\\s' . If there is no return value, run the following command to set the default route setting: . echo \"127.0.0.1 localhost\" &gt;&gt; /etc/hosts   .   . | Open the /etc/hosts file with a supported editor and check the configurations in the file. Make sure that localhost resolves to 127.0.0.1. For example: . 127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 . If you want to enable IPv4/IPv6 dual stack for OMT installation, make sure the localhost resolves to ::1 in /etc/hosts file. For example, ::1 localhost localhost6 localhost6.localdomain6 . | . Set the required system parameters . Role . Location . Privileges required . System administrator . All control plane nodes and worker nodes . Root or sudo . OMT uses the br_netfilter module to enable transparent masquerading and to ease Virtual Extensible LAN (VxLAN) traffic for communication between Kubernetes pods across the cluster nodes. Therefore, you must make sure the br_netfilter module is installed on all control plane and worker nodes before you set the system parameters. To do this, follow these steps: . | Log in to the node. | Run the following command to check whether the br_netfilter module is enabled: . lsmod |grep br_netfilter . If there is no return value, the br_netfilter module isn’t installed. Run the following commands to install it: . modprobe br_netfilter echo \"br_netfilter\" &gt; /etc/modules-load.d/br_netfilter.conf . | Open the /etc/sysctl.conf file in a supported editor. The sysctl.conf file contains the following instructions: . # To override a whole file, create a new file with the same in # /etc/sysctl.d/ and put new settings there. To override # only specific settings, add a file with a lexically later # name in /etc/sysctl.d/ and put new settings there. Ignore these instructions and update the sysctl.conf file directly with the settings described below.  . | For the operating system with the parameter fs.may_detach_mounts, run the following command to check if the parameter exists: . # sysctl -n fs.may_detach_mounts . If you get the following error message or get an integer 1, you don’t need to configure the parameter. [root@testVM ~]# sysctl -n fs.may_detach_mounts sysctl: cannot stat /proc/sys/fs/may_detach_mounts: No such file or directory [root@testVM ~]# sysctl -n fs.may_detach_mounts 1 . If you get an integer 0, you must configure the parameter and set it to 1 in the sysctl.conf file. [root@testVM ~]# sysctl -n fs.may_detach_mounts 0 . | Set the following system parameters according to the operating system that’s installed on the node. Redhat 8.1 and later versions . net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 kernel.sem=50100 128256000 50100 2560 . Oracle Linux 7.9 and later versions . net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 kernel.sem=50100 128256000 50100 2560 . Rocky Linux versions . net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 kernel.sem=50100 128256000 50100 2560 . Other older operating systems, including supported versions of CentOS . net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 net.ipv4.tcp_tw_recycle = 0 fs.may_detach_mounts = 1 kernel.sem=50100 128256000 50100 2560 . | Save the /etc/sysctl.conf file, and then run the following commands to apply the updates to the node: . /sbin/sysctl -p . | . Disable swap space . Role . Location . Privileges required . System administrator . All control plane nodes and worker nodes . Root or sudo . Complete this task on all control plane and worker nodes.  . | Run the following command to disable the swap process: . swapoff -a . | By default, the swap configuration is in the /etc/fstab file. You can check and permanently disable the swap process from that configuration file. Open the /etc/fstab file in a supported editor, and then comment out the lines that display “swap” as the disk type.  . For example: . #/dev/mapper/centos_shcentos72x64-swap swap . | . Install the required Linux packages . Role . Location . Privileges required . System administrator . “Host” in the table below . Root or sudo . The installation depends on various packages that are included in standard yum repositories. The following table describes the packages and the servers on which they must be installed. Package . Host . CentOS 7.9 . OL 7.9 . OL 8.8 . OL 9.2 . RHEL 7.9 . RHEL 8.6 . RHEL 8.8 . RHEL 9.0 . RHEL 9.2 . Rocky 8.8 . Rocky 9.2 . device-mapper-libs . control plane, workers . 1.02.170-6 . 1.02.170-6 . 1.02.181-9 . 1.02.187-7 . 1.02.170-6 . 1.02.181-3 . 1.02.181-9 . 1.02.183-4 . 1.02.187-7 . 1.02.181-9 . 1.02.187-7 . java-1.8.0-openjdk . control plane only . 1.8.0.275.b01-0 . 1.8.0_265-b01 . 1.8.0_372-b07 . 1.8.0_372 . 1.8.0.262.b10 . 1.8.0.302.b08-3 . 1.8.0.372.b07-4 . 1.8.0.322.b06-9 . 1.8.0.362.b09-2 . 1.8.0.372.b07-4 . 1.8.0.372.b07-2 . libgcrypt . control plane, workers . 1.5.3-14 . 1.5.3-14 . 1.8.5-7 . 1.10.0-10 . 1.5.3-14 . 1.8.5-6 . 1.8.5-7 . 1.10.0-2 . 1.10.0-10 . 1.8.5-7 . 1.10.0-10 . libseccomp . control plane, workers . 2.3.1-4 . 2.3.1-4 . 2.5.2-1 . 2.5.2-2 . 2.3.1-4 . 2.5.2-1 . 2.5.2-1 . 2.5.2-2 . 2.5.2-2 . 2.5.2-1 . 2.5.2-2 . libtool-ltdl . control plane, workers . 2.4.2-22 . 2.4.2-22 . 2.4.6-25 . 2.4.6-45 . 2.4.2-22 . 2.4.6-25 . 2.4.6-25 . 2.4.6-45 . 2.4.6-45 . 2.4.6-25 . 2.4.6-45 . net-tools . control plane, workers . 2.0-0.25.20131004git . 2.0-0.25.20131004git . 2.0-0.52.20160912git . 2.0-0.62.20160912git . 2.0-0.25.20131004git  . 2.0-0.52.20160912git . 2.0-0.52.20160912git . 2.0-0.62.20160912git . 2.0-0.62.20160912git . 2.0-0.52.20160912git . 2.0-0.62.20160912git . nfs-utils . control plane, workers, NFS servers . 1.3.0-0.68 . 1.3.0-0.66 . 2.3.3-59 . 2.5.4-18 . 1.3.0-0.68 . 2.3.3-51 . 2.3.3-59 . 2.5.4-10 . 2.5.4-18 . 2.3.3-59 . 2.5.4-18 . rpcbind . control plane, workers, NFS servers . 0.2.0-49 . 0.2.0-49 . 1.2.5-10 . 1.2.6-5 . 0.2.0-49 . 1.2.5-8 . 1.2.5-10 . 1.2.6-2 . 1.2.6-5 . 1.2.5-10 . 1.2.6-5 . systemd-libs (version &gt;= 219) . control plane, workers . 219-78 . 219-78 . 239-74.0.2 . 252-14.0.1 . 219-78 . 239-58 . 239-74 . 250-6 . 252-13 . 239-74 . 252-13 . unzip . control plane, workers . 6.0-21 . 6.0-21 . 6.0-46.0.1 . 6.0-56.0.1 . 6.0-21 . 6.0-46 . 6.0-46 . 6.0-56 . 6.0-56 . 6.0-46 . 6.0-56 . conntrack-tools . control plane, workers . 1.4.4-7. el7 . 1.4.4-7.el7 . 1.4.4-11 . 1.4.7-2 . 1.4.4-70.el7 . 1.4.4-10.el8 . 1.4.4-11 . 1.4.5-10.el9 . 1.4.7-2 . 1.4.4-11 . 1.4.7-2 . curl . control plane, workers . 7.29.0-59 . 7.29.0-59 . 7.61.1 . 7.76.1 . 7.29.0-59 . 7.61.1-22 . 7.61.1-30 . 7.76.1-14 . 7.76.1-23 . 7.61.1-25 . 7.76.1-19 . lvm2 . control plane, workers . 2.02.187-6 . 2.02.186-7 . 2.03.14-9 . 2.03.17-7 . 2.02.187-6 . 2.03.14-3 . 2.03.14-9 . 2.03.14-4 . 2.03.17-7 . 2.03.14-9 . 2.03.17-7 . socat . control plane, workers . 1.7.3.2-2 . 1.7.3.2-2 . 1.7.4.1-1 . 1.7.4.1-5 . 1.7.3.2-6 . 1.7.4.1-1 . 1.7.4.1-1 . 1.7.4.1-5 . 1.7.4.1-5 . 1.7.4.1-1 . 1.7.4.1-5 . checkpolicy . control plane, workers . 2.5-8.el7 . 2.5-8.el7 . 2.9-1 . 3.5-1 . 2.5-8.el7 . 2.9-1.el8 . 2.9-1.el8 . 3.3-1.el9 . 3.5-1.el9 . 2.9-1.el8 . 3.5-1.el9 . policycoreutils . control plane, workers . 2.5-8.el7 . 2.5-34.el7 . 2.9-24.0.1 . 3.5-1 . 2.5-8.el7 . 2.9-19.el8 . 2.9-24.el8 . 3.3-6.el9_0 . 3.5-1.el9 . 2.9-24.el8 . 3.5-1.el9 . policycoreutils-python/policycoreutils-python-utils . control plane, workers . 2.5-8.el7 . 2.5-34.el7 . - . - . 2.5-34.el7 . 2.9-19.el8 . 2.9-24.el8 . 3.3-6.el9_0 . 3.5-1.el9 . 2.9-24.el8 . 3.5-1.el9 . container-selinux . control plane, workers . 2.74 . 2.74 . 2.205.0-2.module+el8.8.0 . 2.205.0-1.el9_2.noarch . 2.5-34.el7 . 2.179.1-1 . 2.205.0-2 . 2.179.1-1 . 2.205.0-1 . 2.205.0-2 . 2.205.0-1 . bind-utils . control plane, workers . 9.11.4-26 . 9.11.4-16 . 9.11.36-8 . 9.16.23-11 . 9.11.4 . 9.11.36-3 . 9.11.36-8 . 9.16.23-1 . 9.16.23-11 . 9.11.36-8 . 9.16.23-11 . tar . control plane, workers . 1.26-35 . 1.26-35 . 1.30-9 . 1.34-6 . 1.26-35 . 1.30-5 . 1.30-9 . 1.34-3 . 1.34-6 . 1.30-9 . 1.34-6 . iptables . control plane, workers . 1.4.21-35 . 1.4.21-35 . 1.8.4-24.0.1.el8.x86_64 . - . 1.4.21-35 . 1.8.4-22 . - . - . 1.8.4-24 . - . iptables-nft . control plane, workers . - . - . - . 1.8.8-6.el9_1.x86_64 . - . - . 1.8.4-24 . 1.8.7-28 . 1.8.8-6 . - . 1.8.8-6 . rng-tools***** . control plane, workers . 6.3.1 or higher . 6.3.1 or higher . 6.3.1 or higher . 6.3.1 or higher . 6.3.1 or higher . 6.3.1 or higher . 6.3.1 or higher . 6.3.1 or higher . 6.3.1 or higher . 6.3.1 or higher . 6.3.1 or higher . * This package is used to increase the host entropy. If the entropy is too small on the control plane and worker nodes, containers running on the hosts may not start. Run the cat /proc/sys/kernel/random/entropy_avail command to check if the hosts have enough entropy. Typically, the output should be larger than 2000. You can ignore the entropy check on RHEL/Oracle Linux/Centos/Rocky 9.x because there is an improvement in the kernel. It works well even though the available entropy is lower than 2000. If the packages aren’t installed on any of the servers, follow these steps: . | Make sure a yum repository has been set up correctly on the server. Contact your IT administrator for help if a yum repository hasn’t been set up correctly on your server. | Run the following command to install the required packages: . yum install &lt;package name&gt; . For example, to install the java-1.8.0-openjdk package on OL 8.6, run the following command: . yum install java-1.8.0-openjdk-1.8.0.332.b09 . | . Set the default gateway settings . Role . Location . Privileges required . System administrator . All control plane nodes and worker nodes . Root or sudo . If you get an error message “Default gateway not set” after the node_prereq script checks the default gateway. Run the following command to add a default gateway on that node: . route add default gw &lt;IP address&gt; &lt;interface name&gt; . For example, you run the following command: . route add default gw 192.0.2.24 eth0 . Configure the hosts file in the etc directory . Role . Location . Privileges required . System administrator . All control plane nodes and worker nodes . Root or sudo . If you get an error message after the node_prereq script checks the node host name, you’ll have to configure the host name. If you have configured Domain Name Service (DNS) in your environment, and the control plane and worker nodes can resolve the FQDN of all cluster nodes, load balancer host, NFS server, and external databases, skip this section. If the Domain Name Service (DNS) isn’t configured in your environment, configure the /etc/hosts file on every node (control plane and worker) in the cluster. List all the control plane nodes, worker nodes, external database servers, external access host (HA virtual IP, load balancer), and NFS servers in the /etc/hosts file. In an environment that doesn’t have DNS configured, you must also configure the KUBE_DNS_HOSTS parameter in the install.properties file as described in the “Configure the install.properties file” topic. To set up the host name resolution after the installation, follow the steps listed in the “Update the DNS entries” topic. Add the IP address and FQDN details of all the nodes in the cluster. This includes the external access host (HA virtual IP, load balancer), external database servers, and NFS server. Add an entry for each server using the following syntax: &lt;IP address&gt; &lt;FQDN&gt; . For example, you can add the following entries to the /etc/hosts file: . 192.0.2.0 external-accesshost.mycompany.com 192.0.2.1 control1.mycompany.com 192.0.2.2 control2.mycompany.com 192.0.2.3 control3.mycompany.com 192.0.2.4 worker1.mycompany.com 192.0.2.5 worker2.mycompany.com 192.0.2.6 worker3.mycompany.com 192.0.2.7 externalbalancer.mycompany.com 192.0.2.8 externaldb.mycompany.com 192.0.2.9 nfs.mycompany.com . Synchronize time . Role . Location . Privileges required . System administrator . All control plane nodes, worker nodes, NFS servers, load balancers, and database servers . Root or sudo . OMT components require the time on all nodes to be synchronized. If you get a warning message after the node_prereq script checks the node host name.  . Make sure that the Kubernetes cluster nodes can reach the Network Time Protocol (NTP) servers (either internal organization based or external internet based NTP servers). The following example uses the chrony tool to synchronize time across operating systems. You must have a time server prepared for the time synchronization. | On the first control plane node, run the following command to install the chrony client: . yum install chrony -y . | Run the following commands to create the drift file: . mkdir -p /var/lib/chrony echo &gt; /var/lib/chrony/driftfile . | Run the following commands to configure the chrony client. Replace the &lt;Time Server Name or IP Address&gt; placeholder with the host name or IPv4 address of your time server. You can use a public time server if your cluster can access the Internet. Otherwise, use an existing time server of the organization or create a time server. cat &lt;&lt;ENDFILE &gt;/etc/chrony.conf server &lt;Time Server Name or IP Address&gt; iburst driftfile /var/lib/chrony/driftfile stratumweight 0 rtcsync makestep 0.1 3 ENDFILE . | Run the following commands to enable and start the chrony service: . systemctl enable chronyd systemctl start chronyd . | Run the following command to synchronize the operating system time with the NTP server: . chronyc -a makestep . | Run the following command to restart the chrony daemon: . systemctl restart chronyd . | Run the following command to check the server time synchronization: . timedatectl . If your terminal resembles the following with the NTP synchronized set to “yes”, then you have successfully synchronized the time on the host with the time server: . [root@ ~]# timedatectl status Local time: Thu 2020-07-21 13:05:21 CST Universal time: Thu 2020-07-21 05:05:21 UTC RTC time: Thu 2020-07-21 05:05:21 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yes NTP synchronized: yes RTC in local TZ: no DST active: n/a . | Run the following command to synchronize the hardware time from the current system time: . hwclock -w . | Repeat these steps on all other control plane nodes (if any) and worker nodes. You must also synchronize the time on the NFS servers, load balancers, and database servers once you have created them. | . ",
    "url": "/pages/write/system_config.html#update-the-system-configurations-manually",
    
    "relUrl": "/pages/write/system_config.html#update-the-system-configurations-manually"
  },"82": {
    "doc": "Update the system configuration",
    "title": "Related topics",
    "content": "When you have finished, return to Set up prerequisites (embedded K8s) to continue. ",
    "url": "/pages/write/system_config.html#related-topics",
    
    "relUrl": "/pages/write/system_config.html#related-topics"
  },"83": {
    "doc": "Update the system configuration",
    "title": "Update the system configuration",
    "content": "On this page . | Check the SSH configurations . | Check whether you have enabled SSH | Check MAC and Cipher algorithms | Check the password or key authentication setting | Enable the password or key authentication setting | Check whether you have enabled SCP | . | Use the script to automate the system configuration . | Copy the script to each node | Run the script | . | Update the system configurations manually . | Ensure localhost is resolved to 127.0.0.1 | Set the required system parameters | Disable swap space | Install the required Linux packages | Set the default gateway settings | Configure the hosts file in the etc directory | Synchronize time | . | Related topics | . Before you can deploy OMT with the embedded Kubernetes, you must:  . | Check the SSH configurations | Ensure the localhost is resolved to 127.0.0.1 | Set the required system parameters | Disable swap space | Install the required Linux packages | Check the default gateway | Check the host name on every node | Synchronize time | . The  node_prereq script automates the system configuration process, refer to Use the script to automate the system configuration for more information. If you decide not to use the node_prereq script and configure the parameters manually, follow the steps in the Update the system configurations manually section. ",
    "url": "/pages/write/system_config.html",
    
    "relUrl": "/pages/write/system_config.html"
  },"84": {
    "doc": "Writing",
    "title": "Writing",
    "content": " ",
    "url": "/pages/write.html",
    
    "relUrl": "/pages/write.html"
  }
}
